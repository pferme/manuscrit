With the growing number of connected devices and the explosion in the amount of exchanged data, the need for efficient and reliable communication has never been as critical as in today's world. Noise, coming from physical imperfection, signal interference, or even \emph{lepidopterans}~\cite{Hopper81}, is one of the major hurdles to overcome.

The Theory of Information established by Shannon in his seminal work~\cite{Shannon48} provides clear and definite answers to the amount of data that can be transmitted through noisy point-to-point channels. The asymptotic rate at which information can be sent through multiple independent copies of a channel is fully characterized by a mathematical quantity called \emph{capacity}. Although one cannot hope to surpass this fundamental limit, developing codes achieving the channel's capacity is not a simple task in practice, as the code construction by Shannon was only probabilistic and the decoding is inefficient as a function of the number of copies of the channel.

Nonetheless, restricting the study to binary symmetric channels, the research in error-correcting codes led to capacity-approaching solutions with efficient encoding and decoding procedures, such as low-density parity-check codes~\cite{Gallager62}, turbo codes~\cite{BG96} or more recently polar codes~\cite{Arikan09}; see~\cite{RU08} for a general review on error-correcting codes.

Another approach, developed by Barman and Fawzi in~\cite{BF18}, looks only at a unique copy of an arbitrary channel. Contrary to usual one-shot information theory~\cite{RWW06,Tomamichel12,TBR16}, they focus on the algorithmic aspects of coding. The objective shifts to maximizing the probability of successfully transmitting a message through that channel over all possible encoders and decoders, known as the channel coding problem. As this algorithmic task is \textrm{NP}-hard, they proposed a polynomial-time approximation algorithm achieving a ratio of $1-e^{-1}$, which cannot be increased if $\textrm{P}\not=\textrm{NP}$ thanks to a connection to the maximum coverage problem~\cite{Feige98}. Note that this approach differs from algorithmic information theory~\cite{Chaitin77}, where information is defined using Türing machines rather than probability theory.

Following this work, the $\ell$-list-decoding task~\cite{Elias57,Wozencraft58}, where the decoder is only asked to output a list of $\ell$ possible guesses instead of giving the right input message, was studied from an algorithmic point of view in~\cite{BFGG20}. Similarly, they developed a $\left(1-\frac{\ell^{\ell}e^{-\ell}}{\ell!}\right)$-approximation algorithm running in polynomial time, and proved that this ratio cannot be increased under the unique games conjecture~\cite{Khot02}. More generally, a larger class of combinatorial optimization problems, called maximum $\ell$-multi-coverage, falls in the analysis of~\cite{BFGG20}.

A natural extension of the maximum $\ell$-multi-coverage problem entails maximizing the quantity $\sum_{a \in [n]}  w_a\varphi(\abs{\set{i \in S : a \in T_i}})$ over subsets $S \subseteq [m]$ of cardinality $k$, for a general nondecreasing concave function $\varphi$; the  maximum $\ell$-multi-coverage problem is retrieved by taking $\varphi(x) := \min(x,\ell)$. In terms of channel coding interpretation, this corresponds to what we call $\varphi$-list-decoding, where the length restriction on the list of guesses of the decoder in $\ell$-list-decoding is replaced by a probability $\frac{\varphi(\ell)}{\ell}$ of correctly decoding a list of guesses of variable size $\ell$. This problem will be the first subject of study of the thesis.

Shaking the physical foundations of the world at the beginning of the twentieth century, the rise of quantum mechanics has revolutionized the comprehension of nature at atomic and subatomic scale. One of its most intriguing characteristics is the notion of \emph{entanglement}. Two entangled particles have the particularity of being correlated, even when spread apart, in a way that cannot be explained by non-quantum physics laws.

Einstein, Podolsky and Rosen~\cite{EPR35} first discovered that phenomenon, which implied either that the quantum-mechanical description of physical reality was not complete or that two incompatible physical quantities did not have simultaneously a concrete value. Surprisingly and contrary to the original goal of what has become known as the Einstein–Podolsky–Rosen paradox~\cite{EPR35}, it is the latter that describes correctly the atomic and subatomic physical reality.

Bell showed in~\cite{Bell64} that no hidden-variable theory, where we could associate unknown local variables to each of the entangled particles, would be enough to explain their correlation when spread apart. This \emph{nonlocal} nature of quantum physics is in contradiction with the usual principle of locality of classical physics, which states that any object can be influenced only by its immediate surroundings. The results in~\cite{Bell64} were generalized into the so-called CHSH inequality~\cite{CHSH69}, which if violated, imply that a hidden-variable theory cannot explain the correlations between the particles. These inequalities violations were practically realized by Aspect et al.~\cite{ADG82}, which finally proved the nonlocal nature of quantum physics.

Another particularity of quantum mechanics is that any measurement of physical properties of a quantum system actually disturbs it. Furthermore, in the case of entangled particles, measuring one of the particles will also affect the other. This \emph{spooky action at a distance}, as stated by Einstein~\cite{Born71}, does not violate special relativity, as one cannot actually transmit information through that process. %The result of a quantum measurement being intrinsically random, only correlation is obtained between particles.

However, entanglement can be used to enhance communication. A qubit, the quantum basic unit of information, cannot by itself convey more than one bit of information~\cite{Holevo73}. However, Bennett and Wiesner showed in~\cite{BW92} that if quantum entanglement is shared between the parties, in the form of what is called an EPR pair, then one can actually transmit two bits of information using only one qubit. This is known today as superdense coding. For point-to-point (classical) channels, quantum entanglement shared between the sender and the receiver can increase the optimal success probability of channel coding \cite{CLMW10,PLMKR11}, although it does not increase the channel capacity~\cite{BBCJPW93,BSST99}. 

One can also abstract quantum entanglement into \emph{non-signaling} correlations~\cite{Tsirelson80,PR94}. In order to understand those, it is useful to look at the game interpretation of the previously mentioned CHSH inequality. We consider a two-player game with Alice and Bob. A referee gives a uniformly random bit $x$ (resp. $y$) to Alice (resp. Bob), and their common goal is to output respectively bits $a$ and $b$ such that $a \oplus b = x \land y$ without communicating, where $\oplus$ denotes the exclusive disjunction. It is easy to see that no strategy can lead to a better success probability than $\frac{3}{4}$, even assuming that hidden variables are shared between the players. However, as shown in~\cite{CHSH69}, if Alice and Bob share an entangled EPR pair, they can apply smart measurements and achieve a success probability of $\cos^2\left(\frac{\pi}{8}\right) \simeq 0.85$, which in fact cannot be outperformed~\cite{Tsirelson80}. Note that the correlation $P(ab|xy)$ describing the result of this quantum strategy confirms that no information is transmitted using quantum entanglement. Indeed, one can show that the marginal from a player is independent from the other player's input, that is to say that $\sum_bP(ab|xy)=\sum_bP(ab|xy')$ and $\sum_aP(ab|xy)=\sum_aP(ab|x'y)$ for all $a,b,x,y,x',y'$. Non-signaling correlations are defined as the set of \emph{all} joint distributions $P(ab|xy)$ that satisfy the previous equalities. They naturally include quantum strategies, but are in fact stronger, as one can even achieve a success probability of $1$ for the CHSH game using a general non-signaling strategy, simply by defining $P(ab|xy):=\frac{1}{2}$ if $a \oplus b = x \land y$, and $P(ab|xy):=0$ otherwise~\cite{PR94}.

General non-signaling correlations are not representing physical reality, as they imply stronger nonlocal behaviors than those occurring within quantum mechanics. However, they present a strong theoretical interest. Notably, the description of the set of non-signaling correlations is much simpler than with quantum ones, as they are characterized by simple linear constraints; see~\cite{BCPSW14} for a general review on nonlocality.

The approximation algorithm for the channel coding problem in~\cite{BF18} relies in fact on non-signaling assisted codes. As finding the best non-signaling assisted code for a channel is a linear program, it can be solved exactly in polynomial time. They have developed a strategy to transform that non-signaling assisted code into a classical one, losing at most a factor $1-e^{-1}$ in the success probability. A more precise statement of this strategy actually implies that the capacity regions with or without non-signaling assistance are the same, retrieving back a result by~\cite{Matthews12}. It should also be noted that non-signaling assistance does not change the capacity region of the reverse task of channel simulation~\cite{CRBT22}. This unexpected link between approximation algorithms for the channel coding problem and non-signaling correlations is the main motivation of the rest of the thesis. Specifically, we will develop these analysis in the main network communication scenarios.

Network information theory, which aims at understanding communication over multiple-sender multiple-receiver channels, was first studied by Shannon in the particular case of two-way channels~\cite{Shannon61} (known today as interference channels). Later, Cover~\cite{Cover72} introduced broadcast channels, with multiple receivers but a single sender. Ahlswede~\cite{Ahlswede73} and independently Liao~\cite{Liao73} studied the reverse scenario of multiple-access channels, with multiple senders but a single receiver. These more complex channels allow a better modelization of real-life interconnected communication.

Contrary to the point-to-point setting, nonlocality can increase the capacity of network channels. Quek and Shor showed in~\cite{QS17} the existence of two-sender two-receiver interference channels with gaps between their classical, quantum-entanglement assisted and non-signaling assisted capacity regions. Following this result, Leditzky et al.~\cite{LALS20,SLSS22} showed that quantum entanglement shared between the two senders of a multiple-access channel can strictly enlarge its capacity region.

We will address the coding problem for both multiple-access channels and broadcast channels. We will study the impact of non-signaling correlations on their capacities, as well as their links to approximation algorithms of the related coding problems.

\paragraph{Contributions} In Chapter \ref{chap:MaxCoverage}, we consider the following generalization of the maximum $\ell$-multi-coverage problem depending on a concave, nondecreasing function $\varphi$: given subsets $T_1, \ldots, T_m$ of a universe $[n]$, positive weights $w_a$ on the universe $[n]$ and an integer $k$, the objective is to find a subset $S \subseteq [m]$ of size $k$ that maximizes $C^{\varphi}(S) \coloneqq \sum_{a \in [n]}w_a\varphi(\abs{S}_a)$, where $\abs{S}_a := \abs{\set{i \in S : a \in T_i}}$; the maximum $\ell$-multi-coverage problem is retrieved by taking $\varphi(x) := \min(x,\ell)$. For any such $\varphi$, we provide an efficient algorithm that achieves an approximation ratio equal to the \emph{Poisson concavity ratio} of $\varphi$, defined by $\alpha_{\varphi} := \min_{x \in \mathbb{N}^*} \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(\mathbb{E}[\Poi(x)])}$. Complementing this approximation guarantee, we establish a matching \textrm{NP}-hardness result when $\varphi$ grows in a sublinear way. Applied to channel coding, and more specifically to $\varphi$-list-decoding, where the length restriction on the list of guesses of the decoder in $\ell$-list-decoding is replaced by a probability $\frac{\varphi(\ell)}{\ell}$ of correctly decoding a list of guesses of variable size $\ell$, we obtain a tight approximation guarantee $\alpha_{\varphi}$ for the class of channels $W$ of the form $W(y|x) = \frac{1}{t}$ for $y \in T_x$ with $\abs{T_x}=t$ and $W(y|x) = 0$ elsewhere. Our result goes beyond this particular setting and we illustrate it with applications to distributed resource allocation problems, welfare maximization problems and approval-based voting for general rules.

In Chapter \ref{chap:MAC}, we address the problem of coding for multiple-access channels. We first show that it cannot be approximated in polynomial time within any constant ratio, under a complexity hypothesis on random $k$-SAT formulas. Then, we study the influence of non-signaling correlations between parties. We develop a linear program computing the optimal success probability for coding over $n$ copies of a multiple-access channel $W$ with size growing polynomially in $n$. Solving this linear program allows us to achieve inner bounds for multiple-access channels. Applying this method to the binary adder channel, we show that using non-signaling assistance, the sum-rate $\frac{\log_2(72)}{4} \simeq 1.5425$ can be reached even with zero error, which beats the maximum sum-rate capacity of $1.5$ in the unassisted case. For noisy channels, where the zero-error non-signaling assisted capacity region is trivial, we can use concatenated codes to obtain achievable points in the capacity region. Applied to a noisy version of the binary adder channel, we show that non-signaling assistance still improves the sum-rate capacity. Complementing these achievability results, we give an outer bound on the non-signaling assisted capacity region that has the same expression as the unassisted region except that the channel inputs are not required to be independent. Finally, we show that the capacity region with non-signaling assistance shared only between each sender and the receiver independently is the same as without assistance.

In Chapter \ref{chap:Broadcast}, we address the problem of coding for broadcast channels. We first show that when non-signaling assistance is shared only between decoders, the capacity region does not change. For the class of deterministic broadcast channels, we describe a $(1-e^{-1})^2$-approximation algorithm running in polynomial time, and we show that the capacity region for that class is the same with or without non-signaling assistance. Finally, we show that in the value query model, we cannot achieve a better approximation ratio than $\Omega\left(\frac{1}{\sqrt{m}}\right)$ in polynomial time for the general broadcast channel coding problem, with $m$ the size of one of the outputs of the channel.
