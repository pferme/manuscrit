General Intro on Information Theory? With definitions of entropies etc. MAYBE
Graph notations on partitions?
Groups and orbits and actions (basic stuff)?

Put common lemmas and general stuff: negatively associated random variables: recall important properties etc. (clean 3.5.4 with that and Prop 5.15 moved to intro)

\newpage
\section{Probability}
\subsection{Usual Distributions and Properties}
\begin{definition}[Usual Distributions]
  \begin{enumerate}
  \item The Bernouilli distribution $\Ber(p)$ is defined on $\{0,1\}$ with the mass function:
        \[ (1-p,p) \ . \]
  \item The binomial distribution $\Bin(n,p)$ is defined on $\{0,\ldots,n\}$ with the mass function:
        \[ \left(\binom{n}{k}p^k(1-p)^{n-k}\right)_{k \in \{0,\ldots,n\}} \ .\]
  \item The Poisson distribution $\Poi(\lambda)$ is defined on $\mathbb{N}$ with the mass function:
        \[ \left(\frac{\lambda^ke^{-\lambda}}{k!}\right)_{k \in \mathbb{N}} \ .\]
  \end{enumerate}      
\end{definition}

\begin{proposition}
    For $\varphi$ concave, and $p \in [0,1]^m$, we have:
    \[\mathbb{E}\Big[\varphi\Big(\sum_{i=1}^m\Ber(p_i)\Big)\Big] \geq \mathbb{E}\Big[\varphi\Big(\Poi\Big(\sum_{i=1}^m p_i\Big)\Big)\Big]\ .\]
  \label{prop:ConvexOrder}
\end{proposition}

\begin{proof}
  The notion of \emph{convex order} discussed in~\cite{SS07} allows us to prove this result. We say that $X \leq_{\text{cx}} Y \iff \mathbb{E}[f(X)] \leq \mathbb{E}[f(Y)]$ for any convex $f$. Thanks to Lemma 2.3 of~\cite{BFGG20}, we have that for $p \in [0,1]$:
  \[\Ber(p) \leq_{\text{cx}} \Poi(p)\ .\]
  Since this order is preserved through convolution (Theorem 3.A.12 of~\cite{SS07}), and the fact that $\sum_{i=1}^m \Poi(p_i) \sim \Poi\Big(\sum_{i=1}^m p_i\Big)$, we have:
  \[\sum_{i=1}^m\Ber(p_i) \leq_{\text{cx}}  \Poi\Big(\sum_{i=1}^m p_i\Big)\ .\]
  Applying this result to $-\varphi$, which is convex, concludes the proof.
\end{proof}

\subsection{Negatively Associated Random Variables}
TODO

\subsection{Tail Bounds}
TODO 

\subsection{Non-Signaling Probability Distributions}
\begin{definition}
  \label{defi:nonsignaling}
  We say that a conditional probability distribution $P(a^n|x^n)$ defined on $\bigtimes_{i=1}^n\mathcal{A}_i \times \bigtimes_{i=1}^n \mathcal{X}_i$ is \emph{non-signaling} if for all $a^n, x^n, \hat{x}^n$, we have
    \[ \forall i \in [n], \sum_{\hat{a}_i}P(a_1\ldots \hat{a}_i \ldots a_n|x_1\ldots x_i \ldots x_n) = \sum_{\hat{a}_i}P(a_1\ldots \hat{a}_i \ldots a_n|x_1\ldots \hat{x}_i \ldots x_n) \ .\]
\end{definition}

\begin{definition}
  Let $P(a^n|x^n)$ a conditional probability distribution defined on $\bigtimes_{i=1}^n\mathcal{A}_i \times \bigtimes_{i=1}^n \mathcal{X}_i$ and $P'(a'^n|x'^n)$ defined on $\bigtimes_{i=1}^n\mathcal{A}'_i \times \bigtimes_{i=1}^n \mathcal{X}'_i$. We define $P \otimes P'$ the tensor product conditional probability distribution defined on $\bigtimes_{i=1}^n(\mathcal{A}_i \times \mathcal{A}'_i) \times \bigtimes_{i=1}^n (\mathcal{X}_i \times \mathcal{X}'_i)$ by $\left(P \otimes P'\right)(a_1a'_1\ldots a_na'_n|x_1x'_1\ldots x_nx'_n) := P(a^n|x^n) \cdot P'(a'^n|x'^n)$.
\end{definition}

\begin{proposition}
  \label{prop:NStensor}
  If both $P$ and $P'$ are non-signaling, then $P \otimes P'$ is non-signaling.
\end{proposition}
\begin{proof}
  Let $a^n \in \bigtimes_{j=1}^n\mathcal{A}_j$, $a'^n \in \bigtimes_{j=1}^n\mathcal{A}'_j$, $x^n \in \bigtimes_{j=1}^n \mathcal{X}_j$, $x'^n \in \bigtimes_{j=1}^n \mathcal{X}'_j$ and $\hat{x}_i \in \mathcal{X}_i$, $\hat{x}'_i \in \mathcal{X}'_i$. Using the fact that $P,P'$ are non-signaling, we have:
  \begin{equation}
    \begin{aligned}
      &\sum_{\hat{a}_i\hat{a}_i'}P(a_1a'_1\ldots \hat{a}_i\hat{a}_i' \ldots a_na'_n|x_1x'_1\ldots x_ix'_i \ldots x_nx'_n)\\
      = &\sum_{\hat{a}_i\hat{a}_i'}  P(a_1\ldots \hat{a}_i \ldots a_n|x_1\ldots x_i \ldots x_n) \cdot P'(a'_1\ldots \hat{a}'_i \ldots a'_n|x'_1\ldots x'_i \ldots x'_n)\\
      = &\left(\sum_{\hat{a}_i}  P(a_1\ldots \hat{a}_i \ldots a_n|x_1\ldots x_i \ldots x_n)\right) \cdot \left(\sum_{\hat{a}'_i}  P'(a'_1\ldots \hat{a}'_i \ldots a'_n|x'_1\ldots x'_i \ldots x'_n)\right)\\
      = &\left(\sum_{\hat{a}_i}  P(a_1\ldots \hat{a}_i \ldots a_n|x_1\ldots \hat{x}_i \ldots x_n)\right) \cdot \left(\sum_{\hat{a}'_i}  P'(a'_1\ldots \hat{a}'_i \ldots a'_n|x'_1\ldots \hat{x}'_i \ldots x'_n)\right)\\
      = &\sum_{\hat{a}_i\hat{a}_i'}\left(P \otimes P'\right)(a_1a'_1\ldots \hat{a}_i\hat{a}'_i \ldots a_na'_n|x_1x'_1\ldots \hat{x}_i\hat{x}'_i \ldots x_nx'_n) \ ,
    \end{aligned}
  \end{equation}
  so $P \otimes P'$ is non-signaling.
\end{proof}

\section{Information Theory}
\subsection{Information Quantities}
In this subsection, $X,Y,Z$ will denote random variables over finite sets $\mathcal{X},\mathcal{Y},\mathcal{Z}$ and we will denote their distributions by $P_X(x) = \mathbb{P}(X=x)$, $P_{X,Y}(x,y) = \mathbb{P}(X=x,Y=y)$, $P_{X|Y}(x|y) = \mathbb{P}(X=x|Y=y), \ldots$
\begin{definition}[(Shannon) Entropy $H(X)$]
   The (Shannon) entropy $H(X)$ of $X$ is defined by:
   \[ H(X) := \mathbb{E}_X[-\log(P_X(X))] = -\sum_{x \in \mathcal{X}}P_X(x)\log(P_X(x))\ . \]
   The entropy of two random variables $X,Y$ is defined as $H(X,Y) := H((X,Y))$.
\end{definition}

\begin{definition}[Conditional Entropy $H(X|Y)$]
  The conditional entropy $H(X|Y)$ of $X$ given $Y$ is defined by:
   \begin{equation}
      \begin{aligned}
        H(X|Y) &:=  \sum_{y \in \mathcal{Y}}P_Y(y)H(X|Y=y)\\
        &=  \sum_{y \in \mathcal{Y}}P_Y(y)\left(-\sum_{x \in \mathcal{X}}P_{X|Y}(x|y)\log(P_{X|Y}(x|y)) \right)\\
        &= -\sum_{x \in \mathcal{X}, y \in \mathcal{Y}}P_{X,Y}(x,y)\log(P_{X|Y}(x|y)) \ .
        \end{aligned}
     \end{equation}
\end{definition}

\begin{definition}[Mutual Information $I(X:Y)$]
  The mutual information $I(X:Y)$ of $X$ and $Y$ is defined by:
    \[ I(X:Y) := \sum_{x \in \mathcal{X}, y \in \mathcal{Y}}P_{X,Y}(x,y)\log\left(\frac{P_{X,Y}(x,y)}{P_X(x)P_Y(y)}\right)\ .\]
\end{definition}

\begin{proposition}
Equivalently, we have:
\[ I(X:Y) = H(X) - H(X|Y) = H(Y) - H(Y|X) = H(X) + H(Y) - H(X,Y) \ .\]
\end{proposition}

\begin{definition}[Conditional Mutual Information $I(X:Y|Z)$]
  The mutual information $I(X:Y|Z)$ of $X$ and $Y$ given $Z$ is defined by:
    \begin{equation}
       \begin{aligned}
          I(X:Y|Z) &:= \sum_{z \in \mathcal{Z}}P_Z(z)I(X : Y|Z=z)\\
          &= \sum_{x \in \mathcal{X}, y \in \mathcal{Y}, z \in \mathcal{Z}}P_{X,Y,Z}(x,y,z)\log\left(\frac{P_{X,Y|Z}(x,y|z)}{P_{X|Z}(x|z)P_{Y|Z}(y|z)}\right)\ .
       \end{aligned}
     \end{equation}   
\end{definition}

\begin{definition}[Markov chain $X \rightarrow Y \rightarrow Z$]
We say that $X \rightarrow Y \rightarrow Z$ forms a Markov chain if $Z$ is conditionally independent from $X$ given $Y$, that is to say:
\[\forall x \in \mathcal{X},\forall y \in \mathcal{Y},\forall z \in \mathcal{Z}, P_{Z|X,Y}(z|x,y) = P_{Z|Y}(z|y)   \ .\]
\end{definition}

\subsection{Typical Sets}
\label{subsection:typicalSets}
We will consider the following typical sets defined in Chapter 2.5 of~\cite{GK11}: $\mathcal{T}^n_{\varepsilon}(X_1,X_2,Y)$, $\mathcal{T}^n_{\varepsilon}(X_1,X_2)$, $\mathcal{T}^n_{\varepsilon}(Y)$, $\mathcal{T}^n_{\varepsilon}(X_1|x_2^n)$, $\mathcal{T}^n_{\varepsilon}(X_2|x_1^n)$, $\mathcal{T}^n_{\varepsilon}(X_1|x_2^n,y^n)$, $\mathcal{T}^n_{\varepsilon}(X_2|x_1^n,y^n)$, $\mathcal{T}^n_{\varepsilon}(X_1,X_2|y^n)$. Recall that:
      \begin{definition}[Typical set and conditional typical set]
        We have the following definitions:
        \begin{enumerate}
        \item $\mathcal{T}^n_{\varepsilon}(X_1,X_2) := \{(x_1^n,x_2^n) : |\pi(x_1,x_2|x_1^n,x_2^n) - P_{X_1X_2}(x_1,x_2)|\leq \varepsilon P_{X_1X_2}(x_1,x_2)$ for all $(x_1,x_2) \in \mathcal{X}_1 \times \mathcal{X}_2\}$, where $\pi(x_1,x_2|x_1^n,x_2^n) := \frac{|\{i : (x_{1,i},x_{2,i}) = (x_1,x_2)\}|}{n}$. This definition generalizes for any $t$-uple of variables.
    \item $\forall y^n \in \mathcal{T}^n_{\varepsilon}(Y), \mathcal{T}^n_{\varepsilon}(X_1,X_2|y^n) := \{ (x_1^n,x_2^n) : (x_1^n,x_2^n,y^n) \in \mathcal{T}^n_{\varepsilon}(X_1,X_2,Y) \}$
        \end{enumerate}
      \end{definition}

      A crucial property of such typical sets is the typical average lemma:

      \begin{lemma}[Typical Average Lemma~\cite{GK11}]
        Let $(x_1^n,x_2^n) \in \mathcal{T}^n_{\varepsilon}(X_1,X_2)$. Then for any nonnegative function $g$ on $\mathcal{X}_1\times\mathcal{X}_2$:
        \[ (1-\varepsilon)\mathbb{E}[g(X_1,X_2)] \leq \frac{1}{n}\sum_{i=1}^ng(x_{1,i},x_{2,i}) \leq (1+\varepsilon)\mathbb{E}[g(X_1,X_2)] \ . \]
      \end{lemma}
      
      In particular, with this tool, we can derive the following properties:
      
      \begin{proposition}[Properties of typical sets~\cite{GK11}]
        We have, among others, the following statements about typical sets:
        \begin{enumerate}
        \item $\forall (x_1^n,x_2^n) \in \mathcal{T}^n_{\varepsilon}(X_1,X_2), 2^{-n(1+\varepsilon)H(X_1,X_2)} \leq P_{X_1^nX_2^n}(x_1^n,x_2^n) \leq 2^{-n(1-\varepsilon)H(X_1,X_2)}$.
        \item $\underset{n \rightarrow +\infty}{\lim} \mathbb{P}\left((X_1^n,X_2^n) \in \mathcal{T}^n_{\varepsilon}(X_1,X_2) \right) = 1$.
        \item $|\mathcal{T}^n_{\varepsilon}(X_1,X_2)| \leq 2^{n(1+\varepsilon)H(X_1,X_2)}$.
        \item For $n$ sufficiently large, $|\mathcal{T}^n_{\varepsilon}(X_1,X_2)| \geq (1-\varepsilon)2^{n(1-\varepsilon)H(X_1,X_2)}$.
        \item If $(x_1^n,x_2^n) \in \mathcal{T}^n_{\varepsilon}(X_1,X_2)$ then $x_1^n \in \mathcal{T}^n_{\varepsilon}(X_1)$ and $x_2^n \in \mathcal{T}^n_{\varepsilon}(X_2)$.
        \item $\forall y^n \in \mathcal{T}^n_{\varepsilon}(Y), \mathcal{T}^n_{\varepsilon}(X_1,X_2|y^n) \subseteq \mathcal{T}^n_{\varepsilon}(X_1,X_2)$.
        \item $\forall (x_1^n,x_2^n,y^n) \in \mathcal{T}^n_{\varepsilon}(X_1,X_2,Y), 2^{-n(1+\varepsilon)H(X_1,X_2|Y)} \leq P_{X_1^nX_2^nY^n}(x_1^n,x_2^n|y^n) \leq 2^{-n(1-\varepsilon)H(X_1,X_2|Y)}$.
        \item $\forall y^n \in \mathcal{T}^n_{\varepsilon}(Y), |\mathcal{T}^n_{\varepsilon}(X_1,X_2|y^n)| \leq 2^{n(1+\varepsilon)H(X_1,X_2|Y)}$.
        \item For $\varepsilon' < \varepsilon$ and $n$ sufficiently large, we have that $\forall y^n \in \mathcal{T}^n_{\varepsilon'}(Y), |\mathcal{T}^n_{\varepsilon}(X_1,X_2|y^n)| \geq (1-\varepsilon)2^{n(1-\varepsilon)H(X_1,X_2|Y)}$.
        \end{enumerate}
      \end{proposition}
      \begin{proof}
        We reproduce the proof of the last statement here to emphasize on the fact that there is an $n_0$ such that for all $n \geq n_0$ and for all $y^n \in \mathcal{T}^n_{\varepsilon'}(Y)$, the property holds.

        For any $\varepsilon > \varepsilon' > 0$, let us show that there exists $n$ such that we have:
        \[ \forall y^n \in \mathcal{T}^n_{\varepsilon'}(Y), \mathbb{P}\left((X_1^n,X_2^n,y^n) \in \mathcal{T}^n_{\varepsilon}(X_1,X_2,Y) \right) \geq 1-\varepsilon \ , \]        
        where $X_1^n, X_2^n$ are drawn from the distribution $P_{X_1^nX_2^n|Y^n=y^n}$. This will imply the statement. Indeed, we have that:
        \begin{equation}
          \begin{aligned}
            \mathbb{P}\left((X_1^n,X_2^n,y^n) \in \mathcal{T}^n_{\varepsilon}(X_1,X_2,Y) \right) &= \sum_{(x_1^n,x_2^n) \in \mathcal{T}^n_{\varepsilon}(X_1,X_2|y^n)} P_{X_1^nX_2^n|Y^n}(x_1^n,x_2^n|y^n) \\
            &\leq |\mathcal{T}^n_{\varepsilon}(X_1,X_2|y^n)|2^{-n(1-\varepsilon)H(X_1,X_2|Y)} \ ,
          \end{aligned}
        \end{equation}
      
        since $P_{X_1^nX_2^n|Y^n}(x_1^n,x_2^n|y^n) \leq 2^{-n(1-\varepsilon)H(X_1,X_2|Y)}$ as $(x_1^n,x_2^n,y^n) \in \mathcal{T}^n_{\varepsilon}(X_1,X_2,Y)$. Thus, we have that $|\mathcal{T}^n_{\varepsilon}(X_1,X_2|y^n)| \geq (1-\varepsilon)2^{n(1-\varepsilon)H(X_1,X_2|Y)}$. In order to prove our result, we take the proof in Appendix 2A of~\cite{GK11}. We take $y^n \in \mathcal{T}^n_{\varepsilon'}(\mathcal{Y})$ and $(X_1^n,X_2^n) \sim P_{X_1^nX_2^n|Y^n}(x_1^n,x_2^n|y^n) = \prod_{i=1}^nP_{X_1X_2|Y}(x_{1,i},x_{2,i}|y_i)$. Applied to our choice of variables, we have that $\mathbb{P}\left((X_1^n,X_2^n,y^n) \notin \mathcal{T}^n_{\varepsilon}(X_1,X_2,Y) \right)$ is equal to:
        \begin{equation}
          \begin{aligned}
            &\mathbb{P}\left(\exists (x_1,x_2,y) : |\pi(x_1,x_2,y|X_1^n,X_2^n,y^n) - P_{X_1X_2Y}(x_1,x_2,y)| > \varepsilon P_{X_1X_2Y}(x_1,x_2,y) \right) \\
            &\leq \sum_{x_1,x_2,y} \mathbb{P}\left(|\pi(x_1,x_2,y|X_1^n,X_2^n,y^n) - P_{X_1X_2Y}(x_1,x_2,y)| > \varepsilon P_{X_1X_2Y}(x_1,x_2,y) \right) \\
            &= \sum_{x_1,x_2,y} \mathbb{P}\left(\left|\frac{\pi(x_1,x_2,y|X_1^n,X_2^n,y^n)}{P_{X_1X_2Y}(x_1,x_2,y)} - 1\right| > \varepsilon \right) \\
            &= \sum_{x_1,x_2,y} \mathbb{P}\left(\left|\frac{\pi(x_1,x_2,y|X_1^n,X_2^n,y^n)}{P_{X_1X_2|Y}(x_1,x_2|y)\pi(y|y^n)}\frac{\pi(y|y^n)}{P_Y(y)} - 1\right| > \varepsilon \right) \\
            &\leq \sum_{x_1,x_2,y} \mathbb{P}\left(\frac{\pi(x_1,x_2,y|X_1^n,X_2^n,y^n)}{\pi(y|y^n)} > \frac{1+\varepsilon}{1+\varepsilon'}P_{X_1X_2|Y}(x_1,x_2|y) \right)\\
            &+ \sum_{x_1,x_2,y} \mathbb{P}\left(\frac{\pi(x_1,x_2,y|X_1^n,X_2^n,y^n)}{\pi(y|y^n)} < \frac{1-\varepsilon}{1-\varepsilon'}P_{X_1X_2|Y}(x_1,x_2|y) \right) \ ,
          \end{aligned}
        \end{equation}
        since $y^n \in  \mathcal{T}^n_{\varepsilon'}(\mathcal{Y})$ and thus $1-\varepsilon' \leq \frac{\pi(y|y^n)}{P_Y(y)} \leq 1+\varepsilon'$. However, since $\varepsilon' < \varepsilon$, we have $\frac{1+\varepsilon}{1+\varepsilon'} > 1$ and $\frac{1-\varepsilon}{1-\varepsilon'} < 1$. We will show that for all $x_1,x_2,y$ with $P_Y(y) > 0$, we have $\frac{\pi(x_1,x_2,y|X_1^n,X_2^n,y^n)}{\pi(y|y^n)} \underset{n \rightarrow +\infty}{\rightarrow} P_{X_1X_2|Y}(x_1,x_2|y)$ in probability, with a convergence rate independent from $y^n \in \mathcal{T}^n_{\varepsilon'}(Y)$, which will be enough to conclude the proof.

        Let us fix some $x_1,x_2,y$ with $P_Y(y) > 0$. Since $y^n \in \mathcal{T}^n_{\varepsilon'}(Y)$, we have in particular $(1-\varepsilon')P_Y(y) \leq \pi(y|y^n) \leq (1+\varepsilon')P_Y(y)$. Thus $N := |\{i:y_i=y\}| = n\pi(y|y^n) \geq (1-\varepsilon')P_Y(y)n$. Then we have:
        \[ \frac{\pi(x_1,x_2,y|X_1^n,X_2^n,y^n)}{\pi(y|y^n)} = \frac{1}{N}\sum_{i \in S}Z_i \text{ with }Z_i := \mathbbm{1}_{(X_{1,i},X_{2,i})=(x_1,x_2)} \text{ and } S := \{i:y_i=y\}\ . \]

        Thus, all $Z_i$ with $i \in S$ are independent and follow the same law:      
          \[ Z_i := \begin{cases}
            1 & \text{ with probability } P_{X_1X_2|Y}(x_1,x_2|y) \\
            0 & \text{ otherwise}
          \end{cases}
          \]


          Furthermore, we have $\mathbb{E}[Z_i] = P_{X_1X_2|Y}(x_1,x_2|y)$, and all $Z_i$ have the same variance $\sigma_{x_1,x_2|y}^2 < +\infty$ (depending only on $X_1,X_2,Y,x_1,x_2,y$). Thus we can apply Chebyshev inequality:
          \[ \mathbb{P}\left( \left|\frac{1}{N}\sum_{i \in S} Z_i - P_{X_1X_2|Y}(x_1,x_2|y) \right| \geq \eta \right) \leq \frac{\sigma_{x_1,x_2|y}^2}{N\eta^2} \ .\]

          However, since $N \geq (1-\varepsilon')P_Y(y)n$, we get:
          \[ \mathbb{P}\left( \left|\frac{\pi(x_1,x_2,y|X_1^n,X_2^n,y^n)}{\pi(y|y^n)} - P_{X_1X_2|Y}(x_1,x_2|y) \right| \geq \eta \right) \leq \frac{\sigma_{x_1,x_2|y}^2}{\eta^2(1-\varepsilon')P_Y(y)n} \underset{n \rightarrow +\infty}{\rightarrow} 0 \ .\]

          Thus, we have $\frac{\pi(x_1,x_2,y|X_1^n,X_2^n,y^n)}{\pi(y|y^n)} \underset{n \rightarrow +\infty}{\rightarrow} P_{X_1X_2|Y}(x_1,x_2|y)$ in probability with a convergence rate independent from $y^n \in \mathcal{T}^n_{\varepsilon'}(Y)$.
      \end{proof}

\subsection{Channels}
Formally, a channel $W$ is a conditional probability distribution depending on $n$ inputs belonging to $\mathcal{X}^n$ and $m$ outputs belonging to $\mathcal{Y}^m$, so $W := \left(W(y^m|x^n)\right)_{x^n \in \mathcal{X}^n, y^m \in \mathcal{Y}^m}$ with:
  \[ \forall x^n,y^m, W(y^m|x^n) \geq 0 \text{ and } \forall x^n, \sum_{y^m \in \mathcal{Y}^m} W(y^m|x^n) = 1 \ . \]
We will denote such a channel by $W : \mathcal{X}^n \rightarrow \mathcal{Y}^m$. The tensor product of two channels $W: \mathcal{X}^n \rightarrow \mathcal{Y}^m$ and $W': \mathcal{X}'^n \rightarrow \mathcal{Y}^m$ is denoted by $W \otimes W' : (\mathcal{X}^n \times \mathcal{X}^{\prime n}) \to  (\mathcal{Y}^n \times \mathcal{Y}^{\prime n})$ and defined by:
\[ (W \otimes W')(y^my^{\prime m}|x^nx^{\prime n}) := W(y^m|x^n) \cdot W'^{\prime m}|x^{\prime n}) \ .\]
We denote by $W^{\otimes n}$ the $n$th tensor product of $W$, i.e. $W^{\otimes n} = W \otimes \ldots \otimes W$ with $n$ occurences of $W$.

When $n=m=1$, we will speak of regular or one-way channels. When $n>1,m=1$, we will speak of multiple-access channels. When $n=1,m>1$, we will speak of broadcast channels. If both $n,m$ are greater than $1$, we will speak of interference channels. More specifically, in this thesis, we will focus on the cases of $n=2,m=1$ and $n=1,m=2$, which are at the core of the specificity of network channels.

\subsection{Capacity Regions}
\begin{definition}[Capacity Region ${\mathcal{C}[\mathrm{S}](W)}$ for a success probability $\mathrm{S}(W,k_1,k_2)$]
  \label{defi:generalCapacityRegion}
  A rate pair $(R_1,R_2)$ is $\mathrm{S}$-achievable (for the channel $W$) if:
  \[ \underset{n \rightarrow +\infty}{\lim} \mathrm{S}(W^{\otimes n},\ceil{2^{R_1n}},\ceil{2^{R_2n}}) = 1 \ . \]
  We define the $\mathrm{S}$-capacity region $\mathcal{C}[\mathrm{S}](W)$ as the closure of the set of all achievable rate pairs (for the channel $W$).
\end{definition}

\begin{definition}[Zero-Error Capacity Region ${\mathcal{C}_0[\mathrm{S}](W)}$ for a success probability $\mathrm{S}(W,k_1,k_2)$]
  A rate pair $(R_1,R_2)$ is $\mathrm{S}$-achievable with zero-error (for the channel $W$) if:
  \[ \exists n_0  \in \mathbb{N}^*, \forall n \geq n_0, \mathrm{S}(W^{\otimes n},\ceil{2^{R_1n}},\ceil{2^{R_2n}}) = 1 \ . \]
  We define the zero-error $\mathrm{S}$-capacity region $\mathcal{C}[\mathrm{S}](W)$ as the closure of the set of all achievable rate pairs with zero-error (for the channel $W$).
\end{definition}

\begin{proposition}[Time-sharing]
  \label{prop:timesharing}
  If the success probability $\mathrm{S}$ satisfies:
  \[ \forall W,W', \forall k_1,k_2,k_1',k_2', \mathrm{S}(W \otimes W',k_1k_1',k_2k_2') \geq \mathrm{S}(W,k_1,k_2) \cdot \mathrm{S}(W',k_1',k_2') \ , \]
  then for all channels $W$, $\mathcal{C}[\mathrm{S}](W)$ and $\mathcal{C}_0[\mathrm{S}](W)$ are convex.
\end{proposition}
\begin{proof}
  Let $(R_1,R_2)$ and $(R_1',R_2')$, two pairs of $\mathrm{S}$-achievable rational rates for $W$, i.e.:
    \[ \mathrm{S}(W^{\otimes n},\ceil{2^{R_1n}},\ceil{2^{R_2n}}) \underset{n \rightarrow +\infty}{\rightarrow} 1 \text{ and }\mathrm{S}(W^{\otimes n},\ceil{2^{R_1'n}},\ceil{2^{R_2'n}}) \underset{n \rightarrow +\infty}{\rightarrow} 1 \ . \]
    Let $\lambda \in (0,1)$ rational and define $R_{\lambda,i} := \lambda \cdot R_i + (1-\lambda) \cdot R'_i$, let us show that $(R_{\lambda,1},R_{\lambda,2})$ is achievable with non-signaling assistance. Let us call respectively $k_i:=2^{R_i}, k_i' := 2^{R_i'}, k_{\lambda,i} := 2^{R_{\lambda,i}} = k_i^{\lambda}\cdot k_i^{(1-\lambda)}$.

    We have $R_{\lambda,i}n = \lambda \cdot R_in + (1-\lambda) \cdot R_i'n = (\lambda n) \cdot R_i + (1-\lambda)n \cdot R_i'$. This is the idea of \emph{time-sharing}: for $\lambda n$ copies of the channel, we use the strategy with rate $(R_1,R_2)$ and for the $(1-\lambda)n$ other copies of the channel, we use the strategy with rate $(R_1',R_2')$. There exists some $n$ such that $\lambda n,(1-\lambda)n,\lambda n R_i,(1-\lambda)n R_i'$ are integers, since everything is rational. This implies that $k_i^{\lambda n},k_i^{\prime (1-\lambda)n},k_{\lambda,i}^n$ are integers. Thus, by hypothesis, we have that $\mathrm{S}(W^{\otimes n},k^n_{\lambda,1},k^n_{\lambda, 2})$ is larger than or equal to:
\[ \mathrm{S}(W^{\otimes (\lambda n)}, k_1^{\lambda n}, k_2^{\lambda n}) \cdot \mathrm{S}(W^{\otimes ((1-\lambda) n)}, k_1^{\prime (1-\lambda) n}, k_2^{\prime (1-\lambda) n}) \underset{n \rightarrow +\infty}{\rightarrow} 1 \cdot 1 = 1  \ .\]

Thus, since $\mathrm{S}(W^{\otimes n},k^n_{\lambda,1},k^n_{\lambda,2}) \leq 1$, we get the result $\mathrm{S}(W^{\otimes n},k^n_{\lambda,1},k^n_{\lambda,2}) \underset{n \rightarrow +\infty}{\rightarrow} 1$, so $(R_{\lambda,1},R_{\lambda,2})$ is $\mathrm{S}$-achievable for the channel $W$. Finally, since $\mathcal{C}[\mathrm{S}](W)$ is defined as the closure of $\mathrm{S}$-achievable rates for the channel $W$, we get that $\mathcal{C}[\mathrm{S}](W)$ is convex.

For zero-error capacity regions, since by hypothesis there exists ranks $n_0,n_0'$ such that $\mathrm{S}(W^{\otimes n},\ceil{2^{R_1n}},\ceil{2^{R_2n}}) = 1$ for $n \geq n_0$ and $\mathrm{S}(W^{\otimes n},\ceil{2^{R_1'n}},\ceil{2^{R_2'n}}) = 1$ for $n \geq n_0'$, then in particular we get that for $\lambda n \geq n_0$ and $(1-\lambda) n \geq n_0'$ that $\mathrm{S}(W^{\otimes n},k^n_{\lambda,1},k^n_{\lambda, 2})$ is larger than or equal to:
\[ \mathrm{S}(W^{\otimes (\lambda n)}, k_1^{\lambda n}, k_2^{\lambda n}) \cdot \mathrm{S}(W^{\otimes ((1-\lambda) n)}, k_1^{\prime (1-\lambda) n}, k_2^{\prime (1-\lambda) n}) = 1 \ .\]
It is in particular true for $n \geq n_0 := \max\left(\ceil{\frac{n_0'}{1-\lambda}},\ceil{\frac{n_0}{\lambda}}\right)$. As $\mathrm{S}^{\mathrm{NS}}(W^{\otimes n},k^n_{\lambda,1},k^n_{\lambda,2}) \leq 1$, we have  for all $n \geq n_0$ that $\mathrm{S}^{\mathrm{NS}}(W^{\otimes n},k^n_{\lambda,1},k^n_{\lambda,2}) = 1$, i.e. $(R_{\lambda,1},R_{\lambda,2})$ is $\mathrm{S}$-achievable with zero-error for the channel $W$.  Finally, since $\mathcal{C}_0[\mathrm{S}](W)$ is defined as the closure of $\mathrm{S}$-achievable rates with zero-error for the channel $W$, we get that $\mathcal{C}_0[\mathrm{S}](W)$ is convex.
\end{proof}

\section{Complexity?}
TODO

\section{Graphs?}
TODO

\section{Groups?}
TODO 

\section{Channel Coding for One-Way Channels}
\label{subsection:onewaychannelcoding} 
We recall here the main results from~\cite{BF18} on channel coding for one-way channels, i.e. with one input and one output. Let us first recall the definition of the maximum success probability $\mathrm{S}(W,k)$ of transmitting $k$ messages using the channel $W$:
\begin{equation}
  \begin{aligned}
    \mathrm{S}(W,k) := &&\underset{e,d}{\maxi} &&& \frac{1}{k} \sum_{i,x,y} W(y|x)e(x|i)d(i|y)\\
    &&\st &&& \sum_{x \in \mathcal{X}} e(x|i) = 1, \forall i \in [k]\\
    &&&&& \sum_{j \in [k]} d(j|y) = 1, \forall y \in \mathcal{Y}\\
    &&&&& e(x|i), d(j|y) \geq 0
  \end{aligned}
\end{equation}

Then, the following characterization of $\mathrm{S}(W,k)$ can be derived:

\begin{proposition}[Proposition 2.1 of~\cite{BF18}]
  $\mathrm{S}(W,k) = \frac{1}{k} \underset{S \subseteq X: |S| \leq k}{\max} f_W(S)$ with $f_W(S) := \sum_{y \in Y} \max_{x \in S} W(y|x)$.
\end{proposition}

One can consider non-signaling assistance shared between the sender and the receiver, which leads to the following maximum success probability:
\begin{equation}
  \begin{aligned}
    \mathrm{S}^{\mathrm{NS}}(W,k) := &&\underset{P}{\maxi} &&& \frac{1}{k} \sum_{i,x,y} W(y|x)P(xi|iy)\\
    &&\st &&& \sum_{x} P(xj|iy) = \sum_{x} P(xj|i'y)\\
    &&&&& \sum_{j} P(xj|iy) = \sum_{j} P(xj|iy')\\
    &&&&& \sum_{x,j} P(xj|iy) = 1\\
    &&&&& P(xj|iy) \geq 0
  \end{aligned}
\end{equation}

A symmetrization can also be done to simplify the expression of the linear program defining $\mathrm{S}^{\mathrm{NS}}(W,k)$:
\begin{proposition}[Appendix A of~\cite{BF18}]
  \label{prop:NSonewayLP}
  \begin{equation}
    \begin{aligned}
      \mathrm{S}^{\mathrm{NS}}(W,k) = &&\underset{r,p}{\maxi} &&& \frac{1}{k} \sum_{x,y} W(y|x)r_{x,y}\\
      &&\st &&& \sum_{x} r_{x,y} = 1\\
      &&&&& \sum_{x} p_{x} = k\\
      &&&&& 0 \leq r_{x,y} \leq p_{x}
    \end{aligned}
  \end{equation}
\end{proposition}

Finally, the main tool we will use from~\cite{BF18} is the following random coding technique, which describes how to find a classical code with a success probability close to the non-signaling assisted one:
\begin{theorem}[Theorem 3.1 of~\cite{BF18}]
  \label{theo:RandomCoding}
  Given a solution $r,p$ of the program computing $\mathrm{S}^{\mathrm{NS}}(W,k)$, we have that:
  \[ \mathbb{E}_S\left[\frac{f_W(S)}{\ell}\right] \geq \frac{k}{\ell}\left(1-\left(1-\frac{1}{k}\right)^{\ell}\right) \cdot \frac{1}{k} \sum_{x,y} W(y|x)r_{x,y} , \]
  for the multiset $S$ obtained by choosing $\ell$ elements of $\mathcal{X}$ independently according to the distribution $\left(\frac{p_{x}}{k}\right)_{x \in \mathcal{X}}$.
\end{theorem}
