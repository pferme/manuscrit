We have introduced the $\varphi$-\textsc{MaxCoverage} problem where having $c$ copies of element $a$ gives a value $\varphi(c)$. We have shown that when $\varphi$ is normalized, nondecreasing and concave, we can obtain an approximation guarantee given by the \emph{Poisson concavity ratio} $\alpha_{\varphi} := \min_{x \in \mathbb{N}} \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(\mathbb{E}[\Poi(x)])}$ and we showed it is tight for sublinear functions $\varphi$. The Poisson concavity ratio strictly beats the bound one gets when using the notion of curvature submodular functions, except in very special cases such as \textsc{MaxCoverage} where the two bounds are equal. An interesting open question is whether there exists combinatorial algorithms that achieve this approximation ratio. As mentioned in~\cite{BFGG20}, for the $\ell$-\textsc{MultiCoverage} with $\ell \geq 2$, which is the special case where $\varphi(x) = \min(x,\ell)$, the simple greedy algorithm only gives a $1 - e^{-1}$ approximation ratio, which is strictly less than the ratio $\alpha_{\varphi} = 1-\frac{\ell^{\ell}e^{-\ell}}{\ell!}$ in that case. Also, for any geometrically dominant vector $w=(\varphi(i+1)-\varphi(i))_{i \in \mathbb{N}}$ which is not $p$-geometric, such as \textsc{Proportional Approval Voting}, the greedy algorithm achieves an approximation ratio which is strictly less than $\alpha_{\varphi}$ (see Theorem 18 of~\cite{DMMS20}). Another open question is whether the hardness result remains true even when $\varphi(n) \not= o(n)$. A good example is given by $\varphi(0)=0$ and $\varphi(1+t) = 1 + (1-c)t$ with $c \in (0,1)$. We know that the problem is hard for $c=1$ but easy for $c=0$. One can show that the approximation ratio achieved by our algorithm is $\alpha_{\varphi} = 1 - \frac{c}{e}$ in that case (which is the same approximation ratio obtained from the curvature in~\cite{SVW17}), but the tightness of this approximation ratio remains open.

We have addressed the computational complexity of the multiple-access channel coding problem, by showing that no polynomial-time constant approximation exists if some complexity hypothesis on random clauses is true. Then, we studied the impact of non-signaling assistance on the capacity of multiple-access channels. We have developed an efficient linear program computing the success probability of the best non-signaling assisted code for a finite number of copies of a multiple-access channel. In particular, this gives lower bounds on the zero-error non-signaling assisted capacity of multiple-access channels. Applied to the binary adder channel, these results were used to prove that a sum-rate of $\frac{\log_2(72)}{4} \simeq 1.5425$ can be reached with zero error, which beats the maximum classical sum-rate capacity of $\frac{3}{2}$. For noisy channels, we have developed a technique giving lower bounds through the use of concatenated codes. Applied to the noisy binary adder channel, this technique was used to show that non-signaling assistance still improves the sum-rate capacity. We have also found an outer bound on the non-signaling assisted capacity region through a relaxed notion of non-signaling assistance, whose capacity region was characterized by a single-letter formula. Finally, we have shown that independent non-signaling assistance does not change the capacity region. Our results suggest that quantum entanglement may also increase the capacity of such channels. However, even for the binary adder channel, this question remains open. One could also ask if such efficient methods to compute the best non-signaling assisted codes can be extended to Gaussian multiple-access channels. Finally, establishing a single-letter formula for the non-signaling assisted capacity of multiple-access channels is the main open question left here. It remains open even for the binary adder channel. Proving that non-signaling assistance and relaxed non-signaling assistance coincide asymptotically would directly answer this question and show that the capacity region is described in Theorem~\ref{theo:CharaNSrelaxed}.

We have studied several algorithmic aspects and non-signaling assisted capacity regions of broadcast channels. We have shown that when non-signaling assistance is shared only between the decoders, the capacity region does not change. For the class of deterministic broadcast channels, we have described a $(1-e^{-1})^2$-approximation algorithm running in polynomial time, and we have shown that the capacity region for that class is the same with or without non-signaling assistance. Finally, we have shown that in the value query model, we cannot achieve a better approximation ratio than $\Omega\left(\frac{1}{\sqrt{m}}\right)$ in polynomial time for the general broadcast channel coding problem, with $m$ the size of one of the outputs of the channel. Our results suggest that non-signaling assistance could improve the capacity region of general broadcast channels, which is left as a major open question. An intermediate result would be to show that it is \textrm{NP}-hard to approximate the broadcast channel coding problem within any constant, strengthening our hardness result without relying on the value query model. Finally, one could also try to develop approximations algorithms for other sub-classes of broadcast channels, such as semi-deterministic or degraded ones. This could be a crucial step towards showing that the capacity region for those classes is the same with or without non-signaling assistance.
 
