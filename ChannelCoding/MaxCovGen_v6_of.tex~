%\documentclass[6pt]{article}
\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{multicol}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{bm}
\usepackage{bbm}
\usepackage{xparse}
\usepackage{physics}
\usepackage{empheq}
\usepackage{url}
\usepackage{hyperref}
\usepackage[affil-it]{authblk}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{quotes,angles,calc}
\usepackage{rotating}
\usepackage{graphicx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output

\usepackage{pgfplots}
\pgfplotsset{compat = newest}

%\usepackage{wasysym}
%\usepackage{listings}
%\usepackage{moreverb}

%\usepackage[top=3cm,bottom=2cm,right=1cm,left=1cm]{geometry}
\usepackage[top=3cm,bottom=2cm,right=2cm,left=2cm]{geometry}

\theoremstyle{definition}
\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{defi}[theo]{Definition}
\newtheorem{conj}[theo]{Conjecture}

\newtheorem{theo*}{Theorem}

\theoremstyle{remark}
\newtheorem*{rk}{Remark}

\DeclareMathOperator{\Poi}{\text{Poi}}
\DeclareMathOperator{\Ber}{\text{Ber}}
\DeclareMathOperator{\Bin}{\text{Bin}}
\DeclareMathOperator{\maxi}{\text{maximize}}
\DeclareMathOperator{\mini}{\text{minimize}}
\DeclareMathOperator{\st}{\text{subject to}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiterX\set[1]\lbrace\rbrace{\def\given{\;\delimsize\vert\;}#1}

\newcommand{\OF}[1]{\textcolor{blue}{OF: #1}}
\newcommand{\Sid}[1]{\textcolor{blue}{Sid: #1}}

\colorlet{darkgreen}{green!40!black}

\title{\bfseries Tight Approximation Guarantees for \\ Concave Coverage Problems}
%\title{\huge{Tight Approximation Bounds for nondecreasing concave $\varphi$-\textsc{MaxCoverage}}}
%\space\space\space\space\space
\author{Siddharth Barman\footnote{Indian Institute of Science, Bangalore, India. \href{mailto:barman@iisc.ac.in}{\texttt{barman@iisc.ac.in}}} \qquad Omar Fawzi\footnote{Univ Lyon, ENS Lyon, UCBL, CNRS, LIP, F-69342, Lyon Cedex 07, France. \href{mailto:omar.fawzi@ens-lyon.fr}{\texttt{omar.fawzi@ens-lyon.fr}}} \qquad Paul FermÃ©\footnote{Univ Lyon, ENS Lyon, UCBL, CNRS, LIP, F-69342, Lyon Cedex 07, France. \href{mailto:paul.ferme@ens-lyon.fr}{\texttt{paul.ferme@ens-lyon.fr}}}
}
\date{}



\begin{document}

\maketitle

\begin{abstract}
In the maximum coverage problem, we are given subsets $T_1, \ldots, T_m$ of a universe $[n]$ along with an integer $k$ and the objective is to find a subset $S \subseteq [m]$ of size $k$ that maximizes $C(S) := \abs{\bigcup_{i \in S} T_i}$. It is a classic result that the greedy algorithm for this problem achieves an optimal approximation ratio of $(1-e^{-1})$.

% and there is a matching inapproximability result. 
%We note that in the maximum coverage problem even if an element $a \in [n]$ is covered by multiple sets, it still contributes only one to the maximization objective. 
In this work we consider a generalization of this problem wherein an element $a$ can contribute by an amount that depends on the number of times it is covered. Given a concave, nondecreasing function $\varphi$, we define $C^{\varphi}(S) \coloneqq \sum_{a \in [n]}w_a\varphi(\abs{S}_a)$, where $\abs{S}_a = \abs{\set{i \in S : a \in T_i}}$. The standard maximum coverage problem corresponds to taking $\varphi(j) = \min\{j,1\}$. For any such $\varphi$, we provide an efficient algorithm that achieves an approximation ratio equal to the \emph{Poisson concavity ratio} of $\varphi$, defined by $\alpha_{\varphi} := \inf_{x \in \mathbb{N}} \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(\mathbb{E}[\Poi(x)])}$. Complementing this approximation guarantee, we establish a matching NP-hardness result when $\varphi$ grows in a sublinear way. 

As special cases, we improve the result of~\cite{BFGG20} about maximum multi-coverage, that was based on the unique games conjecture, and we recover the result of~\cite{DDMS20} on multi-winner approval-based voting for geometrically dominant rules. Our result goes beyond these special cases and we illustrate it with applications to distributed resource allocation problems, welfare maximization problems and approval-based voting for general rules. 
%\OF{to adapt (after finishing the rest)}
%\OF{Add a sentence here about new applications}
%Our algorithmic result addresses also the problem of utility design for distributed resource allocation \cite{PM19}, and as a special case the vehicle-target assignment problem \cite{Murphey00}. Both of these problems are special cases of maximizing $C^{\varphi}$, under matroid constraints, and for the latter, our analytic expression of $\alpha_{\varphi}$ improves upon the previously-known numerical approximation ratios.
\end{abstract}

%\begin{multicols}{2}



\input{intro}



\section{Approximation Algorithm for $\varphi$-\textsc{MaxCoverage}}
\label{section:ApproxAlgo}

Fix a function $\varphi : \mathbb{N} \to \mathbb{R}_+$ that is normalized, nondecreasing and concave. The $\varphi$-\textsc{MaxCoverage} problem is defined as follows. The input to the problem is given by positive integers $n,m,t$ and $m$ subsets $T_{1}, \dots, T_{m}$ of the set $[n]$ (described as characteristic vectors), the weights $w_a \in \mathbb{N}^*$ for $a \in [n]$ (described as a bitstring of length $t$), as well as an integer $k \in \{1, \dots, m\}$.  The output is a subset $S \subseteq [m]$ of size exactly $k$ that maximizes $C^{\varphi}(S) = \sum_{a \in [n]} w_a \varphi(|S|_a)$, where $|S|_{a} = |\{ i \in S : a \in T_i\}|$.

Note that the input to this problem can be specified using $n(m+t) + O(\log nmt)$ bits. To reduce the number of parameters, we will assume that $t$ is polynomial in $n$ and $m$, so that a polynomial time algorithm for this problem means an algorithm that runs in time polynomial in $n$ and $m$. The counting function $\varphi$ is fixed and does not depend on the instance of the problem, but for a given instance the problem only depends on the values $\varphi(0), \varphi(1), \dots, \varphi(m)$. We assume that we have black box access to $\varphi$ and to ensure that all the algorithms run in polynomial time, we assume that $\varphi(j)$ can be described with a number of bits that is polynomial in $j$ and that this description can be computed in polynomial time.
%  The algorithm we construct has black box access to $\varphi$, i.e., we can query $\varphi$ at any integer and we assume it takes a unit computational cost. 

We now describe the approximation algorithm for $\varphi$-\textsc{MaxCoverage} that we analyze. As described above, we follow the standard relax and round strategy, as in~\cite{BFGG20}.
%We follow the same strategy as the one developed in \cite{BFGG20}. The algorithm we analyze is composed of two steps (relax and round): 
First, we define a natural convex relaxation.
\begin{defi}[Relaxed program]
  \begin{equation}
    \begin{aligned}
      &\maxi&& \sum_{a \in [n]} w_ac_a \\
      &\st&& c_a \leq \varphi(\abs{x}_a), \forall a \in [n],\text{ with }\abs{x}_a := \sum_{i \in [m] : a \in T_i} x_i\\
      &&& 0 \leq x_i \leq 1, \forall i \in [m]\\
      &&& \sum_{i=1}^m x_i = k \ .
    \end{aligned}
  \end{equation}
  \label{defi:relaxedProg}
\end{defi}


As previously mentioned, $\varphi$ is defined on $\mathbb{R}_+$ by extending it in a piecewise linear fashion on non-integral points.
%Strictly speaking, this program is not a linear program since $\varphi$ is not a linear function. 
As such, the constraint $c_a \leq \varphi(\abs{x}_a)$ is equivalent to $m$ linear constraints. In fact, we can define $\varphi_j$ to be the linear function $\varphi_j(t) = (\varphi(j) - \varphi(j-1)) t  - (j-1) \varphi(j) + j \varphi(j-1)$ for $j \in [m]$. Since $\varphi$ is concave, we have that for all $t \in [0, m]$, $\varphi(t) = \min_{j \in [m]} \varphi_j(t)$. As such, the constraint $c_a \leq \varphi(\abs{x}_a)$ is equivalent to $c_a \leq \varphi_j(\abs{x}_a)$ for all $j\in [m]$ and so the program~\ref{defi:relaxedProg} is a linear program. Overall there are $n+m$ variables and $(n+1)m + 1$ linear constraints, and by assumptions all the coefficients can be described using a number of bits that is polynomial in $n$ and $m$. Hence an optimal solution of this linear program can be found in polynomial time.

%on nonintegral values, we have that $\forall j \in [m], \exists p_j \text{ linear }, \forall x \in [j-1,j], \varphi(x) = p_j(x)$. Furthermore since $\varphi$ is concave, we have that $\forall j \in [m], \forall x \geq 0, \varphi(x) \leq p_j(x)$. Thus we can replace one constraint $c_a \leq \varphi(y)$ by the $m$ linear constraints $c_a \leq p_j(y), \forall k \in [m]$, so the program is indeed linear. 

Also observe that this program~\ref{defi:relaxedProg} is indeed a relaxation of the $\varphi$-\textsc{MaxCoverage} problem. To see this, given a set $S$ of size $k$, consider the characteristic vector $x \in \{0,1\}^m$ defined by $x_i = 1$ if and only if $i \in S$. Then for all $a \in [n]$, we can set $c_a = \varphi(\abs{x}_a) = \varphi(\abs{S}_a)$, and we get an objective value of $\sum_{a \in [n]}w_a\varphi(\abs{S}_a)$ which is exactly $C^{\varphi}(S)$. When solving the program~\ref{defi:relaxedProg}, we get an optimal $x^* \in [0,1]^m$ which is in general not integral. Next, we describe a method to round it to an integral vector $x^{\text{int}} \in \set{0,1}^m$.


% which satisfies $\sum_{i=1}^m x^*_i = k$. 


%Note that if the vector $x^*$ is integral, i.e., $x^* \in \{0,1\}^m$, then we have that $\varphi\Big(\sum_{i \in [m] : a \in T_i} x^*_i\Big) = \varphi(\abs{S}_a)$ with $S = \{i \in [m] : x^*_i = 1\}$. For an optimal solution, we would have $c_a =  \varphi(\abs{S}_a)$ since $w_a > 0$. Thus the objective value we get for $x^*$ is $\sum_{a \in [n]}w_a\varphi(\abs{S}_a) = C^{\varphi}(S)$. So it is indeed the same problem when $x$ is an integral solution.
%In order to find an integral vector $x^{\text{int}} \in \set{0,1}^m$, we apply pipage rounding to $x^*$.

%The second step is to use pipage rounding to find an integral vector $x^{\text{int}} \in \set{0,1}^m$ with the property that $\sum_{i=1}^m x^{\text{int}}_i = k$. This defines naturally a size $k$ set $S := \set{i \in [m] : x^{\text{int}}_i = 1}$, which we identify with $x^{\text{int}}$, and will be the set output by the algorithm. These two steps are detailed below.

%\paragraph{Step 1. Solve the LP relaxation:} Specifically, we consider the following linear programming relaxation of the $\varphi$-\textsc{MaxCoverage} problem under cardinality $k$ constraint.

\paragraph{Rounding} 
For a submodular function $f : \{0,1\}^m \to \mathbb{R}$ , one can use pipage rounding~\cite{AS04, Vondrak07, CCPV11} to transform, in polynomial time, any fractional solution $x \in [0,1]^m$ satisfying $\sum_{i=1}^m x_i = k$ into an integral vector $x^{\text{int}} \in \set{0,1}^m$ such that $\sum_{i=1}^m x^{\text{int}}_i = k$ and $F(x^{\text{int}}) \geq F(x)$, where $F$ corresponds to the multilinear extension of $f$, provided that $F(x)$ is computable in polynomial time for a given $x$; see e.g.,~\cite[Lemma 3.4]{Vondrak07}. The multilinear extension $F : [0, 1]^m \rightarrow \mathbb{R}$ of $f$ is defined by $F(x_1,\ldots,x_m): = \mathbb{E}[f(X_1,\ldots,X_m)]$ where $X_i$ are independent random variables with $X_i \sim \Ber(x_i)$, i.e., $X_i \in \set{0,1}$ with $\mathbb{P}(X_i = 1) = x_i$. Note that $F(x) = f(x)$ for an integral vector $x \in \{0,1\}^m$.

We apply this strategy to $C^{\varphi}$, which is shown to be submodular in Proposition \ref{prop:SubCurv}, and the solution $x^*$ of the LP relaxation \ref{defi:relaxedProg}. Note that overall the algorithm is polynomial time, since here $F(x)$ is computable in polynomial time for a given $x$ (see Proposition \ref{prop:Fpoly}). We now analyze the value returned by the algorithm. Using the property of pipage rounding, with the notation $X = (X_1,\ldots,X_m)$ and $\Ber(x) = (\Ber(x_1),\ldots,\Ber(x_m))$, we get
%
%\paragraph{Step 2. Round the fractional, optimal solution:} We round the computed fractional solution $x^*$ by considering the \emph{multilinear extension} of the objective, and applying pipage rounding \cite{AS04, Vondrak07, CCPV11} on it. Formally, given any function $f : \set{0,1}^m \rightarrow \mathbb{R}$, one can define the multilinear extension  by $F(x_1,\ldots,x_m): = \mathbb{E}[f(X_1,\ldots,X_m)]$ where $X_i$ are independent random variables with $X_i \sim \Ber(x_i)$, ie. $X_i \in \set{0,1}$ with $\mathbb{P}(X_i = 1) = x_i$.

\[
C^{\varphi}(x^{\text{int}}) = \mathbb{E}_{X \sim \Ber(x^{\text{int}})}[C^{\varphi}(X)] \geq \mathbb{E}_{X \sim \Ber(x^*)}[C^{\varphi}(X)] \ .
\]
Then it suffices to relate $\mathbb{E}_{X \sim \Ber(x^*)}[C^{\varphi}(X)]$ to the optimal value of the LP relaxation~\ref{defi:relaxedProg}, which can only be larger than the optimal value of the $\varphi$-\textsc{MaxCoverage} problem. 
%than the integral LP, since we maximize over a larger space.

\begin{theo*}
  Let $x,c$ be a feasible solution of the program \ref{defi:relaxedProg} and $X \sim \Ber(x)$. Recalling the definition of $\alpha_{\varphi}$ and $\alpha_{\varphi}(j)$ from~\eqref{eq:def-alpha-varphi}, we have
  \[\mathbb{E}_{X \sim \Ber(x)}[C^{\varphi}(X)] \geq \left(\min_{j \in [m]} \alpha_{\varphi}(j)\right) \sum_{a \in [n]} w_ac_a\]
  In particular, this implies that the described polynomial time algorithm has an approximation ratio of $\alpha_{\varphi}$:
  \[C^{\varphi}(x^{\text{int}}) \geq \alpha_{\varphi} \sum_{a \in [n]} w_ac^*_a \geq \alpha_{\varphi} \max_{S \subseteq [m] : \abs{S} = k} C^{\varphi}(S)\ .\]
  \label{theo:AlgoCard}
\end{theo*}

In order to prove this theorem, we need the following lemma:

\begin{lem}
    For $\varphi$ concave, and $p \in [0,1]^m$, we have:
    \[\mathbb{E}\Big[\varphi\Big(\sum_{i=1}^m\Ber(p_i)\Big)\Big] \geq \mathbb{E}\Big[\varphi\Big(\Poi\Big(\sum_{i=1}^m p_i\Big)\Big)\Big]\]
  \label{lem:ConvexOrder}
\end{lem}

\begin{proof}
  The notion of \emph{convex order} discussed in \cite{StochasticOrders} allows us to prove this result. We say that $X \leq_{\text{cx}} Y \iff \mathbb{E}[f(X)] \leq \mathbb{E}[f(Y)]$ for any convex $f$. Thanks to Lemma 2.3 of \cite{BFGG20}, we have that for $p \in [0,1]$:
  \[\Ber(p) \leq_{\text{cx}} \Poi(p)\]
  Since this order is preserved through convolution (Theorem 3.A.12 of \cite{StochasticOrders}), and the fact that $\sum_{i=1}^m \Poi(p_i) \sim \Poi\Big(\sum_{i=1}^m p_i\Big)$, we have:
  \[\sum_{i=1}^m\Ber(p_i) \leq_{\text{cx}}  \Poi\Big(\sum_{i=1}^m p_i\Big)\]
  Applying this result to $-\varphi$, which is convex, concludes the proof.
\end{proof}

\begin{proof}[Proof of Theorem \ref{theo:AlgoCard}]
  By linearity of expectation and the fact that the weights $w_a$ are positive, it is sufficient to show that for all $a \in [n]$:
  \[ \mathbb{E}[C_a^{\varphi}(X)] \geq \left(\min_{j \in [m]} \alpha_{\varphi}(j) \right) c_a \ , \]
 where $C_a^{\varphi}(S) := \varphi(\abs{S}_a)$. Note that $\abs{X}_a = \sum_{i \in [m] : a \in T_i} X_i$, and thus:
  \begin{equation}
    \begin{aligned}
      \mathbb{E}[C_a^{\varphi}(X)] &=&& \mathbb{E}\Big[\varphi\Big(\sum_{i \in [m] : a \in T_i} X_i\Big)\Big] = \mathbb{E}\Big[\varphi\Big(\sum_{i \in [m] : a \in T_i}\Ber(x_i) \Big)\Big]\\
      &\geq&& \mathbb{E}\Big[\varphi\Big(\Poi\Big(\sum_{i \in [m] : a \in T_i} x_i\Big)\Big)\Big] \text{ thanks to Lemma \ref{lem:ConvexOrder}}\\
      &=&& \mathbb{E}[\varphi(\Poi(\abs{x}_a))] \ .
    \end{aligned}
  \end{equation}
Furthermore, since $x \mapsto \mathbb{E}[\varphi(\Poi(x))]$ is concave thanks to Proposition \ref{prop:PoiCon}, if we write $\abs{x}_a = \lambda \lfloor \abs{x}_a \rfloor + (1-\lambda) \lceil \abs{x}_a \rceil$ for some $\lambda \in [0,1]$, we have
  \begin{equation*}
    \begin{aligned}
      \mathbb{E}[\varphi(\Poi(\abs{x}_a))] &\geq&& \lambda \mathbb{E}[\varphi(\Poi(\lfloor \abs{x}_a \rfloor))] + (1-\lambda) \mathbb{E}[\varphi(\Poi(\lceil \abs{x}_a \rceil))] \\
      &=&& \lambda \alpha_{\varphi}(\lfloor \abs{x}_a \rfloor)\varphi(\lfloor \abs{x}_a \rfloor) + (1-\lambda) \alpha_{\varphi}(\lceil \abs{x}_a \rceil)\varphi(\lceil \abs{x}_a \rceil) \quad \text{ by definition of } \alpha_{\varphi}(x)\\
      &\geq&& \left(\min_{j \in [m]} \alpha_{\varphi}(j) \right) \left(\lambda \varphi(\lfloor \abs{x}_a \rfloor) + (1-\lambda) \varphi(\lceil \abs{x}_a \rceil) \right) \quad\text{ since } \lfloor \abs{x}_a \rfloor \leq \lceil \abs{x}_a \rceil \leq m\\
      &=&& \left(\min_{j \in [m]} \alpha_{\varphi}(j) \right) \varphi(\abs{x}_a) \quad \text{ since $\varphi$ linear between integer points}\\
      &\geq&& \left(\min_{j \in [m]} \alpha_{\varphi}(j) \right) c_a \ .
    \end{aligned}
  \end{equation*}
\end{proof}

\subsection{Generalization to Matroid Constraints}
\label{sec:matroid}

Instead of taking a cardinality constraint $k$ on the size of the subset $S$, we look now at general matroid constraints on $S$. Specifically, as input, instead of $k$, we take a matroid $\mathcal{M}$ defined on $[m]$ and given by a set of linear constraints describing its base polytope $B(\mathcal{M})$. The output is a set $S \in \mathcal{M}$ that maximizes $C^{\varphi}(S)$. Note that the cardinality constraint considered above is the special case where $\mathcal{M}$ is the uniform matroid of all subsets of size at most $k$ and the base polytope $B(\mathcal{M}) = \{ x \in [0,1]^m : \sum_{i=1}^m x_i = k\}$.
%We can in fact restrict our search to maximally independent sets, which are the vertices of $B(\mathcal{M})$.

We first note that in the order to establish Theorem \ref{theo:AlgoCard}, the cardinality constraint $\sum_{i=1}^m x_i = k$ is not used. Thus, since the pipage rounding strategy applies to matroid constraints $\mathcal{M}$ (see \cite[Lemma 3.4]{Vondrak07}), the strategy and the analysis of its efficiency generalize immediately when applied to the following linear program:

\begin{defi}[Relaxed program for matroid constraints]
  \begin{equation}
    \begin{aligned}
      &\maxi&& \sum_{a \in [n]} w_ac_a \\
      &\st&& c_a \leq \varphi(\abs{x}_a), \forall a \in [n]\\
      &&& 0 \leq x_i \leq 1, \forall i \in [m]\\
      &&& x \in B(\mathcal{M}) \quad \text{ the base polytope of } \mathcal{M} \ .
    \end{aligned}
  \end{equation}
  \label{defi:relaxedMatProg}
\end{defi}

\begin{theo*}
  Let $x,c$ a feasible solution of the program \ref{defi:relaxedMatProg} and $X \sim \Ber(x)$. Then:
  \[\mathbb{E}_{X \sim \Ber(x)}[C^{\varphi}(X)] \geq \left(\min_{j \in [m]} \alpha_{\varphi}(j)\right) \sum_{a \in [n]} w_ac_a \ .\]
  In particular, this implies that the described polynomial time algorithm has an approximation ratio of $\alpha_{\varphi}$:
  \[C^{\varphi}(x^{\text{int}}) \geq \alpha_{\varphi} \sum_{a \in [n]} w_ac^*_a \geq \alpha_{\varphi} \max_{S \in \mathcal{M}} C^{\varphi}(S)\ .\]
  \label{theo:AlgoMat}
\end{theo*}

    
\section{Hardness of Approximation for $\varphi$-\textsc{MaxCoverage}}
\label{section:Hardness}
In this section, we establish an inapproximability bound for the $\varphi$-\textsc{MaxCoverage} problem with weights $1$ under cardinality constraints. Throughout this section we use $\Gamma$ to denote the universe of elements and, hence, an instance of the $\varphi$-\textsc{MaxCoverage} problem consists of $\Gamma$, along with a collection of subsets $\mathcal{F} = \set{F_i \subseteq \Gamma}_{i=1}^m$  and an integer $k$. Recall that the objective of this problem is to find a size-$k$ subset $S \subseteq [m]$ that maximizes $C^{\varphi}(S) = \sum_{a \in \Gamma}\varphi(\abs{S}_a)$.

We establish the following theorem in this section:

\begin{theo*}
 It is NP-hard to approximate the $\varphi$-\textsc{MaxCoverage} problem for $\varphi(n) = o(n)$ within a factor greater that $\alpha_{\varphi} + \varepsilon$ for any $\varepsilon > 0$.
  \label{theo:Hardness}
\end{theo*}

Our reduction is based on a problem called $h$-\textsc{AryLabelCover}, which is equivalent to the more standard \textsc{GapLabelCover} problem as will be shown in Appendix~\ref{NPhardnessGap}.
\begin{defi}[$h$-\textsc{AryLabelCover}]
  An instance $\mathcal{G} = (V,E,[L],[R],\set{\pi_{e,v}}_{e \in E, v \in e})$ of $h$-\textsc{AryLabelCover} is characterized by an $h$-uniform regular hypergraph $(V, E)$ and bijection constraints $\pi_{e,v} : [L] \rightarrow [R]$. Here, each $h$-uniform hyperedge represents a $h$-ary constraint. Additionally, for any labeling $\sigma : V \rightarrow [L]$, we have the following notions of strongly and weakly satisfied constraints:
  \begin{itemize}
  \item An edge $e = (v_1,\ldots,v_h) \in E$ is \emph{strongly satisfied} by $\sigma$ if:
    \[ \forall x,y \in [h], \pi_{e,v_x}(\sigma(v_x)) = \pi_{e,v_y}(\sigma(v_y)) \]
  \item An edge $e = (v_1,\ldots,v_h) \in E$ is \emph{weakly satisfied} by $\sigma$ if:
    \[ \exists x\not=y \in [h], \pi_{e,v_x}(\sigma(v_x)) = \pi_{e,v_y}(\sigma(v_y)) \]
  \end{itemize}
\end{defi}

\begin{prop}[$\delta,h$-\textsc{AryGapLabelCover}]
\label{prop:hardness-ary-lc}
  For any fixed integer $h \geq 2$ and fixed $\delta > 0$, there exists an $R_0$ such that for any integer $R \geq R_0$, it is {\rm NP}-hard for instances $\mathcal{G} (V,E,[L],[R],\set{\pi_{e,v}}_{e \in E, v \in e})$ of $h$-\textsc{AryLabelCover} with right alphabet $[R]$ to distinguish between: 
  \begin{itemize}
  \item[\textbf{YES:}] There exists a labeling $\sigma$ that \emph{strongly satisfies} all the edges.
  \item[\textbf{NO:}] No labeling \emph{weakly satisfies} more than $\delta$ fraction of the edges.
  \end{itemize}
  \label{prop:AryGapLabelCover}
\end{prop}
The proof of this proposition can be found in Appendix \ref{NPhardnessGap}.

% a equivalent problem of the $\delta$-Gap-Label-Cover used in in \cite{DDMS20}, 

\subsection{Partitioning System}

The key ingredient to prove Theorem~\ref{theo:Hardness} is a constant size combinatorial object called partitioning system, generalizing the work of one of Feige~\cite{Feige98} and~\cite{BFGG20}.
For any set $[n]$, $\mathcal{Q} \subseteq 2^{[n]}$, we overload the definition $C^{\varphi}(\mathcal{Q}) := \sum_{a \in [n]} \varphi(\abs{\mathcal{Q}}_a)$ with $\abs{\mathcal{Q}}_a:=\abs{\set{P \in \mathcal{Q} : a \in P}}$ and $C_a^{\varphi}(\mathcal{Q}) := \varphi(\abs{\mathcal{Q}}_a)$. Let us take $x_{\varphi} \in \text{argmin}_{x \in \mathbb{N}^*} \alpha_{\varphi}(x)$, thus $\alpha_{\varphi} = \alpha_{\varphi}(x_{\varphi})$.\footnote{Note that to make the notation lighter, we assumed that the infimum defining $\alpha_{\varphi}$ is achieved at some $x_{\varphi}$. If this is not the case, for any $\varepsilon > 0$, we choose $x_{\varphi, \varepsilon}$ such that $\alpha_{\varphi}(x_{\varphi, \varepsilon}) \leq \alpha_{\varphi} + \varepsilon$ and in the proof we replace all the occurrences of $x_{\varphi}$ with $x_{\varphi, \varepsilon}$.}
%can do the hardness proof with any $x \in \mathbb{N}$ and the minimum $\alpha_{\varphi} = \lim_{x \rightarrow \infty} \alpha_{\varphi}(x)$ will be reached as a limit result of the hardness of all $\alpha_{\varphi}(x)$.
 %\OF{Is it clear that there is a limit? We could a priori have some weird things happening}

We say that $\mathcal{Q}$ is an \emph{$x$-cover} of $x \in \mathbb{N}$ if every element of $[n]$ is covered $x$ times, so $C^{\varphi}(\mathcal{Q}) = n\varphi(x)$.

\begin{defi}
  Given $[n]$, an \emph{$([n],h,R,\varphi,\eta)$-partitioning system} consists of $R$ distinct collections of subsets of $[n]$, $\mathcal{P}_1,\ldots,\mathcal{P}_R \subseteq 2^{[n]}$, that satisfy:
  \begin{enumerate}
  \item For every $i \in [R], \mathcal{P}_i$ is a collection of $h$ subsets $P_{i,1}, \ldots, P_{i,h} \subseteq [n]$ each of size $x_{\varphi}n/h$ (thus we need that $h \geq x_{\varphi}$) which is an $x_{\varphi}$-cover.
  \item For any $T \subseteq [R]$ and $\mathcal{Q} = \set{P_{i,j(i)} : i \in T}$ for some function $j : T \rightarrow [h]$, we have $\abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} \leq \eta n$ where:
  \begin{align}
  \label{eq:def-phi}
   \psi^{\varphi}_{k,h} := \mathbb{E}\Big[\varphi\Big(\Bin\Big(k,\frac{x_{\varphi}}{h}\Big)\Big)\Big] \ .
   \end{align}
  \end{enumerate}
  \label{defi:PartSystem}
\end{defi}

\begin{rk}
  In particular, for any $\mathcal{Q} = \set{Q_1, \ldots, Q_k}$ with $Q_i$ of size $\frac{x_{\varphi}n}{h}$, we have that $C^{\varphi}(\mathcal{Q}) \leq n\varphi(k\frac{x_{\varphi}}{h})$. Indeed $C^{\varphi}(\mathcal{Q}) = \sum_{a \in [n]} \varphi(\abs{\mathcal{Q}}_a)$ with $\sum_{a \in [n]} \abs{\mathcal{Q}}_a = \sum_{i \in [k]} \abs{Q_i} = k \times \frac{x_{\varphi}n}{h}$. By concavity of $\varphi$ and Jensen's inequality, this function is maximized when all $\abs{\mathcal{Q}}_a$ are equals, where we get $n\varphi(k\frac{x_{\varphi}}{h})$.
\end{rk}
\begin{prop}
  For every choice of $R,h \in \mathbb{N}$ with $h \geq x_{\varphi}$, $\eta > 0$,  $n \geq \eta^{-2}R\varphi(R)^2\log(20(h+1))$, there exists an $([n],h,R,\varphi,\eta)$-partitioning system, which can be found in time exp($Rn$log$n).$poly$(h)$ (this will be constant time in the reduction).
  \label{prop:Partitioning}
\end{prop}

The proof can be found in Appendix~\ref{app:Partitioning}.

  
\subsection{The Reduction}
  \begin{proof}[Proof of Theorem \ref{theo:Hardness}]
    Let $\varepsilon > 0$. We show that it is NP-hard to reach an approximation greater than $\alpha_{\varphi} + \varepsilon$ for the $\varphi$-\textsc{MaxCoverage} problem, via a reduction from $\delta,h$-\textsc{AryGapLabelCover}. We define the following constants:
  \begin{itemize}
    \item $\eta = \frac{\varphi(x_{\varphi})}{4x_{\varphi}} \varepsilon$,
    \item Let $h \geq x_{\varphi}$ be such that $\abs{\psi^{\varphi}_{h,h} - \alpha_{\varphi}\varphi(x_{\varphi})} \leq \eta$ (see~\eqref{eq:def-phi} for the definition of $\psi^{\varphi}$); such a choice exists thanks to Proposition \ref{prop:UnboundBinPoi},
    \item Let $\theta$ be such that for all $x \geq \theta$, $\frac{\varphi(x)}{x} \leq \eta$, which exists since $\varphi(x) = o(x)$,
    \item $\xi = \frac{x_{\varphi}}{\theta}$,
    \item $\delta = \frac{\eta}{2} \frac{\xi^3}{h^2}$.
\end{itemize}
    
For this choice of $h$ and $\delta$, we fix $R \geq h$ to be large enough for Proposition~\ref{prop:hardness-ary-lc} to hold. Then given an instance  $\mathcal{G} = (V,E,[L],[R], \Sigma, \set{\pi_{e,v}}_{e \in E, v \in e})$ of $\delta,h$-\textsc{AryGapLabelCover}, 
%where we ask that $R$ is a constant larger than $h$, 
we construct an instance $(\Gamma, \mathcal{F}, k)$ of the $\varphi$-\textsc{MaxCoverage} problem with:

    \begin{itemize}
    \item $n$ a large enough integer to have the existence of $([n],h,R,\varphi,\eta)$-partitioning systems using Proposition \ref{prop:Partitioning}. Note that the size of these partitioning systems is constant independent of the size of the instance $\mathcal{G}$.
    \item $\Gamma = [n] \times E$,
    \item $k =\abs{V}$,
    \item We consider a  $([n],h,R,\varphi,\eta)$-partitioning system, and we call $\mathcal{P} =\set{\mathcal{P}_1,\ldots,\mathcal{P}_R}$ the corresponding set of collections. We first define sets $T_{\beta}^{e,v_j} = P_{\pi_{e,v_j}(\beta),j} \times \set{e}$ for $e = (v_1,\ldots,v_h) \in E, j \in [h], \beta \in [L]$ and we define $F^v_{\beta} := \bigsqcup_{e \in E:v \in e} T^{e,v}_{\beta}$ and $\mathcal{F} := \set{F^v_{\beta}, v \in V, \beta \in [L]}$.
  \end{itemize}

%Let us prove the completeness and soundness properties. 
%Precisely, 
We will now prove that if we are in a YES instance, we have that there exists $\mathcal{T}$ of size $k$ such that $C^{\varphi}(\mathcal{T}) \geq \varphi(x_{\varphi})\abs{\Gamma}$ (completeness). Moreover, if we are in a NO instance, then we have that for all $\mathcal{T}$ of size $k = \abs{V}$, $C^{\varphi}(\mathcal{T}) \leq (\alpha_{\varphi} + \varepsilon)\varphi(x_{\varphi})\abs{\Gamma}$ (soundness). Establishing these two properties would conclude the proof. 
In fact, an algorithm for $\varphi$-\textsc{MaxCoverage} achieving a factor strictly greater than $\alpha_{\varphi} + \varepsilon$ would allow us to decide whether we have YES or a NO instance of the {\rm NP}-hard problem $\delta,h$-\textsc{AryGapLabelCover}.
% would return a set $\mathcal{T}_{\text{approx}}$ satisfying that
%we would be able to distinguish between those two instances, which is known to be NP-hard by the $\delta,h$-\textsc{AryGapLabelCover}. 
%for a YES instance, we would have $C^{\varphi}(\mathcal{T}_{\text{approx}}) \geq \alpha \varphi(x_{\varphi})\abs{\Gamma} > (\alpha_{\varphi} + \varepsilon)\varphi(x_{\varphi})\abs{\Gamma}$, which would say that we are not in a NO instance. 
%Thus we would be in a YES instance if and only if $C^{\varphi}(\mathcal{T}_{\text{approx}}) > (\alpha_{\varphi} + \varepsilon)\varphi(x_{\varphi})\abs{\Gamma}$. So finding such a $\mathcal{T}_{\text{approx}}$ is NP-hard.

In order to achieve this, let us define $C^{\varphi,e} := \sum_{a \in [n] \times \set{e}} C^{\varphi}_a$. In particular, $C^{\varphi} = \sum_{a \in  \Gamma} C^{\varphi}_a = \sum_{e \in E}C^{\varphi,e}$. For $\mathcal{T} \subseteq \mathcal{F}$, we define the relevant part of $\mathcal{T}$ on $e$ by
\[\mathcal{T}_e := \set{T_{\beta}^{e,v} : v \in e, \beta \in [L], F^v_{\beta} \in \mathcal{T}} = \set{F^v_{\beta} \cap ([n] \times \set{e}), F^v_{\beta} \in \mathcal{T}}\]
Note that $C^{\varphi,e}(\mathcal{T}) = C^{\varphi,e}(\mathcal{T}_e)$, and in particular $C^{\varphi}(\mathcal{T}) = \sum_{e \in E} C^{\varphi,e}(\mathcal{T}_e)$. 

\subsubsection{Completeness}
Suppose the given $h$-\textsc{AryLabelCover} instance $\mathcal{G}$ is a YES instance. Then, there exists a labeling $\sigma : V \mapsto [L]$ which strongly satisfies all edges. Consider the collection of $\abs{V}$ subsets $\mathcal{T} := \set{F_{\sigma(v)}^v : v \in V}$. Fix $e = (v_1,\ldots,v_h) \in E$. Since $e$ is strongly satisfied by $\sigma$, there exists $r \in [R]$ such that $\pi_{e,v_i}(\sigma(v_i)) = r$ for all $i \in [h]$. Thus, $\mathcal{T}_e = \set{T_{\sigma(v_i)}^{e,v_i}}_{i \in [h]} = \set{P_{r,i} \times \set{e}}_{i \in [h]}$ is an $x_{\varphi}$-cover of $[n] \times \set{e}$, and so $C^{\varphi,e}(\mathcal{T}_e) = n\varphi(x_{\varphi})$. Thus $C^{\varphi}(\mathcal{T}) = \sum_{e \in E}C^{\varphi,e}(\mathcal{T}_e) = \abs{E}\varphi(x_{\varphi})n = \varphi(x_{\varphi})\abs{\Gamma}$.

\subsubsection{Soundness}
Suppose the given $h$-\textsc{AryLabelCover} instance $\mathcal{G}$ is a NO instance. Let us prove the contrapositive of the soundness: we suppose that there exists $\mathcal{T}$ of size $k = \abs{V}$ such that  $C^{\varphi}(\mathcal{T}) > (\alpha_{\varphi} + \varepsilon)\varphi(x_{\varphi})\abs{\Gamma}$. Let us show that there exists a labelling $\sigma$ that weakly satisfies a strictly larger fraction of the edges than $\delta$.

For every vertex $v \in V$, we define $L(v) := \set{\beta \in [L] : F_{\beta}^v \in \mathcal{T}}$ to be the candidate set of labels that can be associated with the vertex $v$. We extend this definition to hyperedges $e = (v_1,\ldots,v_h)$ where we define $L(e) := \bigcup_{x \in [h]} L(v_x)$ to be the \emph{multiset} of all labels associated with the edge. Note that $\abs{\mathcal{T}_e}=\abs{L(e)}$. We say that $e = (v_1,\ldots,v_h) \in E$ is \emph{consistent} if and only if $\exists x \not= y \in [h], \pi_{e,v_x}(L(v_x)) \cap \pi_{e,v_y}(L(v_y)) \not= \emptyset$. 

We then decompose $E$ in three parts:
\begin{itemize}
\item $B$ is the set of edges $e \in E$ with $\abs{L(e)} \geq \frac{h}{\xi}$.
\item $N$ is the set of consistent edges $e \in E$ with $\abs{L(e)} < \frac{h}{\xi}$.
\item $I = E - (B \cup N)$ is the set of inconsistent edges $e \in E$ with $\abs{L(e)} < \frac{h}{\xi}$.
\end{itemize}

We want to show that the contribution of $N$ is not too small, which we will use to construct a labelling weakly satisfying enough edges. First, we will need the following lemma:

\begin{lem}
  $\sum_{e \in E} \abs{L(e)} = \abs{E} h$
  \label{lem:labelBound}
\end{lem}

\begin{proof}  
  Recall that our $h$-uniform hypergraph is regular; call $d$ its regular degree. In particular, we have that $d\abs{V} = \abs{E}h$. Note also that $\sum_{v \in V} \abs{L(v)} = \abs{\mathcal{T}} = \abs{V}$. Thus:
  \begin{equation}
    \begin{aligned}
      \sum_{e \in E} \abs{L(e)} = \sum_{e \in E} \sum_{v \in V : v \in e} \abs{L(v)} = \sum_{v \in V} \sum_{e \in E: v \in e} \abs{L(v)} = d \sum_{v \in V} \abs{L(v)} = d \abs{V} = \abs{E}h
    \end{aligned}
  \end{equation}
\end{proof}
Next we show that the contribution of $B$ to the coverage can be bounded.
\begin{lem}
  $\sum_{e \in B} C^{\varphi,e}(\mathcal{T}_e) \leq \frac{\varepsilon}{4}\varphi(x_{\varphi})\abs{\Gamma}$.
  \label{lem:contribB}
\end{lem}
\begin{proof}
  We have:
  \begin{equation}
    \begin{aligned}
      \sum_{e \in B} C^{\varphi,e}(\mathcal{T}_e) &\leq&& \sum_{e \in B} n\varphi\Big(\abs{L(e)}\frac{x_{\varphi}}{h}\Big) \quad \text{by the remark on Definition \ref{defi:PartSystem} and } \abs{\mathcal{T}_e} = \abs{L(e)}\\
      &\leq&& \abs{B} \times n\varphi\Big(\frac{\sum_{e \in B} \abs{L(e)}}{\abs{B}}\frac{x_{\varphi}}{h}\Big) \quad \text{by Jensen's inequality on concave } \varphi\\
      &\leq&&  \abs{B} \times n\varphi\Big(\frac{\abs{E}h}{\abs{B}}\frac{x_{\varphi}}{h}\Big) \quad \text{since } \varphi \text{ nondecreasing and } \sum_{e \in B} \abs{L(e)} \leq \abs{E}h \text{ by Lemma \ref{lem:labelBound}}\\
      &=&& \frac{\varphi\big(\frac{\abs{E}x_{\varphi}}{\abs{B}}\big)}{\frac{\abs{E}x_{\varphi}}{\abs{B}}} x_{\varphi} \abs{\Gamma} \ .
    \end{aligned}
  \end{equation}

Thanks to Lemma \ref{lem:labelBound}, we have that $\sum_{e \in E} \abs{L(e)} = \abs{E} h$, but $\sum_{e \in B} \abs{L(e)} \geq \abs{B} \frac{h}{\xi}$ by definition of $B$, so we have that $\frac{\abs{B}}{\abs{E}} \leq \xi$. Thus $\frac{\abs{E}x_{\varphi}}{\abs{B}} \geq \frac{x_{\varphi}}{\xi} = \theta$. By definition of $\theta$, we get that $\sum_{e \in B} C^{\varphi,e}(\mathcal{T}_e) \leq \eta x_{\varphi} \abs{\Gamma} = \frac{\varepsilon}{4} \varphi(x_{\varphi})\abs{\Gamma}$.
\end{proof}

In order to bound the contribution of $I$, we will prove a property on inconsistent edges:

\begin{prop}
 Let $e = (v_1,\ldots,v_h) \in E$ be an inconsistent hyperedge with respect to $\mathcal{T}$. Then we have that $\abs{C^{\varphi,e}(\mathcal{T}_e) - \psi^{\varphi}_{\abs{L(e)},h}n } \leq \eta n$.
  \label{prop:inconsistent}
\end{prop}

\begin{proof}
  Since $e$ is inconsistent, $\forall x \not= y \in [h], \pi_{e,v_x}(L(v_x)) \cap \pi_{e,v_y}(L(v_y)) = \emptyset$. Therefore, for every $i \in [R]$, there is at most one $v \in e$ such that $i \in \pi_{e,v}(L(v))$, i.e., $\mathcal{T}_e$ intersects with $\mathcal{P}_i \times \set{e}$ in at most one subset. This gives us a subset $T \subseteq [R]$ and a function $j : T \rightarrow [h]$ such that $\mathcal{T}_e = \set{P_{i,j(i)} \times \set{e} : i \in T}$. As a consequence, $\abs{T} = \abs{\mathcal{T}_e} = \abs{L(e)}$ and by the second condition of the partitioning system, we get the expected result.
\end{proof}

Now, we can bound the contribution of $I$:

\begin{lem}
  $\sum_{e \in I} C^{\varphi,e}(\mathcal{T}_e) \leq (\alpha_{\varphi} + \frac{\varepsilon}{2})\varphi(x_{\varphi})\abs{\Gamma}$.
  \label{lem:contribI}
\end{lem}

\begin{proof}
Thanks to Proposition \ref{prop:inconsistent}, we have:

  \begin{equation}
    \begin{aligned}
      \sum_{e \in I} C^{\varphi,e}(\mathcal{T}_e) \leq \sum_{e \in I} (\psi^{\varphi}_{\abs{L(e)},h} +\eta)n \leq \sum_{e \in E} (\psi^{\varphi}_{\abs{L(e)},h} +\eta)n \ ,
    \end{aligned} 
  \end{equation}
  since $I \subseteq E$ and $\psi^{\varphi}_{\abs{L(e)},h} \geq 0$. But $\sum_{e \in E} \abs{L(e)} = \abs{E}h$ by Lemma \ref{lem:labelBound} and $x \mapsto \psi^{\varphi}_{x,h}$ is concave thanks to Proposition \ref{prop:BinCon}, so we can use Jensen's inequality to get $\sum_{e \in E} \psi^{\varphi}_{\abs{L(e)},h} \leq \abs{E} \psi^{\varphi}_{\frac{\sum_{e \in E} \abs{L(e)}}{\abs{E}},h} = \abs{E}\psi^{\varphi}_{h,h}$ and thus:
  \begin{equation}
    \begin{aligned}
      \sum_{e \in I} C^{\varphi,e}(\mathcal{T}_e) \leq (\psi^{\varphi}_{h,h} +\eta)n\abs{E} \leq (\alpha_{\varphi}\varphi(x_{\varphi}) + 2\eta)\abs{\Gamma} \ ,
    \end{aligned} 
  \end{equation}
  by definition of $h$. This implies that the total contribution of inconsistent edges $I$ is at most $\sum_{e \in I} C^{\varphi,e}(\mathcal{T}_e) \leq (\alpha_{\varphi}\varphi(x_{\varphi}) + 2\eta)\abs{\Gamma} \leq (\alpha_{\varphi}+ \frac{\varepsilon}{2})\varphi(x_{\varphi})\abs{\Gamma}$ by definition of $\eta$.
\end{proof}

\begin{lem}
  $\frac{\abs{N}}{\abs{E}} \geq \xi\eta$
  \label{lem:nice}
\end{lem}

\begin{proof}
Since we have supposed that $\sum_{e \in E} C^{\varphi,e}(\mathcal{T}_e) = C^{\varphi}(\mathcal{T}) > (\alpha_{\varphi} + \varepsilon)\varphi(x_{\varphi})\abs{\Gamma}$, and with the help of Lemmas \ref{lem:contribB} and \ref{lem:contribI}, we have that the contribution of $N$ is:

\[\sum_{e \in N}  C^{\varphi,e}(\mathcal{T}_e) > \frac{\varepsilon}{4}\varphi(x_{\varphi})\abs{\Gamma}\]

However, we have that for $e \in N$ that $C^{\varphi,e}(\mathcal{T}_e) \leq  n\varphi\Big(\abs{\mathcal{T}_e}\frac{x_{\varphi}}{h}\Big) = n\varphi\Big(\abs{L(e)}\frac{x_{\varphi}}{h}\Big) \leq n \varphi\Big(\frac{x_{\varphi}}{\xi}\Big) \leq \frac{nx_{\varphi}}{\xi}$ thanks to the remark on definition \ref{defi:PartSystem} and the bound $\abs{L(e)} < \frac{h}{\xi}$. This implies that:
\[\frac{\abs{N}}{\abs{E}} \geq \frac{\xi}{x_{\varphi}}\frac{\varepsilon \varphi(x_{\varphi})}{4} = \xi \eta \ .\]
\end{proof}

Finally, we construct a randomized labeling $\sigma : V \mapsto [L]$ as follows: for $v \in V$, if $L(v) \not= \emptyset$, set $\sigma(v)$ uniformly form $L(v)$, otherwise set it arbitrarily. We claim that in expectation, this labeling must weakly satisfy $\delta$ fraction of the hyperedges.

To see this, fix any $e = (v_1,\ldots,v_h) \in N$. Thus $\exists x \not= y \in [h], \pi_{e,v_x}(L(v_x)) \cap \pi_{e,v_y}(L(v_y)) \not= \emptyset$. Furthermore $\abs{L(v_x)},\abs{L(v_y)} \leq \frac{h}{\xi}$. Thus, we have that $\pi_{e,v_x}(L(v_x)) = \pi_{e,v_y}(L(v_y))$ with probability at least $\frac{1}{\abs{L(v_x)}\abs{L(v_y)}} \geq \Big(\frac{\xi}{h}\Big)^2$.

Therefore:
\begin{equation}
  \begin{aligned}
    &&& \mathbb{E}_{\sigma}\mathbb{E}_{e \sim E}[\sigma \text{ weakly satisfies } e]\\
    &\geq&& \xi \eta \mathbb{E}_{\sigma}\mathbb{E}_{e \sim E}[\sigma \text{ weakly satisfies } e | e \in N] \quad \text{by Lemma \ref{lem:nice}}\\
    &>&& \frac{\eta}{2} \frac{\xi^3}{h^2} = \delta \ .
  \end{aligned}
\end{equation}

In particular there exists some labeling $\sigma$ such that $\mathbb{E}_{e \sim E}[\sigma \text{ weakly satisfies } e] > \delta$, and thus the soundness is also proved.
\end{proof}

%  \newpage

\input{applications}

 
\section*{Conclusion}
%The standard coverage function $C(S)$ counts the number of elements $a \in [n]$ that are covered by at least one set $T_i$ with $i \in S$. Note that the contribution of an $a \in [n]$ to $C(S)$ is exactly the same whether $a$ appears in just one set $T_i$ or in all of them. It is very natural to consider settings wherein having more than one copy of $a$ is more valuable than just one copy of it. 

We have introduced the $\varphi$-\textsc{MaxCoverage} problem where having $c$ copies of element $a$ gives a value $\varphi(c)$. We have shown that when $\varphi$ is normalized, nondecreasing and concave, we can obtain an approximation guarantee given by the \emph{Poisson concavity ratio} $\alpha_{\varphi} := \inf_{x \in \mathbb{N}} \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(\mathbb{E}[\Poi(x)])}$ and we showed it is tight for sublinear functions $\varphi$. The Poisson concavity ratio strictly beats the bound one gets when using the notion of curvature submodular functions, except in very special cases such as \textsc{MaxCoverage} where the two bounds are equal.

An interesting open question is whether there exists combinatorial algorithms that achieve this approximation ratio. As mentioned in \cite{BFGG20}, for the $\ell$-\textsc{MultiCoverage} with $\ell \geq 2$, which is the special case where $\varphi(x) = \min(x,\ell)$, the simple greedy algorithm only gives a $1 - e^{-1}$ approximation ratio, which is strictly less than the ratio $\alpha_{\varphi} = 1-\frac{\ell^{\ell}e^{-\ell}}{\ell!}$ in that case.

Another open question is whether the hardness result remains true even when $\varphi(n) \not= o(n)$.
%which make the $\varphi$-\textsc{MaxCoverage} still $\alpha_{\varphi}$-hard to approximate. 
A good example is given by $\varphi(0)=0$ and $\varphi(1+t) = 1 + ct$ with $c \in (0,1)$. We know that the problem is hard for $c=0$ but easy for $c=1$. One can show that the approximation ratio achieved by our algorithm is $\alpha_{\varphi} = 1 - \frac{c}{e}$ in that case (which is the same approximation ratio obtained from the curvature in \cite{SVW17}), but the tightness of this approximation ratio remains open.


\section*{Acknowledgements}

This research is supported by the French ANR project ANR-18-CE47-0011 (ACOM). SB gratefully acknowledges the support of a Ramanujan Fellowship (SERB - {SB/S2/RJN-128/2015}) and a Pratiksha Trust Young Investigator Award.

\bibliographystyle{plain}
\bibliography{these.bib}

\newpage
\appendix

\section{General properties}
In this section, we will assume that $\varphi : \mathbb{N} \rightarrow \mathbb{R}^+$ is nondecreasing concave and normalized ($\varphi(0)=0$ and $\varphi(1)=1$).

\begin{prop}
  $C^{\varphi}$ is submodular, its curvature is at most $c = 1 - (\varphi(m) - \varphi(m-1))$ and it cannot be improved for a general instance with $m$ cover sets.
  \label{prop:SubCurv}
\end{prop}

\begin{proof}

  We use the following lemma which is trivial to prove:

  \begin{lem}[Properties of $\abs{S}_a = \abs{\set{i \in S : a \in T_i}}$.]
    We have:
  \begin{enumerate}
  \item $\abs{S}_a \leq \abs{S}$
  \item $\abs{S \cup S'}_a\leq \abs{S}_a + \abs{S'}_a$. In particular, if $S \subseteq T$ then $\abs{S}_a \leq \abs{T}_a$ and $\abs{S\cup\set{x}}_a \leq \abs{S}_a + 1$.
  \item If $S \subseteq T$, $x \not\in T$ then $\abs{S}_a = \abs{T}_a \Rightarrow \abs{S\cup\set{x}}_a = \abs{T\cup\set{x}}_a$
  \end{enumerate}
  \label{lem:ke}
\end{lem}
  
  Let us show first the submodularity of $C^{\varphi}$. Let $S \subseteq T \subseteq [m]$ and $x \not\in T$:
  \begin{equation}
    \begin{aligned}
      &&& C^{\varphi}(S \cup \set{x}) - C^{\varphi}(S) - (C^{\varphi}(T \cup \set{x}) - C^{\varphi}(T))= \\
      &=&& \sum_{a \in [n]} w_a[\varphi(\abs{S\cup\set{x}}_a) - \varphi(\abs{S}_a) - (\varphi(\abs{T\cup\set{x}}_a) - \varphi(\abs{T}_a))]\\
    \end{aligned}
  \end{equation}

  Let us call $g(a) := \varphi(\abs{S\cup\set{x}}_a) - \varphi(\abs{S}_a) - (\varphi(\abs{T\cup\set{x}}_a) - \varphi(\abs{T}_a))$:
  \begin{enumerate}
  \item If $\abs{T}_a = \abs{S}_a$ then thanks to Lemma \ref{lem:ke}, we have that $\abs{T\cup\set{x}}_a = \abs{S\cup\set{x}}_a$, so $g(a) = 0$
  \item Else, we have that $\abs{T}_a > \abs{S}_a$:
    \begin{enumerate}
    \item If $\abs{S\cup\set{x}}_a = \abs{S}_a$, then we add elements of $T-S$ using Lemma \ref{lem:ke} to get that $\abs{T\cup\set{x}}_a = \abs{T}_a$, so $g(a)=0$ in that case.
    \item Else $\abs{S\cup\set{x}}_a \not= \abs{S}_a$. So with $\abs{S}_a = k$, we get that $\abs{S\cup\set{x}}_a = k+1$ and $\abs{T}_a > \abs{S}_a$ so $\abs{T}_a \geq k+1$.

      \begin{enumerate}
      \item If $\abs{T\cup\set{x}}_a = \abs{T}_a$, then $g(a) =  \varphi(k+1) - \varphi(k) \geq 0$ since $\varphi$ is nondecreasing.
      \item Else $\abs{T\cup\set{x}}_a \not= \abs{T}_a$ so with $\abs{T}_a = \ell$ with $\ell \geq k+1$, we get that $\abs{T}_a = \ell+1$. So we have that:
        \begin{equation}
          \begin{aligned}
            g(a) &=&& \varphi(k+1) - \varphi(k) - (\varphi(\ell+1) - \varphi(\ell))\\
            &=&& \frac{\varphi(k+1) - \varphi(k)}{(k+1) - k} - \frac{\varphi(\ell+1) - \varphi(\ell)}{(\ell+1) - \ell} \geq 0
          \end{aligned}
        \end{equation}
        by concavity of $\varphi$: its slopes are nonincreasing.
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
  So in all cases, we have $g(a) \geq 0$ so $ C^{\varphi}(S \cup \set{x}) - C^{\varphi}(S) - (C^{\varphi}(T \cup \set{x}) - C^{\varphi}(T)) \geq 0$: $C^{\varphi}$ is submodular.

  Let us now compute its curvature:
  \[c = 1 - \min_{i \in [m]} \frac{C^{\varphi}([m]) - C^{\varphi}([m]-\set{i})}{C^{\varphi}(\set{i}) - C^{\varphi}(\emptyset)}\]

  Let $i \in [m]$ fixed:
  \begin{equation}
    \begin{aligned}
      &&& \frac{C^{\varphi}([m]) - C^{\varphi}([m]-\set{i})}{C^{\varphi}(\set{i}) - C^{\varphi}(\emptyset)}\\
      &=&& \frac{\sum_{a \in [n]} w_a[\varphi(\abs{[m]}_a) - \varphi(\abs{[m]-\set{i}}_a)]}{\sum_{a \in [n]}w_a[\varphi(\abs{\set{i}}_a) - \varphi(\abs{\emptyset}_a)]}\\
      &=&& \frac{\sum_{a \in T_i} w_a[\varphi(\abs{[m]}_a) - \varphi(\abs{[m]-\set{i}}_a)]}{\sum_{a \in T_i} w_a}\\
      &=&& \frac{\sum_{a \in T_i} w_a[\varphi(\abs{[m]}_a) - \varphi(\abs{[m]}_a-1)]}{\sum_{a \in T_i} w_a} \text{ since } a \in T_i \ .\\
    \end{aligned}
  \end{equation}
  But $\abs{[m]}_a \leq m$ and $\varphi$ concave, so $\varphi(\abs{[m]}_a)) - \varphi(\abs{[m]}_a-1) \geq \varphi(m) - \varphi(m-1)$ for all $a \in [n]$. As a consequence we have that:

  \[\frac{C^{\varphi}([m]) - C^{\varphi}([m]-\set{i})}{C^{\varphi}(\set{i}) - C^{\varphi}(\emptyset)} \geq \varphi(m) - \varphi(m-1)\]
  and this lower bound is true for its minimum over $i$. Thus we get that $c \leq 1 - (\varphi(m) - \varphi(m-1))$.
  Also one can find instances for all $m$ such that this bound is tight: take $T_1 =\set{a}$ and $\forall j \in [m], a \in T_j$ for instance.
\end{proof}

\begin{prop}
  The approximation ratio $\alpha_{\varphi}$ is always better than the general ratio given in $\cite{SVW17}$: $\min_{x \in [m]} \alpha_{\varphi}(x) \geq 1 - ce^{-1}$ with $1-c = \varphi(m) - \varphi(m-1)$. Furthermore, we have always that $\alpha_{\varphi}(0) = 1$, so the minimum range in the definition of $\alpha_{\varphi}$ can be taken on positive integers only.
  \label{prop:BetterRatio}
\end{prop}

\begin{proof}
  We first suppose that $x \geq 1$. Also, we can ask that for all $j \geq m$, we have $\varphi(j+1) -\varphi(j) = \varphi(m) -\varphi(m-1)$, since these quantities are never achieved in a instance with $m$ cover sets. So we can suppose that for all $j$ we have $\varphi(j+1) -\varphi(j) \geq \varphi(m) -\varphi(m-1)$. Also for $k \geq 1$, one can write:

  \[\varphi(k)= \sum_{j=0}^{k-1} \varphi(j+1) -\varphi(j) = 1 + \sum_{j=1}^{k-1} \varphi(j+1) -\varphi(j) \ . \]
  
  Thus:
  \begin{equation}
    \begin{aligned}
      &&&\mathbb{E}[\varphi(\Poi(x))] = e^{-x}\sum_{k=1}^{+\infty}\varphi(k) \frac{x^k}{k!}\\
      &=&& e^{-x}\sum_{k=1}^{+\infty}\Big(1 + \sum_{j=1}^{k-1} \varphi(j+1) -\varphi(j)\Big) \frac{x^k}{k!} \\
      &=&& e^{-x}\Big[(e^x - 1) + \sum_{k=1}^{+\infty}\Big(\sum_{j=1}^{k-1} \varphi(j+1) -\varphi(j)\Big) \frac{x^k}{k!} \Big]\\
      &\geq&& (1 - e^{-x}) + e^{-x}\sum_{k=1}^{+\infty}(k-1)[\varphi(m) -\varphi(m-1)] \frac{x^k}{k!}\\
      &=&& (1 - e^{-x}) + [1-c]e^{-x}\Big(x\sum_{k=1}^{+\infty}\frac{x^{k-1}}{(k-1)!} - \sum_{k=1}^{+\infty}\frac{x^k}{k!}\Big)\\
      &=&& (1 - e^{-x}) + [1-c]e^{-x}\Big(xe^x - (e^x-1)\Big)\\
      &=&& 1 - e^{-x} + [1-c](x-1 + e^{-x}) =: f(x) \ .
    \end{aligned}
  \end{equation}

  But since $\varphi(x) \leq x$  and with $g(x) := \frac{f(x)}{x}$, we get that $g'(x) =\frac{c}{x^2}(x-1+e^{-x}) \geq 0$ for $x \geq 1$, thus $g(x) \geq g'(1)$ and then
  \[\alpha_{\varphi}(x) \geq \frac{f(x)}{\varphi(x)} \geq g(x) \geq g(1) = f(1) \ , \]
  with $f(1) =  1 - e^{-1} + (\varphi(m) - \varphi(m-1)) e^{-1}$ the approximation ratio of the general algorithm.

  Let us now suppose that $x \in [0,1]$. We have that $\varphi(x) = x$ on that interval since we have taken its piecewise linear extension and $\varphi(0) = 0$ and $\varphi(1) = 1$. Thus we have that
  \[\alpha_{\varphi}(x) = \frac{\mathbb{E}[\varphi(\Poi(x))]}{x} = e^{-x}\sum_{k=1}^{+\infty}\frac{\varphi(k)}{k}\frac{x^{k-1}}{(k-1)!} \]
  and we can then compute its value at $0$: $\alpha_{\varphi}(0) = e^{-0}\sum_{k=1}^{+\infty}\frac{\varphi(k)}{k}\frac{0^{k-1}}{(k-1)!} = \frac{\varphi(1)}{1}\frac{0^0}{0!} = 1$. Since $\alpha_{\varphi}(x) \leq 1$, we have that $\alpha_{\varphi} = \inf_{x \in \mathbb{N}^*} \alpha_{\varphi}(x)$.
\end{proof}

\begin{prop}
  Let $F(x) := \mathbb{E}_{X \sim x}[C^{\varphi}(X)]$ for $x \in \set{0,1}^m$. We have an explicit formula for $F$:
  \[  F(x) = \sum_{a = 1}^n \sum_{k=0}^{m} \Big[\frac{1}{m+1}\sum_{\ell = 0}^{m} \omega_{m+1}^{-\ell k}\prod_{j \in [m] : a \in  T_j}(1 +(\omega_{m+1}^{\ell}-1)x_j)\Big]\varphi(k) \text{ with } \omega_{m+1} := \exp(\frac{2i\pi}{m+1}) \]
  Thus, $F$ is computable in polynomial time in $n$ and $m$.
  \label{prop:Fpoly}
\end{prop}

\begin{proof}
  Recall that $C^{\varphi}(S) = \sum_{a=1}^n C_a^{\varphi}(S)$, so by linearity of expectation we can focus on $\mathbb{E}_{X \sim x}[C_a^{\varphi}(X)]$. But $C_a^{\varphi}(X) = \varphi(\abs{X}_a)$ where $\abs{X}_a = \abs{\set{ i \in [m] : X_i = 1 \text{ and } a \in T_i}} \in [0,m]$. Thus:
  \[ \mathbb{E}_{X \sim x}[C_a^{\varphi}(X)] = \sum_{k=0}^{m} \mathbb{P}_{X \sim x}(\abs{X}_a=k)\varphi(k)\]

  It remains to compute the distribution of $\abs{X}_a$. But $\abs{X}_a = \sum_{i \in [m] : a \in T_i} X_i$ and $X_i \sim \Ber(x_i)$. Thus, $\abs{X}_a \sim \Poi\Bin((x_i)_{i \in [m] : a \in T_i})$, which is known as the Poisson binomial law. Thanks to \cite{FW10}, we have that:
  \[ \mathbb{P}_{X \sim x}(\abs{X}_a=k) = \frac{1}{m+1}\sum_{\ell = 0}^{m} \omega_{m+1}^{-\ell k}\prod_{j \in [m] : a \in  T_j}(1 +(\omega_{m+1}^{\ell}-1)x_j)\]
  where $\omega_{m+1} := \exp(\frac{2i\pi}{m+1})$, and the result is proved.
\end{proof}

\begin{prop}
  We have that
  \[ \abs{\mathbb{E}[\varphi(\Bin(n,x/n))] - \mathbb{E}[\varphi(\Poi(x))]} \leq \frac{x \varphi(n)}{2n} + \frac{x^{n+1}}{n!} \ .\]
  In particular when $\varphi(n) = o(n)$:
  \[ \lim_{n \rightarrow  \infty} \mathbb{E}[\varphi(\Bin(n,x_{\varphi}/n))] = \mathbb{E}[\varphi(\Poi(x_{\varphi}))] = \alpha_{\varphi}\varphi(x_{\varphi}) \ . \]
  \label{prop:UnboundBinPoi}
\end{prop}

\begin{proof}
  Thanks to \cite{TF19,Barbour84}, we have that the total variation distance between $\Bin(n,x/n)$ and $\Poi(x)$ is bounded in the following way:
  \[ \Delta(\Bin(n,x/n),\Poi(x)) \leq \frac{1 - e^{-x}}{2x} n* \Big(\frac{x}{n}\Big)^2 \leq \frac{x}{2n}\]
  Thus with $B \sim \Bin(n,x/n)$ and $P \sim \Poi(x)$:
  \begin{equation}
    \begin{aligned}
      \abs{\mathbb{E}[\varphi(B)] - \mathbb{E}[\varphi(P)]} &=&&  \abs{\sum_{k=0}^{+\infty}\varphi(k)\mathbb{P}(B=k) - \sum_{k=0}^{+\infty}\varphi(k)\mathbb{P}(P=k)}\\
      &=&&  \abs{\sum_{k=0}^{+\infty}\varphi(k)(\mathbb{P}(B=k) - \mathbb{P}(P=k))}\\
      &\leq&& \sum_{k=0}^{+\infty}\varphi(k)\abs{\mathbb{P}(B=k) - \mathbb{P}(P=k)}\\
      &\leq&& \varphi(n)\Delta(\Bin(n,x/n),\Poi(x))\\
      &+&& \sum_{k=n+1}^{+\infty}\varphi(k)\mathbb{P}(P=k)\\
      &\leq&&\frac{x \varphi(n)}{2n}  + e^{-x}\sum_{k=n+1}^{+\infty}k\frac{x^k}{k!} \quad \text{since } \varphi(k) \leq k\\
      &=&& \frac{x \varphi(n)}{2n}  + xe^{-x}\sum_{k=n}^{+\infty}\frac{x^k}{k!}\\
      &\leq&& \frac{x \varphi(n)}{2n}  + \frac{x^{n+1}}{n!} \underset{n \rightarrow \infty}{\rightarrow} 0 \text{ when } \varphi(n) = o(n)
    \end{aligned}
  \end{equation}
  by a standard upper bound on the remainder of the exponential series.

\end{proof}

\begin{prop}
  The function $g : x \mapsto \mathbb{E}[\varphi(\Poi(x))]$ on $\mathbb{R}^+$ is $\mathcal{C}^{\infty}$ nondecreasing concave.
  \label{prop:PoiCon}
\end{prop}

\begin{proof}
  Since we have that $0 \leq \varphi(k) \leq k$ for $k \in \mathbb{N}$, in particular $g(x) = e^{-x}\sum_{k=0}^{+\infty} \varphi(k)\frac{x^k}{k!}$ is $\mathcal{C}^{\infty}$. It is thus enough to compute its first and second derivatives:

  \begin{equation}
    \begin{aligned}
      g'(x) &=&& -e^{-x}\sum_{k=0}^{+\infty}\varphi(k)\frac{x^k}{k!}+ e^{-x}\sum_{k=1}^{+\infty}\varphi(k)e^{-x}k\frac{x^{k-1}}{k!}\\
      &=&& -e^{-x}\sum_{k=0}^{+\infty}\varphi(k)\frac{x^k}{k!}+ e^{-x}\sum_{k=0}^{+\infty}\varphi(k+1)e^{-x}\frac{x^{k}}{k!}\\
      &=&& e^{-x}\sum_{k=0}^{+\infty}(\varphi(k+1) -\varphi(k))\frac{x^k}{k!} \ .
    \end{aligned}
  \end{equation}
  But $\varphi(k+1) -\varphi(k) \geq 0$ since $\varphi$ nondecreasing, so $g'(x) \geq 0$ and $g$ is nondecreasing.

  
  The calculus of $g''$ is the same where we replace $\varphi$ by $\psi(k) := \varphi(k+1) -\varphi(k)$ which is a nonincreasing function by concavity of $\varphi$. Thus:
  \[g''(x) = e^{-x}\sum_{k=0}^{+\infty}(\psi(k+1) -\psi(k))\frac{x^k}{k!} \leq 0\]
  since $\psi(k+1) -\psi(k) \leq 0$, and so $g$ is concave.
\end{proof}


\begin{prop}
  The function $g_q : n \mapsto \mathbb{E}[\varphi(\Bin(n,q))]$ defined on $\mathbb{N}$ is nondecreasing concave. As a consequence, one can uses Jensen's inequality on the piecewise linear extension of $g_q$ which is also continuous.
  \label{prop:BinCon}
\end{prop}

\begin{proof}
  $\Bin(n,q) \leq_{\text{st}} \Bin(n+1,q)$ and we have that $\varphi$ is nondecreasing, so $\mathbb{E}[\varphi(\Bin(n,q))] \leq \mathbb{E}[\varphi(\Bin(n+1,q))]$, ie $g_q(n+1) - g_q(n) \geq 0$: $g_q$ is nondecreasing.

  We show then the concavity, ie. $g_q(n+2) - g_q(n+1) \leq g_q(n+1) - g_q(n)$. Call $\psi(x) = \varphi(x+1)-\varphi(x)$ which is nonincreasing since $\varphi$ concave. Let us take $X_{k,q} \sim \Bin(k,q)$. Then:

  \begin{equation}
    \begin{aligned}
      g_q(n+1) &=&& \mathbb{E}[\varphi(X_{n+1,q})]\\
      &=&& \sum_{i=0}^n \mathbb{E}[\varphi(X_{n,q}+X_{1,q})|X_{n,q}=i]\mathbb{P}(X_{n,q}=i)\\
      &=&& \sum_{i=0}^n \mathbb{E}[\varphi(i+X_{1,q}) - \varphi(i)]\mathbb{P}(X_{n,q}=i)\\
      &+&& \sum_{i=0}^n \varphi(i)\mathbb{P}(X_{n,q}=i)\\
      &=&& \sum_{i=0}^n \mathbb{E}[\varphi(i+X_{1,q}) - \varphi(i)]\mathbb{P}(X_{n,q}=i) + g_q(n)
    \end{aligned}
  \end{equation}

  Thus:

  \begin{equation}
    \begin{aligned}
      g_q(n+1) -g_q(n) &=&& \sum_{i=0}^n \mathbb{E}[\varphi(i+X_{1,q}) - \varphi(i)]\mathbb{P}(X_{n,q}=i)\\
      &=&& \sum_{i=0}^n q(\varphi(i+1) - \varphi(i))\mathbb{P}(X_{n,q}=i)\\
      &=&& q \mathbb{E}[\psi(\Bin(n,q))]
    \end{aligned}
  \end{equation}

  Then thanks to the fact that  $\Bin(n,q) \leq_{\text{st}} \Bin(n+1,q)$ and $\psi$ is nonincreasing, we have that $\mathbb{E}[\psi(\Bin(n,q))] \geq \mathbb{E}[\psi(\Bin(n+1,q))]$, ie. $g_q(n+2) - g_q(n+1) \leq g_q(n+1) - g_q(n)$.
\end{proof}

\begin{prop}
  With $w_i := \varphi(i)-\varphi(i-1)$, we have:
  \[ \lim_{i \rightarrow +\infty} w_i = 0 \iff \varphi(n) = o(n) \]
  \label{prop:thieleEqLim}
\end{prop}
\begin{proof}
  \begin{itemize}
  \item[($\Rightarrow$)] Let $\epsilon > 0$, let us find a rank $N$ such that for $n \geq N$, $\frac{\varphi(n)}{n} \leq \epsilon$. Let $N_0$ the rank from which $w_i \leq \frac{\epsilon}{2}$ and $N_1$ the rank from which $\frac{1}{n} \sum_{i=1}^{N_0-1} w_i \leq \frac{\epsilon}{2}$. We have
    
    \begin{equation}
      \begin{aligned}
        \frac{\varphi(n)}{n} &=&& \frac{1}{n} \sum_{i=1}^{n} w_i \leq \frac{1}{n} \sum_{i=1}^{N_0-1} w_i + \frac{1}{n} \sum_{i=N_0}^{n-1} \frac{\epsilon}{2}\\
        &\leq&& \frac{\epsilon}{2} + \frac{\epsilon}{2} = \epsilon \text{ for } n \geq \max(N_0,N_1) =: N
      \end{aligned}
    \end{equation}
  \item[($\Leftarrow$)] Since $w_i = \varphi(i)-\varphi(i-1)$ is nonnegative and nonincreasing (respectively because $\varphi$ is nondecreasing and concave), then the sequence $w$ has a limit $L \geq 0$. But
    \[ \frac{\varphi(n)}{n} = \frac{1}{n} \sum_{i=1}^{n} w_i \geq L \ . \]
    
    Since the left hand side tends to $0$ by hypothesis, this means that $L=0$.
  \end{itemize}
\end{proof}

\begin{prop}
  If $\forall x \geq \ell, \varphi(x) = \varphi(\ell) > 0$, then $\alpha_{\varphi}(x)$ is nondecreasing from $\ell$ to $+\infty$ and $\alpha_{\varphi}(x) =  \frac{\varphi(\ell) - e^{-x}\sum_{k=0}^{\ell-1} (\varphi(\ell)-\varphi(k))\frac{x^k}{k!}}{\varphi(x)}$. In particular, $\alpha_{\varphi} = \min_{x \in [\ell]} \alpha_{\varphi}(x)$, and the argmin can be found numerically.
  \label{prop:phiCapped}
\end{prop}

\begin{proof}
  Let $x \geq \ell$, then:
  \[ \alpha_{\varphi}(x) = \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(x)} = \frac{e^{-x}\sum_{k=0}^{+\infty} \varphi(k)\frac{x^k}{k!}}{\varphi(\ell)} = \frac{e^{-x}\sum_{k=0}^{\ell-1} (\varphi(k)-\varphi(\ell))\frac{x^k}{k!} + \varphi(\ell)}{\varphi(\ell)} \ . \]
  In particular, thinking of $x$ as a real number, $\alpha_{\varphi}(x)$ is differentiable and:
  \begin{equation}
    \begin{aligned}
      \alpha_{\varphi}'(x) &=&& -\frac{e^{-x}}{\varphi(m)}\sum_{k=0}^{\ell-1} (\varphi(k)-\varphi(\ell))\frac{x^k}{k!}  + \frac{e^{-x}}{\varphi(\ell)}\sum_{k=0}^{m-2} (\varphi(k+1)-\varphi(\ell))\frac{x^k}{k!}\\
      &=&& \frac{e^{-x}}{\varphi(\ell)}\sum_{k=0}^{\ell-1} (\varphi(k+1)-\varphi(k))\frac{x^k}{k!} \geq 0 \text{ since } \varphi \text{ nondecreasing \ . }
    \end{aligned}
  \end{equation}
  Thus $\alpha_{\varphi}(x)$ is nondecreasing from $\ell$ to $+\infty$ and the result on $\alpha_{\varphi}$ follows.
\end{proof}

\begin{prop}
  If $w_i := \varphi(i) - \varphi(i-1)$ is \emph{geometrically dominant}, ie. $\forall i \in \mathbb{N}^*, \frac{w_i}{w_{i+1}} \leq \frac{w_{i+1}}{w_{i+2}}$, then the function $\alpha_{\varphi}(x)$ is nondecreasing from $1$ to $+\infty$ and thus $\alpha_{\varphi} = \alpha_{\varphi}(1)$.
  \label{prop:geoDomMin}
\end{prop}

\begin{proof}
  We will need the following lemma derived from the geometrically dominant property:
  \begin{lem}
    $\forall k \in \mathbb{N}, j \in \mathbb{N}^*, w_{k+1} \geq w_j \Big(\frac{w_{j+1}}{w_j}\Big)^{k+1-j}$
    \label{lem:geoDomIneq}
  \end{lem}
  \begin{proof}[Proof of Lemma \ref{lem:geoDomIneq}]
    Let us first suppose that $k+1 \geq j$. Let us show by induction on $k$ in that case that the result holds.

    If $k+1=j$ the inequality becomes trivial: $w_j \geq w_j \Big(\frac{w_{j+1}}{w_j}\Big)^{0} = w_j$. Let us now assume that the result is fulfilled for $k$, let us show it for $k+1$. By geometric dominance, we have that $\frac{w_{k+2}}{w_{k+1}} \geq \frac{w_{k+1}}{w_{k}} \geq \ldots \geq \frac{w_{j+1}}{w_j}$. Then, multiplying this last inequality with the rank $k$ inequality gives:
    \[ w_{k+2} = \frac{w_{k+2}}{w_{k+1}}w_{k+1} \geq \frac{w_{j+1}}{w_j} w_j \Big(\frac{w_{j+1}}{w_j}\Big)^{k+1-j} = w_j \Big(\frac{w_{j+1}}{w_j}\Big)^{k+2-j}\]
    
    On the other way around, let us now assume that $k+1 \leq j$ and prove the result on induction on $-k$. The base case is the same as before. Now, assume it is true for $k+1 \leq j$ and show it for $k$. But $\frac{w_{k+1}}{w_{k}} \leq \frac{w_{k+2}}{w_{k+1}} \leq \ldots \leq \frac{w_{j+1}}{w_j}$ so $\frac{w_{k}}{w_{k+1}} \leq \Big(\frac{w_{j+1}}{w_j}\Big)^{-1}$ and again multiplying this inequality with the one at rank $k$ enables us to get the result:
    \[ w_{k} = \frac{w_{k}}{w_{k+1}}w_{k+1} \geq \Big(\frac{w_{j+1}}{w_j}\Big)^{-1} w_j \Big(\frac{w_{j+1}}{w_j}\Big)^{k+1-j} = w_j \Big(\frac{w_{j+1}}{w_j}\Big)^{k-j}\]
    
  \end{proof}

  
  Then, recall from Proposition \ref{prop:PoiCon} that $g : x \mapsto \mathbb{E}[\varphi(\Poi(x))]$ is $\mathcal{C}^{\infty}$ and that:

  \begin{equation}
    \begin{aligned}
      g'(x) &=&& e^{-x}\sum_{k=0}^{+\infty}(\varphi(k+1) -\varphi(k))\frac{x^k}{k!} =  e^{-x}\sum_{k=0}^{+\infty}w_{k+1}\frac{x^k}{k!}\\
      &\geq&& e^{-x} w_j \Big(\frac{w_{j+1}}{w_j}\Big)^{-(j-1)}\sum_{k=0}^{+\infty}\frac{\Big(\frac{w_{j+1}}{w_j}x\Big)^k}{k!} \quad \text{thanks to Lemma \ref{lem:geoDomIneq}}\\
      &=&& w_{j+1}\Big(\frac{w_{j+1}}{w_j}\Big)^{-j} e^{-x} e^{\frac{w_{j+1}}{w_j}x} = w_{j+1}c^{-j} e^{(c-1)x} \quad \text{with } c = \frac{w_{j+1}}{w_j}
    \end{aligned}
  \end{equation}

  Since $\varphi$ is piecewise linear, it is derivable on $\mathbb{R}^+$ except on integers where it has only a left derivative and a right derivative, and so does the function $\alpha_{\varphi}(x)$. Let us take $x \in [j,j+1]$ for $j \in \mathbb{N}^*$. Note that here $\varphi(x)$ and thus $\alpha_{\varphi}(x)$ are derivable on the whole interval. In order to get $\alpha_{\varphi}(x)$ nondecreasing from $1$ to $+\infty$, it is enough to show that on all those intervals $[j,j+1], \alpha_{\varphi}'(x) \geq 0$ , since the function $\alpha_{\varphi}(x)$ is continuous on $\mathbb{R}^+$. Let us compute $\alpha'_{\varphi}(x)$:

  \[ \alpha'_{\varphi}(x) = \frac{g'(x)\varphi(x)-g(x)\varphi'(x)}{\varphi(x)^2}\]

  since $\alpha'_{\varphi}(x) = \frac{g(x)}{\varphi(x)}$. But $g'(x) \geq w_{j+1}c^{-j} e^{(c-1)x}$ and $g(x) \leq \varphi(\mathbb{E}[\Poi(x)]) = \varphi(x)$ by Jensen's inequality on $\varphi$ concave, thus we have:

  \begin{equation}
    \alpha'_{\varphi}(x) \geq \frac{\varphi(x)[w_{j+1}c^{-j} e^{(c-1)x} - \varphi'(x)]}{\varphi(x)^2} = \frac{w_{j+1}c^{-j} e^{(c-1)x} - \varphi'(x)}{\varphi(x)} 
  \end{equation}

  Since $\varphi$ is linear in $[j,j+1]$, we have that $\varphi'(x) = \varphi(j+1) - \varphi(j) = w_{j+1}$. Thus:

  \[\alpha'_{\varphi}(x) \geq \frac{w_{j+1}c^{-j}}{\varphi(x)} \Big(e^{(c-1)x} - c^j\Big) \geq \frac{w_{j+1}c^{-j}}{\varphi(x)} \Big(e^{(c-1)j} - c^j\Big) \geq 0\]

  since $e^{t-1} \geq t$ and then $e^{(t-1)j} \geq t^j$ for $t \geq 0$. Thus $\alpha_{\varphi}(x)$ is nondecreasing from $1$ to $+\infty$, and the result is proved.
\end{proof}

\section{Calculations of $\alpha_{\varphi}$}
\begin{prop}
  For $\ell \in \mathbb{N}^*$ and $\varphi(j) = \min(j,\ell)$, we have that $\alpha_{\varphi}=1-\frac{\ell^{\ell}e^{-\ell}}{\ell!}$.
  \label{prop:lCover}
\end{prop}
\begin{proof}
  Thanks to Proposition \ref{prop:BetterRatio}, we have that $\alpha_{\varphi} = \inf_{x \in \mathbb{N}^*} \alpha_{\varphi}(x)$. Let us compute $\mathbb{E}[\varphi(\Poi(x))]$:

  \begin{equation}
    \begin{aligned}
      \mathbb{E}[\varphi(\Poi(x))] &=&& e^{-x}\sum_{k=0}^{+\infty}\varphi(x)\frac{x^k}{k!}\\
      &=&& e^{-x}\sum_{k=0}^{\ell}k\frac{x^k}{k!} + e^{-x}\sum_{k=\ell+1}^{+\infty}\ell\frac{x^k}{k!}\\
      &=&& e^{-x}x \sum_{k=0}^{\ell-1}\frac{x^k}{k!} + \ell e^{-x}\sum_{k=\ell+1}^{+\infty}\frac{x^k}{k!}\\
      &=&& e^{-x}\Big[(x-\ell)\sum_{k=0}^{\ell-1}\frac{x^k}{k!} - \ell\frac{x^{\ell}}{\ell!}\Big] + \ell e^{-x}\sum_{k=0}^{+\infty}\frac{x^k}{k!}\\
      &=&& \ell - e^{-x}\Big[\frac{x^{\ell}}{(\ell-1)!} - (x-\ell)\sum_{k=0}^{\ell-1}\frac{x^k}{k!}\Big] \ .
    \end{aligned}
  \end{equation}

  Let us show that $\alpha_{\varphi}(x)$ takes its minimum in $\ell$, where we have indeed:
  \[ \alpha_{\varphi}(\ell) = \frac{1}{\ell}\Big( \ell - e^{-\ell}\Big[\frac{\ell^{\ell}}{(\ell-1)!} - (\ell-\ell)\sum_{k=0}^{\ell-1}\frac{\ell^k}{k!}\Big]\Big) = 1 - e^{-\ell}\frac{\ell^{\ell}}{\ell!} \ . \]

  Thanks to proposition \ref {prop:phiCapped}, $\alpha_{\varphi}(x)$ is nondecreasing from $\ell$ to $+\infty$. Suppose now that $\ell \geq 2$ (otherwise the result is already proved). Since $\alpha_{\varphi}(x)$ is derivable, we have for $1 \leq x \leq \ell$:
  \begin{equation}
    \begin{aligned}
      \alpha_{\varphi}'(x) &=&& -\frac{\ell}{x^2} + e^{-x}\Big[\frac{x^{\ell-1}}{(\ell-1)!} -  \sum_{k=0}^{\ell-1}\frac{x^k}{k!}  + \ell\sum_{k=0}^{\ell-2}\frac{x^k}{(k+1)!} + \frac{\ell}{x}\Big]\\ 
      &-&& e^{-x}\Big[\frac{x^{\ell-2}}{(\ell-2)!} - \sum_{k=0}^{\ell-2}\frac{x^k}{k!} + \ell\sum_{k=0}^{\ell-3}\frac{x^k}{(k+2)k!} - \frac{\ell}{x^2}\Big]\\
      &=&&  \frac{\ell}{x}\Big(e^{-x}\Big(1+\frac{1}{x}\Big) - \frac{1}{x}\Big)\\
      &+&&  e^{-x}\Big[\Big(\frac{\ell}{\ell-1}-1\Big)\frac{x^{\ell-2}}{(\ell-2)!} + \ell\sum_{k=0}^{\ell-3}\Big(\frac{x^k}{(k+1)!} - \frac{x^k}{(k+2)k!} \Big) \Big]\\
      &=&&  \frac{\ell}{x}\Big(e^{-x}\Big(1+\frac{1}{x}\Big) - \frac{1}{x}\Big)\\
      &+&&  e^{-x}\Big[\frac{x^{\ell-2}}{(\ell-1)!} + \ell\sum_{k=0}^{\ell-3}\frac{x^k}{k!}\Big(\frac{1}{k+1} - \frac{1}{k+2} \Big) \Big]\\
      &=&&  \frac{\ell}{x}\Big(e^{-x}\Big(1+\frac{1}{x} + \frac{x^{\ell-1}}{\ell!} + x\sum_{k=0}^{\ell-3}\frac{x^k}{k!}\frac{1}{(k+1)(k+2)}\Big) - \frac{1}{x}\Big)\\
      &=&&  \frac{\ell e^{-x}}{x^2}\Big(\Big(1+x+ \frac{x^\ell}{\ell!} + \sum_{k=0}^{\ell-3}\frac{x^{k+2}}{(k+2)!}\Big) - e^x\Big)\\
      &=&& \frac{\ell e^{-x}}{x^2}\Big(\sum_{k=0}^{\ell} \frac{x^k}{k!} -e^x\Big) \leq 0
    \end{aligned}
  \end{equation}
  since the partial sum of the exponential series is bounded by its total sum. Thus $\alpha_{\varphi}(x)$ is nonincreasing from $1$ to $\ell$, and nondecreasing after, so it takes indeed its minimum in $\ell$ and the proposition is proved.
\end{proof}

\begin{prop}
  For $p \in (0,1)$ and $\varphi(j)=\frac{1-(1-p)^j}{p}$, we have that $\alpha_{\varphi} = \frac{1 - e^{-p}}{p}$.
\label{prop:VTA}
\end{prop}

\begin{proof}
    By definition:
    
    \begin{equation}
      \begin{aligned}
        \alpha_{\varphi}(x) &=&& \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(x)} = \frac{\sum_{k=0}^{+\infty}\varphi(k)e^{-x}\frac{x^k}{k!}}{\varphi(x)}\\
        &=&& \frac{1-e^{-x}\sum_{k=0}^{+\infty}(1-p)^k\frac{x^k}{k!}}{p\varphi(x)}\\
        &=&& \frac{1 - e^{-x}e^{(1-p)x}}{p\varphi(x)} = \frac{1 - e^{-px}}{p\varphi(x)} \ .
      \end{aligned}
    \end{equation}

    If $x \geq 1$, $\alpha_{\varphi}(x) = \frac{1 - e^{-px}}{1-(1-p)^x}$ and:
    \[\alpha_{\varphi}'(x) = \frac{pe^{-px}(1-(1-p)^x) - \ln(1-p)(1-p)^x(1-e^{-px})}{(1-(1-p)^x)^2} \ .\]
    
    But $1-(1-p)^x > 0, 1-e^{-px} > 0, \ln(1-p) < 0$ since $x > 0$ and $p \in ]0,1[$, so $\alpha_{\varphi}'(x) > 0$ for $x \geq 1$ and so $\alpha_{\varphi}(x)$ increases from $1$ to infinity. Thus it takes indeed its minimum in $1$:
    
    \[\alpha_{\varphi} = \alpha_{\varphi}(1) = \frac{1 - e^{-p}}{p} \ .\]
\end{proof}
\begin{prop}
  For $d \in (0,1)$ and $\varphi(j)=j^d$, we have that $\alpha_{\varphi} = e^{-1}\sum_{k=1}^{+\infty}\frac{k^d}{k!}$
    \label{prop:dPower}
\end{prop}
\begin{proof}
  We have for $x \geq 1$:
    \[\alpha_{\varphi}(x) = \frac{\mathbb{E}[\Poi(x)^d]}{\varphi(x)} = \frac{e^{-x}\sum_{k=0}^{+\infty}k^d\frac{x^k}{k!}}{\varphi(x)} = e^{-x}\sum_{k=0}^{+\infty}k^d\frac{x^{k-d}}{k!} \ . \]

   Then:
   \begin{equation}
     \begin{aligned}
       \alpha_{\varphi}'(x) &=&& -\alpha_{\varphi}(x)+ e^{-x}\sum_{k=1}^{+\infty}(k-d)k^d\frac{x^{k-d-1}}{k!} \\
       &=&& -\alpha_{\varphi}(x)+ e^{-x}\sum_{k=0}^{+\infty}(k+1-d)(k+1)^d\frac{x^{k-d}}{(k+1)!}\\
       &=&& -\alpha_{\varphi}(x)+ e^{-x}\Big((1-d)x^{-d} +  \sum_{k=1}^{+\infty}(k+1-d)(k+1)^{d-1}\frac{x^{k-d}}{k!}\Big)\\
       &=&& e^{-x}x^{-d}\Big(1-d + \sum_{k=1}^{+\infty}(\frac{k+1-d}{k+1}(k+1)^d - k^d)\frac{x^k}{k!}\Big) \ . 
     \end{aligned}
   \end{equation}

   But the function $f(k) = \frac{k+1-d}{k+1}(k+1)^d - k^d$ is positive on $\mathbb{R}_+^*$, so we get that $\alpha_{\varphi}'(x) > 0$ for $x \geq 1$, thus $\alpha_{\varphi}(x)$ is increasing from $1$ to $+\infty$, so $\alpha_{\varphi} = \alpha_{\varphi}(1) = e^{-1}\sum_{k=1}^{+\infty}\frac{k^d}{k!}$.
\end{proof}

\section{NP-hardness of $\delta,h$-\textsc{AryGapLabelCover}}
\label{NPhardnessGap}
\begin{proof}[Proof of Proposition \ref{prop:AryGapLabelCover}]
  We reduce from the Label Cover problem described in \cite{DDMS20} which is known no be an NP-hard problem. The main idea of this reduction is the usual equivalence between a bipartite graph and a hypergraph.

  \begin{defi}
    A Label Cover instance $\mathcal{L} = (A,B,E,[L],[R],\set{\pi_e}_{e \in E})$ consists of a bi-regular bipartite graph $(A,B,E)$ with right degree $t$, alphabet sets $[L],[R]$ and for every edge $e \in E$, a constraint $\pi_e :[L] \rightarrow [R]$.
    A \emph{labelling} of $\mathcal{L}$ is a function $\sigma : A \rightarrow [L]$. We say that $\sigma$ \emph{strongly satisfies} a right vertex $v \in B$ if for every two neighbours $u,u'$ of $v$, we have $\pi_{(u,v)}(\sigma(u)) = \pi_{(u',v)}(\sigma(u'))$. Moreover, we say that  $\sigma$ \emph{weakly satisfies} a right vertex $v \in B$ if there exists two neighbours $u,u'$ of $v$ such that $\pi_{(u,v)}(\sigma(u)) = \pi_{(u',v)}(\sigma(u'))$.
  \end{defi}

 \begin{theo}[$\delta$-Gap-Label-Cover$(t,R)$ from \cite{DDMS20}]
  For any fixed integer $t \geq 2$ and fixed $\delta > 0$, there exists $R$ that can be taken as large as we want, such that it is NP-hard for Label Cover instances $\mathcal{L} = (A,B,E,[L],[R],\set{\pi_e}_{e \in E})$ with right degree $t$ to distinguish between:
  
  \begin{itemize}
  \item[\textbf{YES:}] There exists a labeling $\sigma$ that \emph{strongly satisfies} all the right vertices.
  \item[\textbf{NO:}] No labeling \emph{weakly satisfies} more than $\delta$ fraction of the right vertices.
  \end{itemize}
\end{theo}

 The reduction is the following. From $\delta$-Gap-Label-Cover$(t,R)$, we take $h=t$ and the same parameters $\delta,R$. Given an instance $\mathcal{L} = (A,B,E,[L],[R],\set{\pi_e}_{e \in E})$, we take $\mathcal{G} = (A,E',[L],[R],\set{\pi'_{e',v}}_{e' \in E',v \in e'})$ with $E' = \set{N(b), b \in B}$ with $N(b)$ the set of neighbours of $b$ in $\mathcal{L}$, and $\pi'_{e',v} = \pi'_{N(b),v} := \pi_{v,b}$ since $v \in N(b)$.
 Since $(A,B,E)$ is bipartite and biregular, we get that our hypergraph has all hyperedges of size $h = \abs{N(b)} = t$, and that it is regular from the regular left degree of $(A,B,E)$. By construction, the notion of weakly and strongly satisfied is the same in both cases, as well as the labellings, and thus we have the NP-hardness of $\delta,h$-\textsc{AryGapLabelCover}.
 
\end{proof}

\section{Proof of existence of partitioning systems}
\label{app:Partitioning}

\begin{proof}[Proof of Proposition \ref{prop:Partitioning}]
    The existential proof is based on the probabilistic method. We take $\mathcal{P}_i$ an $h$-equi-sized uniform random $x_{\varphi}$-cover of $[n]$. Hence in the collection $\mathcal{P}_i=(P_{i,1},\ldots,P_{i,h})$, each of the $h$ subsets is of cardinality $x_{\varphi}n/h$. Write $\mathcal{P} = (\mathcal{P}_1,\ldots,\mathcal{P}_R)$. We have that for any $a \in [n], \mathbb{P}(a \in P_{i,j}) = x_{\varphi}/h$. Note that these events are independent for different $i$s.

    By construction, the first condition is fulfilled. Let us prove the second one.

    Fix $T \subseteq [R]$ and $\mathcal{Q} := \set{P_{i,j(i)} : i \in T}$ for some function $j : T \rightarrow [h]$. We have for $a \in [n]$:
    \[ \mathbb{E}[C_a^{\varphi}(\mathcal{Q})] =  \mathbb{E}[\varphi(\abs{\mathcal{Q}}_a)] = \mathbb{E}[\varphi(\abs{\set{i \in T: a \in P_{i,j(i)}}})]\]

    But the random variables $\set{X_i := \mathbbm{1}_{a \in P_{i,j(i)}}}_{i \in T}$ are independent and $X_i \sim \Ber(x_{\varphi}/h)$, so $X :=\abs{\set{i \in T: a \in P_{i,j(i)}}} = \sum_{i \in T} X_i \sim \Bin(\abs{T},x_{\varphi}/h)$, and thus:
    \[\mathbb{E}[C_a^{\varphi}(\mathcal{Q})] = \mathbb{E}[\varphi(\Bin(\abs{T},x_{\varphi}/h))] =\psi^{\varphi}_{\abs{T},h}\]

    Since $\abs{\mathcal{Q}}_a\leq \abs{\mathcal{Q}} \leq R$ and $\varphi$ nondecreasing, we have $0 \leq C_a^{\varphi}(\mathcal{Q}) \leq \varphi(R)$ and we can apply Azuma-Hoeffding bound on $C^{\varphi}(\mathcal{Q}) = \sum_{a \in [n]} C_a^{\varphi}(\mathcal{Q})$:

    \[ \mathbb{P}\Big( \abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} > \eta n\Big) \leq 2 \text{exp}\Big(-\Big(\frac{\eta}{\varphi(R)}\Big)^2n\Big)\]

    Since there are at most $(h+1)^R$ choices of $T$ and $\mathcal{Q}$, a union bound gives:
    \[ \mathbb{P}\Big(\exists C,\mathcal{Q} : \abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} > \eta n\Big) \leq 2(h+1)^R \text{exp}\Big(-\Big(\frac{\eta}{\varphi(R)}\Big)^2n\Big)\]

    Thus with probability at least $9/10$, we have that $\abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} \leq \eta n$, since we have taken $n \geq \eta^{-2}R\varphi(R)^2\log(20(h+1))$. So there must exists some choice of $\mathcal{P}$ that satisfies the first and second constraints of partitioning systems. Thus, we can enumerate over all choices of $\mathcal{P}$ in time exp($Rn$log$n).$poly$(h)$ to find such a partitioning system.
\end{proof}

\section{Proof of Theorem \ref{theo:HardnessMA}}
\label{app:HardnessMA}
\begin{proof}
We show that  $\varphi$-\textsc{Resource Allocation} problem corresponds to $\varphi$-\textsc{MaxCoverage} under a matroid constraint. Given an instance of the $\varphi$-\textsc{Resource Allocation}, consider the partition matroid $\mathcal{M}$ on $[\sum_{i \in[k]} m_i] := [m_1] + \ldots + [m_k]$, where $(B_i)_{i \in [k]} := ([m_i])_{i \in [k]}$ is a partition of the ground set and the cardinality constraint  for each $i$ is to $d_i=1$. 

Here, $I \subseteq [\sum_{i \in[k]} m_i]$ is an independent set of the matroid iff $\abs{I \cap B_i} \leq d_i = 1$, for all $i \in [k]$. This corresponds to each agent $i \in [k]$ selecting at most one element from the available $m_i$ choices. In other words, we have a bijection $f$ between tuples $(A_1, \ldots, A_k) \in \mathcal{A}_1 \times \ldots \times \mathcal{A}_k$ and maximal independent sets (bases) of $\mathcal{M}$ such that $W^{\varphi}(A) = C^{\varphi}(f(A))$. Therefore, Theorem~\ref{theo:AlgoMat} leads to a polynomial-time $\alpha_{\varphi}$-approximation algorithm for $\varphi$-\textsc{Resource Allocation}. 


 %welfare maximization of the welfare of multiple agents is then solved with our algorithm, which implies in particular that it is $\alpha_{\varphi}$-approximable in polynomial time thanks to theorem \ref{theo:AlgoMat}.

For the hardness part of the theorem, the proof is exactly the same as in Theorem \ref{theo:Hardness}, but instead of $\mathcal{F} := \set{F^v_{\beta}, v \in V, \beta \in [L]}$ and $k = \abs{V}$, we take  $k = \abs{V}$ to be the number of agents and $\mathcal{A}_i := \set{F^{v_i}_{\beta}, \beta \in [L]}$ where $V =\set{v_1, \ldots, v_k}$. Hence, instead of subsets of $\mathcal{F}$ of size $k$, we only consider one set $F^v_{\beta} \in \mathcal{F}$, for each $v \in V$. The function we maximize in the reduction remains unchanged. 

To establish completeness, we note that the subset described is already of the right form and, hence, the arguments continue to hold. For proving soundness, the constraint on the shape of the subset of $\mathcal{F}$ only helps us, since it gives more constraints on the given subset from which we want to construct a labelling. Therefore, both parts of the proof work and the {\rm NP}-hardness follows.
\end{proof}

%\end{multicols}
\end{document}
