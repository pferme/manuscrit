\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{multicol}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{xparse}
\usepackage{physics}
\usepackage{empheq}
\usepackage{url}
\usepackage{hyperref}
\usepackage[affil-it]{authblk}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{quotes,angles,calc}
\usepackage{rotating}
\usepackage{graphicx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output

%\usepackage{wasysym}
%\usepackage{listings}
%\usepackage{moreverb}

\usepackage[top=3cm,bottom=3cm,right=4cm,left=4cm]{geometry}

\theoremstyle{definition}
\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{prop}[theo]{Property}
\newtheorem{defi}[theo]{Definition}

\theoremstyle{remark}
\newtheorem*{rk}{Remark}

\DeclareMathOperator{\Poi}{\text{Poi}}
\DeclareMathOperator{\Ber}{\text{Ber}}
\DeclareMathOperator{\Bin}{\text{Bin}}
\DeclareMathOperator{\maxi}{\text{maximize}}
\DeclareMathOperator{\mini}{\text{minimize}}
\DeclareMathOperator{\st}{\text{subject to}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiterX\set[1]\lbrace\rbrace{\def\given{\;\delimsize\vert\;}#1}

\colorlet{darkgreen}{green!40!black}

\title{Generalization of Maximum Coverage}
\author{Paul Fermé\\ Advisers: Omar Fawzi, Siddharth Barman}
\affil{Laboratoire de l’Informatique du Parallélisme, École Normale Supérieure de Lyon}
\date{}

\begin{document}

\maketitle
\tableofcontents
%\newpage

\section{Definitions, notations, properties}
\begin{itemize}
\item Function $\varphi$: concave, nondecreasing, $\varphi(0)=0$, $\varphi(1)=1$.
\item $T_1, \ldots, T_m \subseteq [n]$ cover sets, $S \subseteq [m]$ choice of cover sets.
\item $\Gamma_e(S) := \set{i \in S : e \in T_i}$, $\Gamma_e : =\Gamma_e([m])$ and $k_e(S) = \abs{\Gamma_e(S)}$
\item Objective function $C^{\varphi}(S) := \sum_{e \in [n]} C^{\varphi}_e(S)$ where $C^{\varphi}_e(S) := \varphi(\abs{\Gamma_e(S)}) = \varphi(k_e(S))$.
\item Problem studied : $\max_{S \subseteq [m], \abs{S} \leq k} C^{\varphi}(S)$
\end{itemize}

\begin{rk}
  Case $\varphi(j):= \min(\ell,j)$ studied in \cite{BFGG20}
\end{rk}

\begin{defi}
  A set function $f$ is \emph{submodular} iff for all $S \subseteq T \subseteq [m] $ and $x \not\in T$ we have:
  \[ f(S | x) := f(S \cup \set{x}) - f(S) \geq f(T \cup \set{x}) - f(T)\]
  We define the \emph{curvature} $c_f$ of $f$ in the following way:
  \[ c_f := 1 - \min_{i \in [m]} \frac{f([m]-\set{i}|i)}{f(\emptyset|i)}\]
  which always lies between $0$ and $1$.
\end{defi}

\begin{prop}[\cite{SVW17}]
  There exists a polynomial-time algorithm that gives an approximation ratio of  $1 - c e^{-1}$ for our problem for a submodular objective function with curvature $c$.
\end{prop}

\begin{prop}
  $C^{\varphi}$ is submodular, and its curvature is at most $1 - (\varphi(m) - \varphi(m-1))$.
\end{prop}

\begin{lem}[Properties of $k_e$]
  We have:
  \begin{enumerate}
  \item $k_e(S) \leq \abs{S}$
  \item $k_e(S \cup S') \leq k_e(S) + k_e(S')$. In particular, if $S \subseteq T$ then $k_e(S) \leq k_e(T)$ and $k_e(S \cup \set{x}) \leq k_e(S) + 1$.
  \item If $S \subseteq T$, $x \not\in T$ then $k_e(S) = k_e(T) \Rightarrow k_e(S \cup \set{x}) = k_e(T \cup \set{x})$
  \end{enumerate}
  \label{lemke}
\end{lem}

\begin{proof}
  Let $S \subseteq T \subseteq [m]$ and $x \not\in T$.
  \begin{equation}
    \begin{aligned}
      &&& C^{\varphi}(S|x) - C^{\varphi}(T|x) = \sum_{e \in [n]} C^{\varphi}_e(S|x) - C^{\varphi}_e(T|x) \\
      &=&& \sum_{e \in [n]} \varphi(k_e(S \cup \set{x})) - \varphi(k_e(S)) - (\varphi(k_e(T \cup \set{x})) - \varphi(k_e(T)))\\
    \end{aligned}
  \end{equation}

  Let us call $g(e) := \varphi(k_e(S \cup \set{x})) - \varphi(k_e(S)) - (\varphi(k_e(T \cup \set{x})) - \varphi(k_e(T)))$:
  \begin{enumerate}
  \item If $k_e(T) = k_e(S)$ then thanks to lemma \ref{lemke}, we have that $k_e(T \cup \set{x}) = k_e(S \cup \set{x})$, so $g(e) = 0$
  \item Else, we have that $k_e(T) > k_e(S)$:
    \begin{enumerate}
    \item If $k_e(S \cup \set{x}) = k_e(S)$, then we add elements of $T-S$ using lemma \ref{lemke} to get that $k_e(T \cup \set{x}) = k_e(T)$, so $g(e)=0$ in that case.
    \item Else $k_e(S \cup \set{x}) \not= k_e(S)$. So with $k_e(S) = k$, we get that $k_e(S \cup \set{x}) = k+1$ and $k_e(T) > k_e(S)$ so $k_e(T) \geq k+1$.

      \begin{enumerate}
      \item If $k_e(T \cup \set{x}) = k_e(T)$, then $g(e) =  \varphi(k+1) - \varphi(k) \geq 0$ since $\varphi$ is nondecreasing.
      \item Else $k_e(T \cup \set{x}) \not= k_e(T)$ so with $k_e(T) = \ell$ with $\ell \geq k+1$, we get that $k_e(T) = \ell+1$. So we have that:
        \[g(e) = \varphi(k+1) - \varphi(k) - (\varphi(\ell+1) - \varphi(\ell)) = \frac{\varphi(k+1) - \varphi(k)}{(k+1) - k} - \frac{\varphi(\ell+1) - \varphi(\ell)}{(\ell+1) - \ell} \geq 0\]
        by concavity of $\varphi$: its slopes are nonincreasing.
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
  So in all cases, we have $g(e) \geq 0$ so $C^{\varphi}(S|x) - C^{\varphi}(T|x)  \geq 0$: $C^{\varphi}$ is submodular.

  Let us now compute its curvature:
  \[c = 1 - \min_{i \in [m]} \frac{C^{\varphi}([m]-\set{i}|i)}{C^{\varphi}(\emptyset|i)}\]

  Let $i \in [m]$ fixed!:
  \begin{equation}
    \begin{aligned}
      \frac{C^{\varphi}([m]-\set{i}|i)}{C^{\varphi}(\emptyset|i)} &=&& \frac{\sum_{e \in [n]} \varphi(k_e([m]-\set{i}|i))}{\sum_{e \in [n]}\varphi(k_e(\emptyset|i))} = \frac{\sum_{e \in T_i} \varphi(k_e([m]-\set{i}|i))}{\abs{T_i}}\\
      &=&& \frac{\sum_{e \in T_i} \varphi(k_e([m])) - \varphi(k_e([m])-1)}{\abs{T_i}} \text{ since } e \in T_i
    \end{aligned}
  \end{equation}

  But $k_e([m]) \leq n$ and $\varphi$ concave, so $\varphi(k_e([m])) - \varphi(k_e([m])-1) \geq \varphi(m) - \varphi(m-1)$ for all $e \in [n]$. as a consequence we have that:
  \begin{equation}
    \begin{aligned}
      \frac{C^{\varphi}([m]-\set{i}|i)}{C^{\varphi}(\emptyset|i)} &=&& \frac{\sum_{e \in T_i} \varphi(k_e([m])) - \varphi(k_e([m])-1)}{\abs{T_i}} \text{ since } e \in T_i\\
      &\geq&& \varphi(m) - \varphi(m-1)
    \end{aligned}
  \end{equation}
  
  and this lower bound is true for its minimum over $i$. Thus we get that $c \leq 1 - (\varphi(m) - \varphi(m-1))$.
\end{proof}


\newpage

\section{Theorem and its lemmas}
Based on \cite{BFGG20}, use of \cite{StochasticOrders}

\begin{defi}[Relaxed Program]
  \begin{equation}
    \begin{aligned}
      &\maxi&& \sum_{e \in [n]} c_e \\
      &\st&& c_e \leq \varphi\Big(\sum_{i \in \Gamma_e} x_i\Big)\\
      &&& 0 \leq x_i \leq 1\\
      &&& \sum_{i=1}^m x_i = k
    \end{aligned}
  \end{equation}
  \label{relaxedProg}
\end{defi}

Some new necessary lemmas proved at the end:

\begin{lem}
  For $\varphi$ concave:
  \[\alpha_{k,\ell} := \frac{\mathbb{E}[\varphi(\Poi(k)+\ell)]}{\varphi(k+\ell)} =  \frac{\mathbb{E}[\varphi(\Poi(k)+\ell)]}{\varphi(\mathbb{E}[\Poi(k) + \ell])} \in [0,1] \]
  satisfies $\alpha_{k,\ell} \geq \alpha_{k+\ell,0}$.
  \label{lempoi}
\end{lem}

\subsection{Main theorem}
\begin{theo}
  Let $(x_i),(c_e)$ a feasible solution of the program \ref{relaxedProg} and $X_i \sim \Ber(x_i)$. Let us call $\alpha = \inf_{i \in ]0,m]} \alpha_{i,0}$. Then:
      \[ \mathbb{E}[C^{\varphi}(X_1,\ldots,X_m)] \geq \alpha \sum_{e \in [n]} c_e \]
      Thus, this gives an approximation ratio for our problem as the mininimum of $\alpha_{i,0} = \frac{\mathbb{E}[\varphi(\Poi(i))]}{\varphi(\mathbb{E}[\Poi(i)])}$, with the polynomial-time pipage rounding strategy of the relaxed program.
\end{theo}

\begin{proof}
  We follow the structure of the proof of theorem 2.1 through lemma 2.4 in \cite{BFGG20}.
  By linearity, it is sufficient to show that:
  \[ \mathbb{E}[C_e^{\varphi}(X_1,\ldots,X_m)] \geq \alpha c_e \]

  To make notations lighter, we write $C_e^{\varphi} := C_e^{\varphi}(X_1,\ldots,X_m)$, $S_X$ the random set corresponding to $X_1,\ldots,X_m$:
  
  \begin{equation}
    \begin{aligned}
      \mathbb{E}[C_e^{\varphi}] = \mathbb{E}[\varphi(\abs{\Gamma_e(S_X)})] = \mathbb{E}\Big[\varphi\Big(\sum_{i \in \Gamma_e} X_i\Big)\Big] = \sum_{a=0}^m \varphi(a)\mathbb{P}\Big(\sum_{i \in \Gamma_e} X_i = a\Big)
    \end{aligned}
  \end{equation}

  We use the lemma 2.5 from \cite{BFGG20} in order to get $(x_i)$ that minimize the value of $\mathbb{E}[C_e^{\varphi}]$ with $x_i \in \set{0,q,1}$ for all $i \in [m]$ and $\sum_{i=1}^m x_i = k$ for some fixed $q \in (0,1)$. We assume then that the values of the relaxed program are in this shape.

  Let $\ell = \abs{\set{i \in \Gamma_e : x_i = 1}}$ and $t = \abs{\set{i \in \Gamma_e : x_i = q}}$. In particular we have that $\ell+t \leq m$.

  \[
  \mathbb{E}[C_e^{\varphi}] = \sum_{a=0}^{m-\ell} \varphi(a+\ell)\mathbb{P}\Big(\sum_{i \in \Gamma_e} X_i - \ell = a\Big) =  \sum_{a=0}^t \varphi(a+\ell)\binom{t}{a}q^a(1-q)^{t-a}
  \]

  since $\sum_{i \in \Gamma_e} X_i - \ell \sim \Bin(t,q)$ and it has zero probability for $a > t$.

  We have then that $\mathbb{E}[C_e^{\varphi}] = \sum_{a=0}^t \varphi_{\ell}(a)\binom{t}{a}q^a(1-q)^{t-a} = \mathbb{E}\Big[\varphi_{\ell}\Big(\Bin\Big(t,q\Big)\Big)\Big]$ with $\varphi_{\ell}(x) := \varphi(\ell+x)$ is the translate of $\varphi$ which is is also concave. Thus we can use lemma 2.3 from \cite{BFGG20} to replace the binomial law by some Poisson law, thanks to the convex ordering between those distributions.
  
  \begin{equation}
    \begin{aligned}
      \mathbb{E}[C_e^{\varphi}] &=&& \mathbb{E}\Big[\varphi_{\ell}\Big(\Bin\Big(t,q\Big)\Big)\Big] \geq \mathbb{E}[\varphi_{\ell}(\Poi(qt))] = \alpha_{qt,\ell}\varphi(\ell+qt)\\
      &\geq&&  \alpha_{\ell+qt,0} \varphi(\ell+qt) \text{ thanks to lemma \ref{lempoi} with } k = qt\\
      &\geq&& \alpha c_e
    \end{aligned}
  \end{equation}
  since $c_e \leq \varphi\Big(\sum_{i \in \Gamma_e} x_i\Big) = \varphi(\ell+qt)$, and if $c_e > 0$, then $\ell+qt > 0$ since $\varphi(x) \leq 0 \iff x=0$, so $\alpha_{\ell+qt,0} \geq \alpha$.
\end{proof}

\begin{rk}
  In the end of the proof, in fact we have that $\ell+qt = \sum_{i \in \Gamma_e} x_i \leq k$, so it would be enough  to look at the minimum in $]0,k]$ for $\alpha$. In order to have a ratio independent of $k$, we would anyway need to get back to the former coefficient $\alpha$.
\end{rk}

\subsection{Lemmas proofs}
\begin{proof}[Proof of Lemma \ref{lempoi}]
  We have that
  \begin{equation}
    \begin{aligned}
      \alpha_{k,\ell} \geq \alpha_{k+\ell,0} &\iff&&  \mathbb{E}[\varphi(\Poi(k)+\ell)] \geq \mathbb{E}[\varphi(\Poi(k+\ell))]\\
      &\iff&& \Poi(k)+\ell \leq_{\text{cx}} \Poi(k+\ell)
    \end{aligned}
  \end{equation}
  since we want this inequality to be true for all $\varphi$ concave (see \cite{StochasticOrders}). Let $X_i,Y_j \sim \Poi(1)$
  We have \[\sum_{i=1}^{k} X_i+\ell \sim \Poi(k)+\ell\] and \[\sum_{j=1}^{k+\ell} Y_j \sim \Poi(k+\ell)\]
  by standard property of Poisson law, so it remains to show that:
  \[ \sum_{i=1}^{k} X_i+\ell \leq_{\text{cx}} \sum_{j=1}^{k+\ell} Y_j \]

  Furthermore, we have that $1 = \mathbb{E}[Y_j] \leq_{\text{cx}} Y_j$, and $X_i \leq_{\text{cx}} Y_i$ since they follow the same law. Then, since the sum of random variables, ie. convolution, is compatible with this order (see \cite{StochasticOrders}, Theorem 3.A.24 and Theorem 3.A.12), then we have that:
  \[ \sum_{i=1}^{k} X_i+\sum_{j=1}^{\ell} 1 \leq_{\text{cx}} \sum_{i=1}^{k} Y_i + \sum_{j=k+1}^{\ell}Y_j\]
  ie.
  \[ \sum_{i=1}^{k} X_i+\ell \leq_{\text{cx}} \sum_{j=1}^{k+\ell} Y_j \]
\end{proof}

\section{Applications}
\subsection{Linear functions}
When
\[\varphi(x) = \begin{cases}
  x \text{ if } 0 \leq x \leq 1\\
  1 + a(x-1) \text{ if } x \geq 1
\end{cases}
\]

for $a \in [0,1]$. Then

\begin{equation}
  \begin{aligned}
    \mathbb{E}[\varphi(\Poi(x))] &=&& e^{-x} \sum_{t \geq 1} \frac{x^t}{t!}(1+a(t-1)) = 1 + a(x-1) - e^{-x} (1-a)\\
    &=&& 1 - e^{-x} + a(x-1+e^{-x}) 
  \end{aligned}
\end{equation}

Thus $\frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(x)}$ is minimal on $1$, and is then equal to:
\[1 - (1-a)e^{-1}\]

But here the curvature of $C^{\varphi}$ is $c = 1 - (\varphi(m)-\varphi(m-1)) = 1-a$, thus we get the same approximation ratio as the general strategy of a submodular function with some curvature $c$

MOTO: Here we have the worst case curvature whereas we compute an \emph{average} one ! In the case of linear functions, they are the same.

\subsection{Piecewise linear functions}

When
\[\varphi(x) = \begin{cases}
  x \text{ if } 0 \leq x \leq 1\\
  1 + a(x-1) \text{ if } 1 \leq x \leq n\\
  \psi(x) := 1 + a(n-1) + b(x-n) \text{ if } x \geq n
\end{cases}
\]

with $a,b \in [0,1]$ and $b < a$. Then

\begin{equation}
  \begin{aligned}
    \mathbb{E}[\varphi(\Poi(x))] &=&& e^{-x} \Big(\sum_{t = 1}^n \frac{x^t}{t!}(1+a(t-1)) + \sum_{t > n}\frac{x^t}{t!}(1 + a(n-1) + b(t-n))\Big)\\
    &=&& e^{-x} \Big((1-a)\Big(\sum_{t = 0}^n \frac{x^t}{t!} - 1\Big) + xa \sum_{t = 0}^{n-1} \frac{x^t}{t!}\\
    &+&& (1 + a(n-1) - bn)\sum_{t > n}\frac{x^t}{t!} + xb\sum_{t \geq n}\frac{x^t}{t!}\Big)\\
    &=&&  e^{-x} \Big((1-a)\Big(\sum_{t = 0}^n \frac{x^t}{t!} - 1\Big) + xb e^x + x(a-b)\sum_{t = 0}^{n-1}\frac{x^t}{t!}\\
    &+&& (1 + a(n-1) - bn)\Big(e^x - \sum_{t = 0}^n\frac{x^t}{t!}\Big)\Big)\\
    &=&& 1 + a(n-1) + b(x-n) - e^{-x} \Big(1-a \\
    &+&& (1 + a(n-1) - bn - (1-a))\sum_{t = 0}^n\frac{x^t}{t!} + x(b-a)\Big(\sum_{t = 0}^n \frac{x^t}{t!} - \frac{x^n}{n!}\Big)\Big)\\
    &=&& \psi(x) - e^{-x}\Big(1-a + (a-b)\Big( \frac{x^{n+1}}{n!}-(x-n)\sum_{t = 0}^n\frac{x^t}{t!} \Big)  \Big)
  \end{aligned}
\end{equation}

Thus we have the formula:
\[ \mathbb{E}[\varphi(\Poi(x))] =\psi(x) - e^{-x}\Big(1-a + (a-b)\Big( \frac{x^{n+1}}{n!}-(x-n)\sum_{t = 0}^n\frac{x^t}{t!} \Big)  \Big) \]

In the particular case of $n=\ell,a=1,b=0$, we recover the example from \cite{BFGG20}. We get then that:

\[ \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(x)} =\frac{\psi(x)}{\varphi(x)} - \frac{e^{-x}}{\varphi(x)}\Big( \frac{x^{\ell+1}}{\ell!}-(x-\ell)\sum_{t = 0}^{\ell}\frac{x^{\ell}}{\ell!} \Big) \]

It takes indeed its minimum value on $\mathbb{R}^+$ at the minimum of the values taken in $x=\ell$ or $x=1$ from the following property, and here it will be in $x=\ell$. Then, we recover the expected value from our previous formula:

\[ \frac{\mathbb{E}[\varphi(\Poi(\ell))]}{\varphi(\ell)} =1 - \frac{e^{-\ell}}{\ell}\Big( \frac{\ell^{\ell+1}}{\ell!}-(\ell-\ell)\sum_{t = 0}^{\ell}\frac{\ell^{\ell}}{\ell!} \Big)  = 1 - e^{-\ell}\frac{\ell^{\ell}}{\ell!}\]

Thus our analysis englobes completely the case studied in \cite{BFGG20}.

One can show that the minimal value is either at $1$ or at $n$, since this function is decreasing from $0$ to $1$, then concave from $1$ to $n$ (TO PROVE), and then increasing from $n$, so the ratio would be in general equals to $\min(\mathbb{E}[\varphi(\Poi(1))],\frac{\mathbb{E}[\varphi(\Poi(n))]}{\varphi(n)})$. Thus is is the minimum of
\[ 1 + (a-b)(n-1) - \Big(1-a + (a-b)\Big(\frac{1}{n!}+(n-1)\sum_{t = 0}^n\frac{1}{t!}\Big)\Big)e^{-1}\]
and
\[ 1 - \frac{1-a + (a-b)\frac{n^{n+1}}{n!}}{1 - a + an} e^{-n} \]

In particular, if we take $a = b$, we recover our previous example with the minimum value obtained in $1$.
In all thoses cases, it will be greater than $1 - (1-b)e^{-1}$, which is the value obtained from the curvature from the general algorithm. Indeed, one can show that, if we consider only the coefficient before $(a-b)$ and let the rest in $b$ unchanged (we increase $a$), in the first case we get that the value becomes
\begin{equation}
  \begin{aligned}
    &&& 1 - (1-b)e^{-1} + (a-b)(n-1 - \Big(-1+\Big(\frac{1}{n!}+(n-1)\sum_{t = 0}^n\frac{1}{t!}\Big)\Big)e^{-1}\\
    &=&& 1 - (1-b)e^{-1} +  (a-b)\Big(1-\Big(\frac{1}{n!}-(n-1)R_{n+1}\Big)\Big)e^{-1}\\
    &=&& 1 - (1-b)e^{-1} + (a-b)\Big(\Big(1-\frac{1}{n!}\Big)+(n-1)R_{n+1}\Big)\Big)e^{-1} > 1 - (1-b)e^{-1}\\
  \end{aligned}
\end{equation}

as soon as $n>1$ and $a>b$, where $R_{n+1} = \sum_{t \geq n+1}\frac{1}{t!} > 0$. Let us call it $f(x) := 1 - (1-b)e^{-1} + x\Big(\Big(1-\frac{1}{n!}\Big)+(n-1)R_{n+1}\Big)\Big)e^{-1}$. In the other case, the formula is:

\begin{equation}
  \begin{aligned}
    &&& 1 - \frac{1-b + (a-b)\Big(\frac{n^{n+1}}{n!}-1\Big)}{1 - b + bn + (a-b)(n-1)}e^{-n}
  \end{aligned}
\end{equation}

So as a function of (a-b), we get $g(x) := 1 - \frac{1-b + x\big(\frac{n^{n+1}}{n!}-1\big)}{1 - b + bn + x(n-1)}e^{-n}$. If we look at the minimum $h(x) := \min(f(x),g(x))$, we can show that $h(x) > h(0)$ for $x \in ]0,1-b]$ as soon as $n > 1$: $f(0) = 1-(1-b)e^{-1} < g(0) = 1 - \frac{1-b}{1 - b + bn}e^{-n}$ so $h(0) = f(0) = (1-b)e^{-1}$. We have seen that $f$ is increasing. One can show that $g$ is decreasing ($g'(x) = \frac{ne^{-n}}{(1-b + x(\frac{n^{n+1}}{n!}-1))^2}(1-(1+b(n-1))\frac{n^n}{n!}) < 0$), so it is enough to show that $g(1-b) > h(0) = 1-(1-b)e^{-1}$ to get the result. But:

\[ g(1-b) = 1 - \frac{(1-b)\frac{n^{n+1}}{n!}}{1 - b + bn + (1-b)(n-1)}e^{-n} = 1 - (1-b)\frac{n^n}{n!}e^{-n} > 1-(1-b)e^{-1}\]

since $\frac{n^n}{n!}e^{-n}$ is a decreasing sequence. Thus we get a strictly better approximation factor in any of those cases than the standard greedy algorithm.

\section{Generalization to multiple agents and matroid contraints}
\subsection{Main Generalized Theorem}
Inspired by \cite{PM19}, we study a generalization of this problem from game theory where we consider $k$ multiple agents, each one choose one cover set among his different possibilities $\mathcal{A}_i := \set{\mathcal{A}_i^1,\ldots,\mathcal{A}_i^{m_i}} \subseteq \mathcal{P}([n])$, but now they won't have all the same cover sets and we add a positive weight on each elements $e \in [n]$ which we call $w_e$ (this corresponds to the weighted maximum coverage problem where $\mathcal{A}_i = \set{T_1,\ldots,T_m}$). As a remark, we should note that in our particular problem, we can take multiple times each cover set, so it is not exactly the same problem yet when we count multiply many times each element. Thus it is not clear that our current problem is a particular case of the multiple agnets case. TO SOLVE

The value we want to maximize is the global welfare, among possible allocations $a = (a_1,\ldots,a_k) \in \mathcal{A} := \mathcal{A}_1 \times \ldots \times \mathcal{A}_k$, which will be given by the following formula:

\[W(a) := \sum_{e \in [n]} w_e\varphi(\abs{a}_e)\]

where $\abs{a}_e :=  \abs{\set{i \in [k] : e \in a_i}}$.

In order to formulate this problem in a more convenient way for us, we consider the partition matroid $\mathcal{M}$ on $[\sum_{i \in[k]} m_i] := [m_1] + \ldots + [m_k]$ where $(B_i) := ([m_i])$ is a partition of our ground set and we take each degree $d_i=1$. Independent sets are then subsets $I \subseteq [\sum_{i \in[k]} m_i]$ such that $\forall i  \in [k], \abs{I \cap B_i} \leq d_i = 1$.
Thus we have a bijection between independent sets of $\mathcal{M}$ and allocations $a \in \mathcal{A}^{\emptyset} := \mathcal{A}_1 \cup \set{\emptyset} \times \ldots \times \mathcal{A}_k \cup \set{\emptyset}$, which corresponds in the previous case to sets of size smaller than $k$.

Then for $S \subseteq [\sum_{i \in[k]} m_i]$, call $\Gamma_e(S) := \set{(i,j) \in S : e \in \mathcal{A}_i^j}$ (where $i$ denotes in which $B_i$ we are and $j$ the element of that $B_i$), and $\Gamma_e := \Gamma_e([\sum_{i \in[k]} m_i])$. Then the objective function we want to maximize among $S \in \mathcal{M}$ is given by:
\[ C^{\varphi}(S) := \sum_{e \in [n]} C_e^{\varphi}(S) \text{ where } C_e^{\varphi}(S) := w_e\varphi(\abs{\Gamma_e(S)})\]

And we look at the following problem : $\max_{S \in \mathcal{M}} C^{\varphi}(S)$. Thus we have that the problem of maximizing the welfare of the multiple agents case is a particular case of the generalized weighted maximum coverage problem under matroid constraints.

We see in particular that the function $C^{\varphi}(S)$ we study is the same as the one we studied previously, but for a different ground set and for a maximization under a matroid constraint. We can define similarly at what has be done before a program that computes the value we are intersted in:

\begin{defi}[Matroid Program]
  \begin{equation}
    \begin{aligned}
      &\maxi&& \sum_{e \in [n]} w_e c_e \\
      &\st&& c_e \leq \varphi\Big(\sum_{(i,j) \in \Gamma_e} x_{i,j}\Big)\\
      &&& x_{i,j} \in \set{0,1} \text{ (we remove these constraints in the relaxed program)}\\
      &&& x_{i,j} \in P(\mathcal{M}) \text{, the matroid polytope of } \mathcal{M}
    \end{aligned}
  \end{equation}
  \label{matroidProg}
\end{defi}

We can see that the constraints in the particular case of the matroid we have chosen for the multiple agents case are given by $0 \leq x_{i,j} \leq 1$ and $\sum_j x_{i,j} = 1$ (which implies the weaker constraint $\sum_{i,j} x_{i,j} = k$), so those are linear constraints, and we get that the relaxed program is linear when $\varphi$ is given as linear constraints.

However we can make the following analysis in the broader case of a general matroid, and thus we recover our initial problem but just with a general matroid constraint and weights. There exists an equivalent result for pipage rounding for matroids, so this part of the proof does not change. Then, the weight won't change either the proof, since we use linear arguments when it appears. Thus we get the same result in the broader case of matroid constraints, and in particular of multiple agents:

\begin{theo}
  Let $m = \sum_{i=1}^{k}m_k$.  Let $(x_i),(c_e)$ a feasible solution of the relaxed version of the program \ref{matroidProg} and $X_i \sim \Ber(x_i)$. Let us call $\alpha = \inf_{i \in ]0,m]} \alpha_{i,0}$. Then:
      
      \[ \mathbb{E}[C^{\varphi}(X_1,\ldots,X_m)] \geq \alpha \sum_{e \in [n]} w_ec_e \]
      
      Since this expectation is the approximation ratio obtained with the pipage rounding strategy of a linear program with a matroid contraint, we get an approximation ratio for our problem as the mininimum of $\alpha_{i,0} = \frac{\mathbb{E}[\varphi(\Poi(i))]}{\varphi(\mathbb{E}[\Poi(i)])}$.
\end{theo}

\begin{proof}
  We follow the structure of the proof of theorem 2.1 through lemma 2.4 in \cite{BFGG20}.
  By linearity, it is sufficient to show that:
  \[ \mathbb{E}[C_e^{\varphi}(X_1,\ldots,X_m)] \geq \alpha w_ec_e \]
  
  To make notations lighter, we write $C_e^{\varphi} := C_e^{\varphi}(X_1,\ldots,X_m)$, $S_X$ the random set corresponding to $X_1,\ldots,X_m$:
  
  \begin{equation}
    \begin{aligned}
      \mathbb{E}[C_e^{\varphi}] = \mathbb{E}[w_e\varphi(\abs{\Gamma_e(S_X)})] = w_e\mathbb{E}\Big[\varphi\Big(\sum_{i \in \Gamma_e} X_i\Big)\Big] = w_e\sum_{a=0}^m \varphi(a)\mathbb{P}\Big(\sum_{i \in \Gamma_e} X_i = a\Big)
    \end{aligned}
  \end{equation}
  
  We use the lemma 2.5 from \cite{BFGG20} in order to get $(x_i)$ that minimize the value of $\mathbb{E}[C_e^{\varphi}]$ with $x_i \in \set{0,q,1}$ for all $i \in [m]$ and $\sum_{i=1}^m x_i = k$ for some fixed $q \in (0,1)$. We assume then that the values of the relaxed program are in this shape.
  
  Let $\ell = \abs{\set{i \in \Gamma_e : x_i = 1}}$ and $t = \abs{\set{i \in \Gamma_e : x_i = q}}$. In particular we have that $\ell+t \leq m$.
  
  \[
  \mathbb{E}[C_e^{\varphi}] = w_e\sum_{a=0}^{m-\ell} \varphi(a+\ell)\mathbb{P}\Big(\sum_{i \in \Gamma_e} X_i - \ell = a\Big) =  w_e\sum_{a=0}^t \varphi(a+\ell)\binom{t}{a}q^a(1-q)^{t-a}
  \]
  
  since $\sum_{i \in \Gamma_e} X_i - \ell \sim \Bin(t,q)$ and it has zero probability for $a > t$.
  
  We have then that $\mathbb{E}[C_e^{\varphi}] = w_e\sum_{a=0}^t \varphi_{\ell}(a)\binom{t}{a}q^a(1-q)^{t-a} = w_e\mathbb{E}\Big[\varphi_{\ell}\Big(\Bin\Big(t,q\Big)\Big)\Big]$ with $\varphi_{\ell}(x) := \varphi(\ell+x)$ is the translate of $\varphi$ which is is also concave. Thus we can use lemma 2.3 from \cite{BFGG20} to replace the binomial law by some Poisson law, thanks to the convex ordering between those distributions. The inequalities work as before since $w_e \geq 0$:
  
  \begin{equation}
    \begin{aligned}
      \mathbb{E}[C_e^{\varphi}] &=&& w_e\mathbb{E}\Big[\varphi_{\ell}\Big(\Bin\Big(t,q\Big)\Big)\Big] \geq w_e\mathbb{E}[\varphi_{\ell}(\Poi(qt))] = \alpha_{qt,\ell}w_e\varphi(\ell+qt)\\
      &\geq&&  \alpha_{\ell+qt,0} w_e\varphi(\ell+qt) \text{ thanks to lemma \ref{lempoi} with } k = qt\\
      &\geq&& \alpha w_ec_e
    \end{aligned}
  \end{equation}
  since $c_e \leq \varphi\Big(\sum_{i \in \Gamma_e} x_i\Big) = \varphi(\ell+qt)$, and if $c_e > 0$, then $\ell+qt > 0$ since $\varphi(x) \leq 0 \iff x=0$, so $\alpha_{\ell+qt,0} \geq \alpha$.
\end{proof}

\subsection{Comparison with \cite{PM19}}
We want to compare our approximation ratio in the case of $\varphi(j) := j^d$ where $0 < d < 1$ and $j \in \mathbb{N}$, since it is the case where the strategy of finding the worst Nash Equilibrium seems to be particularly efficient in \cite{PM19}, which can be found in polynomial time with the round-robin algorithm (TO CHECK). This is what is usually called \emph{Price of Anarchy} (PoA) in Game Theory.

Let us compute our ratio $\alpha(x) := \alpha_{x,0}$ and $\alpha = \inf_{x \in ]0,m]} \alpha(x)$.
    \[\alpha(x) = \frac{\mathbb{E}[\Poi(x)^d]}{\varphi(x)} = \frac{e^{-x}\sum_{k=0}^{\infty}k^d\frac{x^k}{k!}}{\varphi(x)} \]
    
    However, if we take $\varphi(x) := x^d$ for $x \in \mathbb{R}$, then $\alpha(x) = e^{-x}\sum_{k=1}^{\infty}k^d\frac{x^{k-d}}{k!} \sim_0  e^{-x}x^{1-d} \rightarrow 0$ when $x \rightarrow 0$, so we wont get a satisfactory bound. Instead, in order to get $\alpha$ as big as possible, we should take $\varphi$ as small as possible. In fact it seems that is only necessary to take
    \[
    \varphi(x) := \begin{cases}
      x \text{ if } 0 \leq x \leq 1\\
      x^d \text{ if } x \geq 1
        \end{cases}
    \]
    
    to get the best $\alpha$ possible. In that case we get that $\alpha(x) \sim_0 e^{-x} \rightarrow 1$
    
    We will show that $\alpha(x)$ decreases from $0$ to $1$ and then increases up to infinity. Thus $\alpha = \alpha(1) = e^{-1}\sum_{k=1}^{\infty}\frac{k^d}{k!}$. In particular, when $d=1$, we get $\alpha = e^{-1} \sum_{k=1}^{\infty}\frac{1}{(k-1)!} = 1$ and when $d=0$, we get $\alpha =e^{-1} \sum_{k=1}^{\infty}\frac{1}{k!} = e^{-1}(e-1) = 1 - e^{-1}$, so we recover what is expected.
    
    Let us prove this. If $x \in ]0,1[$, then $\alpha(x) =  e^{-x}\sum_{k=1}^{\infty}k^d\frac{x^{k-1}}{k!}$ and:
    \begin{equation}
      \begin{aligned}
        \alpha'(x) &=&& -\alpha(x)+ e^{-x}\sum_{k=2}^{\infty}(k-1)k^d\frac{x^{k-2}}{k!} \\
        &=&& -\alpha(x)+ e^{-x}\sum_{k=1}^{\infty}k(k+1)^d\frac{x^{k-1}}{(k+1)!}\\
        &=&& -\alpha(x)+ e^{-x}\Big(\sum_{k=1}^{\infty}k(k+1)^{d-1}\frac{x^{k-1}}{k!}\Big)\\
        &=&& e^{-x}\Big(\sum_{k=1}^{\infty}(k(k+1)^{d-1} - k^d)\frac{x^{k-1}}{k!}\Big)\\
      \end{aligned}
    \end{equation}
    
    But $k(k+1)^{d-1} - k^d = k((k+1)^{d-1} - k^{d-1}) < 0$ for $k>0$ since $d-1 < 0$. Thus $\alpha'(x) < 0$ for $x \in ]0,1[$. If $x \in ]1,+\infty[$:
        
   \begin{equation}
     \begin{aligned}
       \alpha'(x) &=&& -\alpha(x)+ e^{-x}\sum_{k=1}^{\infty}(k-d)k^d\frac{x^{k-d-1}}{k!} \\
       &=&& -\alpha(x)+ e^{-x}\sum_{k=0}^{\infty}(k+1-d)(k+1)^d\frac{x^{k-d}}{(k+1)!}\\
       &=&& -\alpha(x)+ e^{-x}\Big((1-d)x^{-d} +  \sum_{k=1}^{\infty}(k+1-d)(k+1)^{d-1}\frac{x^{k-d}}{k!}\Big)\\
       &=&& e^{-x}x^{-d}\Big(1-d + \sum_{k=1}^{\infty}(\frac{k+1-d}{k+1}(k+1)^d - k^d)\frac{x^k}{k!}\Big)\\
     \end{aligned}
   \end{equation}

   But the function $f(k) = \frac{k+1-d}{k+1}(k+1)^d - k^d$ is positive on $\mathbb{R}_+^*$, so we get tha $\alpha'(x) > 0$ for $x \in ]1,+\infty[$, QED.

   When we plot $\alpha$ in function of $d$ as in \cite{PM19} page 5, then it seems that we get \emph{exactly} the same values as their PoA$_{\text{opt}}$ (which is the PoA for the best \emph{local} strategy of agents in order to maximize the global welfare). We have plotted that \href{https://www.geogebra.org/classic/fpjzmvs2}{here} (in black, $\alpha$ in function of $d$, which seems to match the PoA$_{\text{opt}}$ in black also, and in red in both cases, the usual approximation we get from the curvature computation as in \cite{PM19}).

\subsection{Application to the Vehicle-Target Assignement Problem}
Continuin our comparison to \cite{PM19}, we look at an application of interest of the use of concave nondecreasing $\varphi$ functions, the so called \emph{Vehicle-Target Assignement Problem}. For our concerns, we should only focus on the form of $\varphi$ which depends only on a parameter $p \in ]0,1[$. Then $\varphi(j) = \frac{1-(1-p)^j}{p}$ for $j \in \mathbb{N}$. In order to compute an efficient lower bound on the ratio $\alpha(x)$ defined as before, we need to extend $\varphi$ on $\mathbb{R}$ in the following way:
       
    \[
    \varphi(x) := \begin{cases}
      x \text{ if } 0 \leq x \leq 1\\
      \frac{1-(1-p)^x}{p} \text{ if } x \geq 1
    \end{cases}
    \]

    Then we will show that $\alpha(x) = \frac{1 - e^{-px}}{p\varphi(x)}$ takes its minimum in $1$, when its value amounts $\alpha = \alpha(1) = \frac{1-e^{-p}}{p}$.

    By definition:
    \begin{equation}
      \begin{aligned}
        \alpha(x) &=&& \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(x)} = \frac{\sum_{k=0}^{\infty}\varphi(k)e^{-x}\frac{x^k}{k!}}{\varphi(x)} = \frac{1-e^{-x}\sum_{k=0}^{\infty}(1-p)^k\frac{x^k}{k!}}{p\varphi(x)}\\
        &=&& \frac{1 - e^{-x}e^{(1-p)x}}{p\varphi(x)} = \frac{1 - e^{-px}}{p\varphi(x)}
      \end{aligned}
    \end{equation}

    Thus if $x \in ]0,1[$, $\alpha(x) = \frac{1 - e^{-px}}{px}$ and:

    \[\alpha'(x) = \frac{pe^{-px}px-p(1-e^{-px})}{(px)^2} = \frac{e^{-px}(px-1)-1}{px^2}\]

    But $px^2 > 0, e^{-px} > 0$ and $px-1 < 0$ here so $\alpha'(x) < 0$ for $x \in ]0,1[$, so $\alpha$ is decreasing from $0$ to $1$.

    On the other hand, if $x \in ]1,+\infty[$, then  $\alpha(x) = \frac{1 - e^{-px}}{1-(1-p)^x}$ and:

    \[\alpha'(x) = \frac{pe^{-px}(1-(1-p)^x) - \ln(1-p)(1-p)^x(1-e^{-px})}{(1-(1-p)^x)^2}\]
        
    But $1-(1-p)^x > 0, 1-e^{-px} > 0, \ln(1-p) < 0$ since $x > 0$ and $p \in ]0,1[$, so $\alpha'(x) > 0$ for $x \in ]1,+\infty[$ and so $\alpha$ increases from $1$ to infinity.

    Thus alpha takes indeed its minimum in $1$ where $\alpha(1) = \frac{1 - e^{-p}}{p}$. Again it seems to match (and even slightly outperforms, because in that case, there were weights taken uniformly to compute the performance) the value achieved with PoA$_{\text{opt}}$ in \cite{PM19} page 5. We can see this \href{https://www.geogebra.org/classic/wrgdvw3m}{here}.

\subsection{General conjecture for PoA of multiple agents games}

Inspired by a result in \cite{CPM19} where they have found that the PoA in the limit of $k$ players when $k \rightarrow \infty$ equals the $\ell$-coverage bound found in \cite{BFGG20}, and the fact that for enough players in the two previous examples, the bouunds seem to match, we formulate here the conjecture that those values are equal.

In order to understand how one could prove this, we try to state formally this conjecture.

First, what is $\alpha$ for us in our conjecture? Recall that the main property it fulfills is the following:
\[ \mathbb{E}[C^{\varphi}(X_1,\ldots,X_m)] \geq \alpha \sum_{e \in [n]} w_ec_e = \alpha \times \textbf{LP}\]

So we will look at $\alpha_k$ as the following ratio for $k$ players:
\[ \alpha_k := \frac{ \mathbb{E}[C^{\varphi}(X_1,\ldots,X_m)] }{\textbf{LP}}\]

What is the solution of the matroid program, which we have called \textbf{LP} here? We look at the relaxed program version of \ref{matroidProg}. There is only one constraint on each $c_e$, so since all $w_e > 0$ and we want to maximize $\sum_e w_e c_e$, then we have that $c_e = \varphi\Big(\sum_{i \in \Gamma_e} x_i\Big)$ for an optimal solution, thus:

\[ \textbf{LP} = \sum_{e \in [n]} w_e \varphi\Big(\sum_{(i,j) \in \Gamma_e} x_{i,j}\Big)\]

But the $(x_{i,j})$ follow the only constraints $0 \leq x_{i,j} \leq 1$ and $\sum_j x_{i,j} = 1$, ie for each $i$,  $(x_{i,j})_j$ is a probability distribution. Recall that $x_{i,j}$ computes the probability that we choose $\mathcal{A}_i^j$ as the set choice of the $i$-th agent.

Let $a = (a_1,\ldots,a_k) \in \mathcal{A} = \mathcal{A}_1 \times \ldots \times \mathcal{A}_k$. We write $\sigma$ for probability distributions on $\mathcal{A}$, with $\sigma(a)$ the probability of getting $a$.

Given $(x_{i,j})$, we define $\sigma$ in the following way:
\[ \sigma((\mathcal{A}_1^{j_1},\ldots,\mathcal{A}_k^{j_k})) := \prod_{i=1}^kx_{i,j_i}  \]

which is by construction a probability distribution. Then we ask:


\[\sum_{(i,j) \in \Gamma_e} x_{i,j} = \mathbb{E}_{S \sim (x_{i,j})}[\abs{\Gamma_e(S)}] \overset{?}{=} \mathbb{E}_{a \sim \sigma}[\abs{a}_e] = \sum_{a \in \mathcal{A}} \sigma(a)\abs{a}_e \]

where $\abs{a}_e := \abs{\set{i \in [k] : e \in a_i}}$. We use the bijection $b$ we spoked before between maximal independent sets $S$ of the matroid $\mathcal{M}$ and allocations of $\mathcal{A}$.
Then with $S_a = b^{-1}(a) = \set{(i,a_i)_{i \in [k]}}$, $\mathbb{P}(S = S_a) = \prod_{i=1}^kx_{i,a_i}\prod_{\text{other } (i,j)}(1-x_{i,j})$ and $\abs{\Gamma_e(S_a)} = \abs{a}_e$
BUG: we have outcomes which do not correspond to correct instances (is also the case for the standard uniform matroid: maybe we cannot have that kind of equality).

GUESS: False. But maybe more or less true when $k \rightarrow \infty$. For now, we have that $\mathbb{P}(S \supseteq S_a)  =  \prod_{i=1}^kx_{i,a_i}$. When $k$ is big should not be too far from what we want. For now, we will have only that:

\[ \mathbb{E}_{S \sim (x_{i,j})}[\abs{\Gamma_e(S)}] \leq \mathbb{E}_{S \not\in \set{S_a}}[\abs{\Gamma_e(S)}] + \mathbb{E}_{a \sim \sigma}[\abs{a}_e] \]

SUPPOSE WE HAVE EQUALITY. Thus the value of the LP is the following:

\[ \textbf{LP} = \max_{\sigma \textbf{ product}}\sum_{e \in [n]} w_e \varphi\Big(\mathbb{E}_{a \sim \sigma}[\abs{a}_e]\Big)\]

Call $\sigma_{\text{algo}}$ a solution of the LP. Then we have that:
\[\mathbb{E}[C^{\varphi}(X_1,\ldots,X_m)] = \mathbb{E}_{a \sim \sigma_{\text{algo}}}[W(a)]\]

where $W(a) = \sum_{e \in [n]} w_e \varphi(\abs{a}_e)$. Thus the definition of our ratio $\alpha_k$ becomes:

\[ \alpha_k = \frac{\mathbb{E}_{a \sim \sigma_{\text{algo}}}[W(a)]}{W(\mathbb{E}_{a \sim \sigma_{\text{algo}}}[a])} \leq  \frac{\mathbb{E}_{a \sim \sigma_{\text{algo}}}[W(a)]}{\max_{a \in \mathcal{A}}W(a)} \] 

with a slight abuse of notation for the denominator whose value is defined as
\[ W(\mathbb{E}_{a \sim \sigma_{\text{algo}}}[a]) := \sum_{e \in [n]} w_e \varphi\Big(\mathbb{E}_{a \sim \sigma_{\text{algo}}}[\abs{a}_e]\Big) \]

and knowing that $W(\mathbb{E}_{a \sim \sigma_{\text{algo}}}[a])$ is bigger than $\max_{a \in \mathcal{A}}W(a)$ since we have maximized over a larger space.

On the other hand, what is computed in \cite{CPM19} is the following:

\[ \text{PoA}_k^* := \frac{ \max_f \min_{\sigma \in \text{ACCE}(G,f)} \mathbb{E}_{a \sim \sigma}[W(a)]}{\max_{a \in \mathcal{A}}W(a)}\]

where
\begin{equation}
  \begin{aligned}
    \sigma \in \text{ACCE}(G,f) &\iff&& \forall a' \in \mathcal{A}, \sum_{a \in \mathcal{A}} \sigma(a)\sum_{i \in [k]} U_i(a) \geq \sum_{a \in \mathcal{A}} \sigma(a)\sum_{i \in [k]} U_i(a_i,a'_{-i})\\
    &\iff&&  \forall a' \in \mathcal{A}, \mathbb{E}_{a \sim \sigma}\Big[\sum_{i \in [k]} U_i(a)\Big] \geq \mathbb{E}_{a \sim \sigma}\Big[\sum_{i \in [k]} U_i(a_i,a'_{-i})\Big]
  \end{aligned}
\end{equation}
with $U_i(a) = \sum_{e \in a_i} w_e f(\abs{a}_e)$ and $(a_i,b_{-i}) := (b_1,\ldots,b_{i-1},a_i,b_{i+1},\ldots,b_k)$ (which is a weaker notion of Nash Equilibrium, and in the case of the games we study it gives the same value (see \cite{CPM19} theorem 2 + 3 + the fact that we look at scalable local cost-sharing games)).

Our stronger conjecture is the following:
\[ \max_f \min_{\sigma \in \text{ACCE}(G,f)} \mathbb{E}_{a \sim \sigma}[W(a)] =  \mathbb{E}_{a \sim \sigma_{\text{algo}}}[W(a)]\]

with maybe the same distribution that would work in both cases.

A weaker form would be that the left part is just greater or equal than the right part, and that:
\[ \alpha = \lim_{k \rightarrow +\infty} \alpha_k = \lim_{k \rightarrow +\infty} \text{PoA}_k^*\]

which seemed to be fulfilled for all the examples we have seen so far.

The main idea behind this inequality / equality at infinity is that for the LP, me maximize $\sigma$ inside the concave $\varphi$, but we look at the expectancy outside of $\varphi$. It is some kind of heuristic local maximization. On the other hand, the game theory strategy looks at the worst average equilibrium of the best local strategy. We hope we can relate those two quantities which have some common 'local' optimization behaviour.

\section{Hardness Proof of Generalized Maximum Coverage}
Based on \cite{BFGG20} section 3.

We take the same conjecture on $h$-\textsc{AryUniqueGames} with parameter $\delta$. Important points: for $\delta$ fixed, $h$ will be fixed, $\abs{\Sigma}$ depends only on $\delta$ so fixed also, and for regular instances, the problem of distinguishing between YES and NO instances is NP-hard. We will show that it will be NP-hard to approximate the generalized problem for some \textbf{bounded} concave nondecreasing $\varphi$ with $\varphi(0)=0,\varphi(1)=1$ and $\varphi(x) \leq \varphi_{\max}$ (on a uniform matroid constraint and with weights equal to one) within a better approximation factor as roughly $\alpha(x)=\frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(x)}$ for the \textbf{integer} $x$ that minimizes this ratio, which we call $x_{\varphi} : \forall x, \alpha(x_{\varphi}) \leq \alpha(x)$ (precisely, we will get the inapproximability of ratio $\frac{\alpha(x_{\varphi}) + \delta'}{1-\delta} \underset{\delta \rightarrow 0}{\rightarrow} \alpha(x_{\varphi})$ for some $\delta' = \Theta(\delta^{1/5})$ with the reduction).

\begin{rk}
  \begin{enumerate}
  \item The proof will work for any integer $x$, but is most interesting when $x$ gives the minimum ratio.
  \item It seems that the minimum is always taken on an integer and not on the linear extension. We should be able to prove this I guess.
  \item The bounded condition comes in two places: the Chernoff bound (where $\varphi(x) = o(x^{1/2})$ is enough and optimal), and in lemma \ref{lemBinPoi}). Maybe we can improve the latter, but the former cannot be improved with that kind of bound.
  \end{enumerate}
\end{rk}


\subsection{Partitionning Gadget}
For any set $[n]$, $\mathcal{Q} \subseteq 2^{[n]}$, we overload the definition $C^{\varphi}(\mathcal{Q}) := \sum_{e \in [n]} \varphi(\abs{\set{P \in \mathcal{Q} : e \in P}})$

We say that $\mathcal{Q}$ is an $x$-cover if every element of $[n]$ is covered $x$ times, so $C^{\varphi}(\mathcal{Q}) := n\varphi(x)$.

\begin{defi}
  Given $[n]$, an $([n],h,s,\varphi,\eta)$-partitioning system consists of $s$-distinct collections of subsets of $[n]$, $\mathcal{P}_1,\ldots,\mathcal{P}_s \subseteq 2^{[n]}$, that satisfy:
  \begin{enumerate}
  \item For every $i \in [s], \mathcal{P}_i$ is a collection of $h$-subsets $P_{i,1}, \ldots, P_{i,h} \subseteq [n]$ each of size $x_{\varphi}n/h$ which is an $x_{\varphi}$-cover.
  \item For any $T \subseteq [s]$ and $\mathcal{Q} = \set{P_i : i \in T}$ such that $P_i \in \mathcal{P}_i$, we have $\abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} \leq \eta n$ where:
    \[ \psi^{\varphi}_{\abs{T},h} = \mathbb{E}\Big[\varphi\Big(\Bin\Big(\abs{T},\frac{x_{\varphi}}{h}\Big)\Big)\Big]\]

  \item Furthermore, $\forall \epsilon > 0$, if $h \geq  \frac{x_{\varphi} \varphi_{\max}}{\epsilon}, \abs{T} = h(1+\mu)$ and $0 \leq \mu \leq \frac{\epsilon}{2x_{\varphi}}$, then:
    \[\abs{\psi^{\varphi}_{\abs{T},h} -\alpha(x_{\varphi})\varphi(x_{\varphi})} \leq \epsilon\]
  \end{enumerate}
\end{defi}

\begin{rk}
  In particular, for any $\mathcal{Q} = \set{P_{i,j}}$ of size $h$, we have that $C^{\varphi}(\mathcal{Q}) \leq n\varphi(x_{\varphi})$. Indeed $C^{\varphi}(\mathcal{Q}) = \sum_{i \in [n]} \varphi(a_i)$ with $\sum_{i \in [n]} a_i = x_{\varphi}n$. By concavity of $\varphi$ and Jensen inequality, this function is maximized when all $a_i$ are equals where we get $n\varphi(x_{\varphi})$.
\end{rk}
  \begin{theo}
    For every choice of $n,h,s \in \mathbb{N}$ and $s \geq h$ and $n \geq \eta^{-2}s\varphi_{\max}^2\log(20(h+1))$, there exists an $([n],h,s,\varphi,\eta)$-partitioning system, which can be found in time exp($sn$log$n).$poly$(h)$ (this will be constant time in the reduction).
  \end{theo}

  \begin{proof}
    The existential proof is based on the probabilistic method. We take $\mathcal{P}_i$ an $h$-equi-sized uniform random $x_{\varphi}$-cover of $[n]$. Hence in the collection $\mathcal{P}_i=(P_{i,1},\ldots,P_{i,h})$, each of the $h$ subsets is of cardinality $x_{\varphi}n/h$. Write $\mathcal{P} = (\mathcal{P}_1,\ldots,\mathcal{P}_s)$.

    We have that for any $e \in [n], \mathbb{P}(e \in P_{i,j}) = x_{\varphi}/h$. We note that although for $i$ fixed those events are not independent, but for different $i$s, these events are independent.

    By construction, the first condition is fulfilled. In order to prove the second condition, we will prove some bound on a fixed $T$ and $\mathcal{Q}$ and with union bound we will go over all of them.

    Let fix $T \subseteq [s]$ and $\mathcal{Q} := \set{P_i : i \in T}$ with $P_i :=P_{i, j(i)}$ for some function $j$.

    We have that for some $e \in [n]$:
    \[ \mathbb{E}[C_e^{\varphi}(\mathcal{Q})] =  \mathbb{E}[\varphi(\abs{\set{P \in \mathcal{Q} : e \in P}})] = \mathbb{E}[\varphi(\abs{\set{i \in T: e \in P_i}})]\]

    But the random variables $\set{X_i := \mathbbm{1}_{e \in P_i}}_{i \in T}$ are independent and follow the same Bernouilli law of parameter $x_{\varphi}/h$, so the variable $X =\abs{\set{i \in T: e \in P_i}} = \sum_{i \in T} X_i \sim \Bin(\abs{T},x_{\varphi}/h)$, and thus we have that
    \[\mathbb{E}[C_e^{\varphi}(\mathcal{Q})] = \mathbb{E}[\varphi(\Bin(\abs{T},x_{\varphi}/h))] =\psi^{\varphi}_{\abs{T},h}\]

    Now since we have that $0 \leq C_e^{\varphi}(\mathcal{Q}) \leq \varphi_{\max}$, thanks to Azuma-Hoeffding we get that:

    \[ \mathbb{P}\Big( \abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} > \eta n\Big) \leq 2 \text{exp}\Big(-\Big(\frac{\eta}{\varphi_{\max}}\Big)^2n\Big)\]

    Now, since there are at most $(h+1)^s$ choices of $T$ and $\mathcal{Q}$, we apply uniond bound to this:
    \[ \mathbb{P}\Big(\exists C,\mathcal{Q} : \abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} > \eta n\Big) \leq 2(h+1)^s \text{exp}\Big(-\Big(\frac{\eta}{\varphi_{\max}}\Big)^2n\Big)\]

    Thus w.p. at least $9/10$, we have that $\abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} \leq \eta n$, since we have taken $n \geq \eta^{-2}s\varphi_{\max}^2\log(20(h+1))$. So there must exists some choice of $\mathcal{P}$ that satisfies the first and second constraints of partitioning systems.

    Now let us prove the third and last part of this theorem. $\psi^{\varphi}_{h(1+\mu),h} =  \mathbb{E}[\varphi(\Bin(h(1+\mu),\frac{x_{\varphi}}{h}))]$ so thanks to lemma \ref{lemBinPoi}:

    \[\abs{\psi^{\varphi}_{h(1+\mu),h} - \mathbb{E}[\varphi(\Poi(x_{\varphi}(1+\mu))] } \leq \frac{x_{\varphi} \varphi_{\max}}{2h} \leq \frac{\epsilon}{2}\]

    since he have supposed that $h \geq \frac{x_{\varphi} \varphi_{\max}}{\epsilon}$ and $\mu \geq 0$.
    
    Furthermore, with $g(x) := \mathbb{E}[\varphi(\Poi(x))]$ which is 1-Lipschitz continuous thanks to lemma \ref{lemPoiCon}:
    \[\abs{g(x_{\varphi}(1+\mu)) - g(x_{\varphi})} \leq x_{\varphi}\mu \]

    So we get that for $0 \leq \mu \leq \frac{\epsilon}{2x_{\varphi}}$:
    \[\abs{\mathbb{E}[\varphi(\Poi(x_{\varphi}(1+\mu))] - \mathbb{E}[\varphi(\Poi(x_{\varphi}))]} \leq \frac{\epsilon}{2}\]

    But $\mathbb{E}[\varphi(\Poi(x_{\varphi}))] = \alpha(x_{\varphi})\varphi(x_{\varphi})$, so combining the two previous inequalities we get:

    \[\abs{\psi^{\varphi}_{h(1+\mu),h} -\alpha(x_{\varphi})\varphi(x_{\varphi})} \leq \epsilon\]

    Finally, since a random choice of $\mathcal{P} = (\mathcal{P}_1,\ldots,\mathcal{P}_s)$ satisfies the desired properties, we can enumerate over all choices of $\mathcal{P}$ in time exp($sn$log$n).$poly$(h)$ to find such a partitioning system.
  \end{proof}


  
  
\subsection{The Reduction}
Let us fix $\delta > 0$, and $\delta' = \Theta(\delta^{1/5})$ yet to be defined. We take $h = \frac{4 x_{\varphi}\varphi_{\max}}{\delta'\varphi(x_{\varphi})}$, and we will do the reduction for those $h$-\textsc{AryUGC}. We set first the parameters $k = \abs{V}, s = \abs{\Sigma}$ as in the previous paper.
We set the constants: $\eta = \epsilon = \frac{\delta'\varphi(x_{\varphi})}{8},  f(\delta') := \frac{1}{8h} = \frac{\delta'\varphi(x_{\varphi})}{32 x_{\varphi}\varphi_{\max}}$, where $f(\delta')$ will be the fraction of nice hyperedges in the Soundness contradiction proof.

Then the reduction is the same as in \cite{BFGG20} (with $\Gamma = \bigcup_{e \in E} [n]_e$) but instead we take our $\varphi$-gadget. Let us prove completeness and soundness properties. Precisely, if we are in a (YES) instance, we have that there exists $\mathcal{T}$ such that $C^{\varphi}(\mathcal{T}) \geq (1-\delta)\varphi(x_{\varphi})\abs{\Gamma}$. If we are in a (NO) instance, then we will have that for all $\mathcal{T}$ of size $k = \abs{V}$, $C^{\varphi}(\mathcal{T}) \leq (\alpha(x_{\varphi}) + \delta')\varphi(x_{\varphi})\abs{\Gamma}$. 

Thus, if we could approximate within a stricly better ratio than $\frac{\alpha(x_{\varphi}) + \delta'}{1-\delta}$, we would be able to distinguish between those two instances, which is known to be NP-hard by the $h$-\textsc{AryUGC}. Indeed, if we had a (YES) instance, we would get that $C^{\varphi}(\mathcal{T}_{\text{approx}}) > \frac{\alpha(x_{\varphi}) + \delta'}{1-\delta}(1-\delta)\varphi(x_{\varphi})\abs{\Gamma} = (\alpha(x_{\varphi}) + \delta')\varphi(x_{\varphi})\abs{\Gamma}$, so it would tell us in particular that we are not in a (NO) instance. Thus we would be in a  (YES) instance iff $C^{\varphi}(\mathcal{T}_{\text{approx}}) > (\alpha(x_{\varphi}) + \delta')\varphi(x_{\varphi})\abs{\Gamma}$. So finding such a $\mathcal{T}_{\text{approx}}$ is NP-hard. QED.

\subsubsection{Completeness}
Suppose the given $h$-\textsc{AryUniqueGames} instance $\mathcal{G}$ is a YES instance. Then, there exists a labeling $\sigma : V \mapsto \Sigma$ which strongly satisfies $1-\delta$ fraction of edges. Consider the collection of $\abs{V}$ subsets $\mathcal{T} := \set{\tilde{T}_{\sigma(v)}^v : v \in V}$. Let $e = (v_1,\ldots,v_h)$ be a hyperedge which is strongly satisfied by $\sigma$, which means that there exists $\alpha_r \in \Sigma$ such that $\pi_{e,v_i}(\sigma(v_i)) = \alpha_r$ for all $i \in [h]$. So $P_{r,i}^e = T_{\sigma(v_i)}^{e,v_i} \subseteq \tilde{T}_{\sigma(v_i)}^{v_i}$. Thus since by construction, $(P_{r,i}^e)_{i \in [h]}$, which comes from the partioning gadget, is an $x_{\varphi}$-cover of $[n]_e$, we have that $C^{\varphi}(\mathcal{T}) \geq (\text{\# strongly satisfied edges}) \times \varphi(x_{\varphi})n \geq (1-\delta)\abs{V}\varphi(x_{\varphi})n = (1-\delta)\varphi(x_{\varphi})\abs{\Gamma}$.

\subsubsection{Soundness}
Suppose the given $h$-\textsc{AryUniqueGames} instance $\mathcal{G}$ is a NO instance. Let us prove the contrapositive of the soundness: we suppose that there exists $\mathcal{T}$ of size $\abs{V}$ such that  $C^{\varphi}(\mathcal{T}) > (\alpha(x_{\varphi}) + \delta')\varphi(x_{\varphi})\abs{\Gamma}$. Let us find a contradiction.

For every vertex $v \in V$, we define $L(v) := \set{\beta \in \Sigma : \tilde{T}_{\beta}^v \in \mathcal{T}}$ to be the candidate set of labels that can be associated with the vertex $v$. We extend this definition to hyperedges $e = (v_1,\ldots,v_h)$ where we define $L(e) := \bigcup_{x \in [h]} L(v_x)$ to be the \emph{multiset} of all labels associated with the edge.

We say that $e = (v_1,\ldots,v_h) \in E$ is \emph{consistent} iff $\exists x \not= y \in [h], \pi_{e,v_x}(L(v_x)) \cap \pi_{e,v_y}(L(v_y)) \not= \emptyset$. We have the following lemma on inconsistent edges:

\begin{lem}
  $e = (v_1,\ldots,v_h) \in E$ be an inconsistent hyperedge with respect to $\mathcal{T}$. Then we have that $\abs{C_e^{\varphi}(\mathcal{T}) - \psi^{\varphi}_{\abs{L(e)},h}n } \leq \eta n$.
\end{lem}

\begin{proof}
  Since $e$ is inconsistent, $\forall x \not= y \in [h], \pi_{e,v_x}(L(v_x)) \cap \pi_{e,v_y}(L(v_y)) = \emptyset$. Therefore, for every $\alpha_i \in \Sigma$, there is at most one $v \in e$ such that $\alpha_i \in \pi_{e,v}(L(v))$: for every $i \in [s]$, $\mathcal{T}$ intersects with $\mathcal{P}_i^e$ in at most one subset. This gives us a subset $T \subseteq [s]$ of size $\abs{L(e)}$ such that the restriction of $\mathcal{T}$ to $[n]_e$ is $\mathcal{Q} = \set{P^e_{i,j} : j \in T}$ (up to empty sets). So by the second condition of the partitioning gadget, we get the expected result.
\end{proof}

Since the overall value of $\mathcal{T}$ is large and inconsistent edges admit small value, we can show that there exists a large fraction of consistent edges. First we claim that for a significant fraction of the edges $e$, the associated label sets $L(e)$ cannot be too large. Specifically, we note that:

\[ \mathbb{E}_{e \sim E}[\abs{L(e)}] \leq \frac{h}{\abs{V}} \sum_{v \in V} \abs{L(v)} = h - (why?)\]

since the hypergraph is regular, hence picking a hyperedge uniformly at random corresponds to selecting vertices with probability $\frac{h}{\abs{V}}$ each.

Therefore, via Markov’s inequality, the size of the label set $\abs{L(e)}$ is greater than $\frac{h}{f(\delta')}$ for at most $f(\delta')$ fraction of the hyperedges:

\[\mathbb{P}\Big(\abs{L(e)} \geq \frac{h}{f(\delta')}\Big) \leq f(\delta')\]

We say that a hyperedge $e \in E$ is \emph{nice} if $e$ is consistent and the associated label set $L(e)$ is of cardinality at most $\frac{h}{f(\delta')}$. Since inconsistent edges with small label sets must result in small value thanks to the previous lemma, it must be that a significant fraction of edges must be nice. This observation is formalized in the following lemma:

\begin{lem}
  At least $f(\delta')$ fraction of the hyperedges must be nice.
  \label{lemNice}
\end{lem}

\begin{proof}
  We prove this by contradiction. We suppose that $\mathbb{P}(e \text{ is nice}) < f(\delta')$. Then with union bound we get:

  \[ \mathbb{P}(e \text{ is consistent}) \leq \mathbb{P}(e \text{ is nice}) + \mathbb{P}\Big(\abs{L(e)} \geq \frac{h}{f(\delta')}\Big) \leq 2f(\delta')\]

  Therefore, the contribution of consistent edges to the value is at most $2 f(\delta')\abs{E} \times \varphi_{\max}n = 2 \varphi_{\max} f(\delta') \abs{\Gamma}$ (can we do better? Maybe $\varphi(sx_{\varphi})$ we can take roughly $s=2h$ all the time, so constant when $\delta$ is fixed, and it will work!).

  Thus since we have supposed $C^{\varphi}(\mathcal{T}) > (\alpha(x_{\varphi}) + \delta')\varphi(x_{\varphi})\abs{\Gamma}$, the value that comes from inconsistent edges is at least $(\alpha(x_{\varphi}) + \delta' - 2 \frac{\varphi_{\max}}{\varphi(x_{\varphi})} f(\delta'))\varphi(x_{\varphi})\abs{\Gamma} \geq (\alpha(x_{\varphi}) + \delta' - \frac{\delta'}{16x_{\varphi}})\varphi(x_{\varphi})\abs{\Gamma} \geq (\alpha(x_{\varphi}) + \frac{3\delta'}{4})\varphi(x_{\varphi})\abs{\Gamma}$ ($x_{\varphi} \geq 1$ since nonzero natural integer)

  To obtain a contradiction, we will now prove that the value that comes from inconsistent hyperedges cannot be this large. Write $E_{\text{inc}} \subseteq E$ to denote the set of inconsistent hyperedges. Since $\abs{E_{\text{inc}}} \geq (1-2f(\delta'))\abs{E}$, by averaging, it follows that:

  \[ \mathbb{E}_{e \sim E_{\text{inc}}}[\abs{L(e)}] \leq \frac{h}{1 - 2f(\delta')} \leq (1 + 4f(\delta'))h\]

  (we suppose $f(\delta')$ to be small, so in particular $2f(\delta') \leq 0.5$ which is the condition to have this inequality).

  Applying lemmma \ref{lemNice} on inconsistent edges with $\mu = 4 f(\delta')$ we get:

  \begin{equation}
    \begin{aligned}
      \mathbb{E}_{e \sim E_{\text{inc}}}[C_e^{\varphi}(\mathcal{T})] &\leq&& \mathbb{E}_{e \sim E_{\text{inc}}}[\psi^{\varphi}_{\abs{L(e)},h} +\eta] \leq (\psi^{\varphi}_{\mathbb{E}_{e \sim E_{\text{inc}}}[\abs{L(e)}],h} +\eta)n\\
      &\leq&& (\psi^{\varphi}_{h(1+\mu),h} +\eta)n \leq (\alpha(x_{\varphi})\varphi(x_{\varphi}) + 2\epsilon)n       \end{aligned} 
  \end{equation}


where we apply succesively lemma \ref{lemNice}, Jensen inequality thanks to concavity and then the nondecreasing property of $x \mapsto \psi^{\varphi}_{x,h} = g_{x_{\varphi}/h}^{\varphi}$ thanks to lemma \ref{lemBinCon}, and finally the third property of the partitioning gadget. We can check indeed that $\eta = \epsilon$, $\mu = 4f(\delta') = \frac{\delta'\varphi(x_{\varphi})}{8x_{\varphi}\varphi_{\max}} \leq \frac{\epsilon}{2x_{\varphi}}$ ($\varphi_{\max} \geq \varphi(1) = 1$) et $h =  \frac{4 x_{\varphi}\varphi_{\max}}{\delta'\varphi(x_{\varphi})} \geq  \frac{x_{\varphi}\varphi_{\max}}{\epsilon}$ since $\epsilon = \frac{\delta'\varphi(x_{\varphi})}{4}$.

This implies that the total contribution of inconsistent edges $E_{\text{inc}}$ is at most $(\alpha(x_{\varphi})\varphi(x_{\varphi}) + 2\epsilon)\abs{\Gamma} = (\alpha(x_{\varphi})+ \frac{\delta'}{2})\varphi(x_{\varphi})\abs{\Gamma}$, but we have also seen that the contribution is at least $(\alpha(x_{\varphi}) + \frac{3\delta'}{4})\varphi(x_{\varphi})\abs{\Gamma}$, thus we have a contradiction and the lemma is proved.
\end{proof}

Finally, we construct a randomized labeling $ \sigma : V \mapsto \Sigma$ as follows: for $v \in V$, if $L(v) \not= \emptyset$, set $\sigma(v)$ uniformly form $L(v)$, otherwise set it arbitrarily. We claim that in expectation, this labeling must weakly satisfy $\delta = \Theta(\delta'^5)$ fraction of the hyperedges.

To see this, fix any nice hyperedge $e = (v_1,\ldots,v_h)$. Thus $\exists x \not= y \in [h], \pi_{e,v_x}(L(v_x)) \cap \pi_{e,v_y}(L(v_y)) \not= \emptyset$. Furthermore the niceness implies that $\abs{L(v_x)},\abs{L(v_y)} \leq \frac{h}{f(\delta')}$. Thus, we have that $\pi_{e,v_x}(L(v_x)) = \pi_{e,v_y}(L(v_y))$ with probability at least $\frac{1}{\abs{L(v_x)}\abs{L(v_y)}} \geq \Big(\frac{f(\delta')}{h}\Big)^2$.

Therefore:

\begin{equation}
  \begin{aligned}
    \mathbb{E}_{\sigma}\mathbb{E}_{e \sim E}[\sigma \text{ weakly satisfies } e] &\geq&& f(\delta') \mathbb{E}_{\sigma}\mathbb{E}_{e \sim E}[\sigma \text{ weakly satisfies } e | e \in E_{\text{nice}}]\\
    &\geq&& \frac{f(\delta')^3}{h^2} = \frac{\Big(\frac{\delta'\varphi(x_{\varphi})}{32 x_{\varphi}\varphi_{\max}}\Big)^3}{\Big(\frac{4 x_{\varphi}\varphi_{\max}}{\delta'\varphi(x_{\varphi})}\Big)^2} > K_{\varphi} \delta'^5 = \delta
  \end{aligned}
\end{equation}

with $K_{\varphi} = 10^{-6} \Big(\frac{\varphi(x_{\varphi})}{x_{\varphi}\varphi_{\max}}\Big)^5$ and setting $\delta' := (\frac{\delta}{K_{\varphi}})^{1/5}$. Thus in particular there exists some labeling $\sigma$ such that $\mathbb{E}_{e \sim E}[\sigma \text{ weakly satisfies } e] > \delta$ which is in contradiction with the fact that we had a NO instance, and thus the soundness is also proved.

\subsection{Useful Lemmas}
  \begin{lem}
    For any \textbf{bounded} function $0 \leq f \leq f_{\max}$, we have that:
    \[ \abs{\mathbb{E}[f(\Bin(n,x/n))] - \mathbb{E}[f(\Poi(x))]} \leq \frac{x f_{\max}}{2n}\]

    In particular if $\mu \geq 0$:

    \[ \abs{\mathbb{E}[f(\Bin(h(1+\mu),x/h))] - \mathbb{E}[f(\Poi(x(1+\mu)))]} \leq \frac{x f_{\max}}{2h}\]
    \label{lemBinPoi}
  \end{lem}

  \begin{proof}
    Thanks to \cite{TF19,Barbour84}, we have that the total variation distance between $\Bin(n,x/n)$ and $\Poi(x)$ is bounded in the following way:
    \[ \Delta(\Bin(n,x/n),\Poi(x)) \leq \frac{1 - e^{-x}}{2x} n* \Big(\frac{x}{n}\Big)^2 \leq \frac{x}{2n}\]
    Thus with $B \sim \Bin(n,x/n)$ and $P \sim \Poi(x)$:
    \begin{equation}
      \begin{aligned}
        \abs{\mathbb{E}[f(B)] - \mathbb{E}[f(P)]} &=&&  \abs{\sum_{k=0}^{+\infty}f(k)\mathbb{P}(B=k) - \sum_{k=0}^{+\infty}f(k)\mathbb{P}(P=k)}\\
        &=&&  \abs{\sum_{k=0}^{+\infty}f(k)(\mathbb{P}(B=k) - \mathbb{P}(P=k))}\\
        &\leq&& \sum_{k=0}^{+\infty}f(k)\abs{\mathbb{P}(B=k) - \mathbb{P}(P=k)}\\
        &\leq&& f_{\max}\sum_{k=0}^{+\infty}\abs{\mathbb{P}(B=k) - \mathbb{P}(P=k)}\\
        &=&& f_{\max}\Delta(\Bin(n,x/n),\Poi(x)) \leq \frac{x f_{\max}}{2n}
      \end{aligned}
    \end{equation}

    Finally, taking $x(1+\mu)$ instead of $x$ and $h(1+\mu)$ instead of $n$, one obtains the second inequality (for $\mu \not= -1$).
  \end{proof}
  
  \begin{lem}
    For $\varphi$ nondecreasing concave with $\varphi(0)=0,\varphi(1)=1$, we have that the function $g : x \mapsto \mathbb{E}[\varphi(\Poi(x))]$ on $\mathbb{R}^+$ is $\mathcal{C}^{\infty}$ nondecreasing concave with $g'(0) = \varphi(1)-\varphi(0) = 1$ so in particular $1$-Lipschitz continuous.
    \label{lemPoiCon}
  \end{lem}

  \begin{proof}
    Since we have that $\varphi$ is sublinear, in particular $g(x) = e^{-x}\sum_{k=0}^{+\infty} \varphi(k)\frac{x^k}{k!}$ is $\mathcal{C}^{\infty}$. It is thus enough to compute its first and second derivatives:

    \begin{equation}
      \begin{aligned}
        g'(x) &=&& -e^{-x}\sum_{k=0}^{+\infty}\varphi(k)\frac{x^k}{k!}+ e^{-x}\sum_{k=1}^{+\infty}\varphi(k)e^{-x}k\frac{x^{k-1}}{k!}\\
        &=&& -e^{-x}\sum_{k=0}^{+\infty}\varphi(k)\frac{x^k}{k!}+ e^{-x}\sum_{k=0}^{+\infty}\varphi(k+1)e^{-x}\frac{x^{k}}{k!}\\
        &=&& e^{-x}\sum_{k=0}^{+\infty}(\varphi(k+1) -\varphi(k))\frac{x^k}{k!}
      \end{aligned}
    \end{equation}

    But $\varphi(k+1) -\varphi(k) \geq 0$ since $\varphi$ nondecreasing, so $g'(x) \geq 0$ and $g$ is nondecreasing.

    The calculus of $g''$ is the same where we replace $\varphi$ by $\psi(k) := \varphi(k+1) -\varphi(k)$ which is a nonincreasing function by concavity of $\varphi$. Thus:

    \[g''(x) = e^{-x}\sum_{k=0}^{+\infty}(\psi(k+1) -\psi(k))\frac{x^k}{k!} \leq 0\]

    since $\psi(k+1) -\psi(k) \leq 0$, and so $g$ is concave.

    Furhtermore $g'$ is decreasing and $g'(0) = e^{-0}\sum_{k=0}^{+\infty}(\varphi(k+1) -\varphi(k))\frac{0^k}{k!} = 1 \times (\varphi(0+1) -\varphi(0))\frac{1}{0!} =  \varphi(1)-\varphi(0) = 1$. Thus $0 \leq g'(x) \leq 1$, and thus $g$ is 1-Lipschitz continuous.
  \end{proof}
  
\begin{lem}
  For $\varphi$ nondecreasing concave with $\varphi(0)=0,\varphi(1)=1$, we have that the function $g_q : n \mapsto \mathbb{E}[\varphi(\Bin(n,q))]$ on $\mathbb{N}$ nondecreasing concave. As a consequence, one can uses Jensen's inequality on the piecewise linear extension of $g_q$ which will also be continuous.
  \label{lemBinCon}
\end{lem}

\begin{proof}
  $\Bin(n,q) \leq_{\text{st}} \Bin(n+1,q)$ and we have that $\varphi$ is nondecreasing, so $\mathbb{E}[\varphi(\Bin(n,q))] \leq \mathbb{E}[\varphi(\Bin(n+1,q))]$, ie $g_q(n+1) - g_q(n) \geq 0$: $g_q$ is nondecreasing.

  We show then the concavity, ie. $g_q(n+2) - g_q(n+1) \leq g_q(n+1) - g_q(n)$. Call $\psi(x) = \varphi(x+1)-\varphi(x)$ which is nonincreasing since $\varphi$ concave. Let us take $X_{k,q} \sim \Bin(k,q)$. Then:

  \begin{equation}
    \begin{aligned}
      g_q(n+1) &=&& \mathbb{E}[\varphi(X_{n+1,q})] = \sum_{i=0}^n \mathbb{E}[\varphi(X_{n,q}+X_{1,q})|X_{n,q}=i]\mathbb{P}(X_{n,q}=i)\\
      &=&& \sum_{i=0}^n \mathbb{E}[\varphi(i+X_{1,q}) - \varphi(i)]\mathbb{P}(X_{n,q}=i) + \sum_{i=0}^n \varphi(i)\mathbb{P}(X_{n,q}=i)\\
      &=&& \sum_{i=0}^n \mathbb{E}[\varphi(i+X_{1,q}) - \varphi(i)]\mathbb{P}(X_{n,q}=i) + g_q(n)
    \end{aligned}
  \end{equation}

  Thus:

  \begin{equation}
    \begin{aligned}
      g_q(n+1) -g_q(n) &=&& \sum_{i=0}^n \mathbb{E}[\varphi(i+X_{1,q}) - \varphi(i)]\mathbb{P}(X_{n,q}=i)\\
      &=&& \sum_{i=0}^n q(\varphi(i+1) - \varphi(i))\mathbb{P}(X_{n,q}=i) = q \mathbb{E}[\psi(\Bin(n,q))]
    \end{aligned}
  \end{equation}

  Then thanks to the fact that  $\Bin(n,q) \leq_{\text{st}} \Bin(n+1,q)$ and $\psi$ is nonincreasing, we have that $\mathbb{E}[\psi(\Bin(n,q))] \geq \mathbb{E}[\psi(\Bin(n+1,q))]$, ie. $g_q(n+2) - g_q(n+1) \leq g_q(n+1) - g_q(n)$.
\end{proof}

\subsection{Extension to unbounded $\varphi(n) = o(\sqrt{n})$}
First let us remark that $\varphi(n) = o(n)$, since the problem studied is in P when $\varphi$ is linear. The fact that we need $\varphi$  in the proof to be bounded arises at 3 places:
\begin{enumerate}
\item When we use the lemma which relates Poisson and Binomial. We can product a better bound (see below), which will work as soon $\varphi(n) = o(n)$, and this will work by taking (constant) larger $h$.
\item When we bound the contribution of consistent edges in the proof. This can be easily improved. Indeed, $\abs{V} \leq sh$ is bounded (we can ask that $s \simeq 2h$ to make things work). The best way to dispatch the $n$ elements into those $sh$ sets of size $\frac{x_{\varphi}n}{h}$ will be to have the same number of those in each set as remarked when the partitioning gadget was introduced. Thus, the larger value that can occur is $C_e^{\varphi}(\mathcal{T}) \leq n\varphi(\frac{sh}{n}\frac{x_{\varphi}n}{h}) = n \varphi(sx_{\varphi})$ which is a constant. We will have to change the definition of $f(\delta')$ accordingly to that new value.
\item When we use the Hoeffding concentration bound. In order to makes this work by taking (constant) larger $n$, we still need some asymptotic concentration, which occurs iff $\varphi(n) = o(\sqrt{n})$, which is the only constraint which asks that condition on $\varphi$.
\end{enumerate}


\begin{lem}[Improved Poisson vs Binomial bound]
  For any nonnegative nondecreasong function $f$ with $f(k) \leq k$, we have that:
  \[ \abs{\mathbb{E}[f(\Bin(n,x/n))] - \mathbb{E}[f(\Poi(x))]} \leq \frac{x f(n)}{2n} + \frac{x^{n+1}}{n!}\]

  In particular if $\mu \in [0,1]$:

  \[ \abs{\mathbb{E}[f(\Bin(h(1+\mu),x/h))] - \mathbb{E}[f(\Poi(x(1+\mu)))]} \leq \frac{x f(2h)}{2h} + \frac{(2x)^{2h+1}}{(2h)!}\]
\end{lem}

\begin{proof}
  Thanks to \cite{TF19,Barbour84}, we have that the total variation distance between $\Bin(n,x/n)$ and $\Poi(x)$ is bounded in the following way:
  \[ \Delta(\Bin(n,x/n),\Poi(x)) \leq \frac{1 - e^{-x}}{2x} n* \Big(\frac{x}{n}\Big)^2 \leq \frac{x}{2n}\]
  Thus with $B \sim \Bin(n,x/n)$ and $P \sim \Poi(x)$:
  \begin{equation}
    \begin{aligned}
      \abs{\mathbb{E}[f(B)] - \mathbb{E}[f(P)]} &=&&  \abs{\sum_{k=0}^{+\infty}f(k)\mathbb{P}(B=k) - \sum_{k=0}^{+\infty}f(k)\mathbb{P}(P=k)}\\
      &=&&  \abs{\sum_{k=0}^{+\infty}f(k)(\mathbb{P}(B=k) - \mathbb{P}(P=k))}\\
      &\leq&& \sum_{k=0}^{+\infty}f(k)\abs{\mathbb{P}(B=k) - \mathbb{P}(P=k)}\\
      &\leq&& f(n)\Delta(\Bin(n,x/n),\Poi(x)) + \sum_{k=n+1}^{+\infty}f(k)\mathbb{P}(P=k)\\
      &\leq&&\frac{x f(n)}{2n}  + e^{-x}\sum_{k=n+1}^{+\infty}k\frac{x^k}{k!} = \frac{x f(n)}{2n}  + xe^{-x}\sum_{k=n}^{+\infty}\frac{x^k}{k!}\\
      &\leq&& \frac{x f(n)}{2n}  + \frac{x^{n+1}}{n!} \underset{n \rightarrow \infty}{\rightarrow} 0
    \end{aligned}
  \end{equation}

  by a standard upper bound on the remainder of the exponential series.
  Finally, taking $x(1+\mu) \leq 2x$ instead of $x$ and $h(1+\mu) \leq 2h$ instead of $n$, one obtains the second inequality.

  However, one also needs to quantify in terms of $\epsilon$ the speed of convergence to $0$ of this bound. Let us assume that $\varphi(n) = o(\sqrt{n})$, and more precisely $\varphi(n) \leq K_{\varphi}\sqrt{n}$ and focus on $x_{\varphi}$. Then $\frac{x_{\varphi}K_f}{\sqrt{2h}} + \frac{(2x_{\varphi})^{2h+1}}{(2h)!} \leq K'_{\varphi}\frac{x_{\varphi}}{\sqrt{h}}$ for some constant $K'_{\varphi}$ which depends only on $\varphi$ (and $x_{\varphi}$).

  So it will be enough to take $h = (K'_{\varphi}x_{\varphi})^2\frac{1}{\epsilon^2}$, whereas now it is linear in $\frac{1}{\epsilon}$. But for our concerns, this is enough to fulfill the proof (we will get some $\delta'=\Theta(\delta^{1/7})$). If we had taken $\varphi(n) = \Theta(n^{1-p})$ we would get $h = \Theta(\frac{1}{\epsilon^{1/p}})$.

  
\end{proof}
\newpage

\bibliographystyle{plain}
\bibliography{these.bib}

\end{document}
