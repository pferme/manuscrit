%\documentclass[6pt]{article}
\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{multicol}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{xparse}
\usepackage{physics}
\usepackage{empheq}
\usepackage{url}
\usepackage{hyperref}
\usepackage[affil-it]{authblk}
\usepackage{enumitem}
\usepackage{tikz}
\usetikzlibrary{quotes,angles,calc}
\usepackage{rotating}
\usepackage{graphicx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\SetKwInput{KwInput}{Input}                % Set the Input
\SetKwInput{KwOutput}{Output}              % set the Output

\usepackage{pgfplots}
\pgfplotsset{compat = newest}

%\usepackage{wasysym}
%\usepackage{listings}
%\usepackage{moreverb}

%\usepackage[top=3cm,bottom=2cm,right=1cm,left=1cm]{geometry}
\usepackage[top=3cm,bottom=2cm,right=2cm,left=2cm]{geometry}

\theoremstyle{definition}
\newtheorem{theo}{Theorem}[section]
\newtheorem{lem}[theo]{Lemma}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{prop}[theo]{Property}
\newtheorem{defi}[theo]{Definition}
\newtheorem{conj}[theo]{Conjecture}

\newtheorem{theo*}{Theorem}

\theoremstyle{remark}
\newtheorem*{rk}{Remark}

\DeclareMathOperator{\Poi}{\text{Poi}}
\DeclareMathOperator{\Ber}{\text{Ber}}
\DeclareMathOperator{\Bin}{\text{Bin}}
\DeclareMathOperator{\maxi}{\text{maximize}}
\DeclareMathOperator{\mini}{\text{minimize}}
\DeclareMathOperator{\st}{\text{subject to}}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiterX\set[1]\lbrace\rbrace{\def\given{\;\delimsize\vert\;}#1}

\newcommand{\OF}[1]{\textcolor{blue}{OF: #1}}

\colorlet{darkgreen}{green!40!black}

\title{\huge{Tight Approximation Guarantees for \\ Concave Coverage Problems}}
%\title{\huge{Tight Approximation Bounds for nondecreasing concave $\varphi$-\textsc{MaxCoverage}}}
\author{Siddharth Barman\footnote{Indian Institute of Science, Bangalore, India. \href{mailto:barman@iisc.ac.in}{\texttt{barman@iisc.ac.in}}} \space\space\space\space\space Omar Fawzi\footnote{Univ Lyon, ENS Lyon, UCBL, CNRS, LIP, F-69342, Lyon Cedex 07, France. \href{mailto:omar.fawzi@ens-lyon.fr}{\texttt{omar.fawzi@ens-lyon.fr}}}\space\space\space\space\space Paul FermÃ©\footnote{ENS Lyon, LIP, France. \href{mailto:paul.ferme@ens-lyon.fr}{\texttt{paul.ferme@ens-lyon.fr}}}
}
\date{}



\begin{document}

\maketitle

\begin{abstract}
In the maximum coverage problem, we are given subsets $T_1, \ldots, T_m$ of a universe $[n]$ along with an integer $k$ and the objective is to find a subset $S \subseteq [m]$ of size $k$ that maximizes $C(S) := \abs{\bigcup_{i \in S} T_i}$. It is a classic result that the greedy algorithm for this problem achieves an approximation ratio of $(1-e^{-1})$ and there is a matching inapproximability result. We note that in the maximum coverage problem even if an element $a \in [n]$ is covered by multiple sets, it still contributes only one to the maximization objective. 

In this work we consider a generalization of this problem wherein an element $a$ can contribute by an amount that depends on the number of times it is covered. Given a concave and nondecreasing function $\varphi$, we define $C^{\varphi}(S) :=\sum_{a \in [n]}w_a\varphi(\abs{S}_a)$, where $\abs{S}_a = \abs{\set{i \in S : a \in T_i}}$. For any such $\varphi$, we provide an efficient algorithm that achieves an approximation ratio equal to the \emph{Poisson concavity ratio} (of $\varphi$) $\alpha_{\varphi} := \inf_{x \in \mathbb{N}} \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(\mathbb{E}[\Poi(x)])}$. Complementing this approximation guarantee, we establish a matching NP-hardness result for sublinear $\varphi$. We note that the current work recovers the result of \cite{BFGG20} as a special case by instantiating $\varphi(x) = \min \{ x,\ell \}$ (for which $\alpha_{\varphi} = 1-\frac{\ell^{\ell}e^{-\ell}}{\ell!}$) and achieves the NP-hardness result without relying on the Unique Games Conjecture.

In terms of applications, our algorithmic result addresses the problem of utility design for distributed resource allocation \cite{PM19}, and as a special case the vehicle-target assignment problem \cite{Murphey00}. Both of these problems are special cases of maximizing $C^{\varphi}$, under matroid constraints, and for the latter, our analytic expression of $\alpha_{\varphi}$ improves upon the previously-known numerical approximation ratios. 
\end{abstract}

%\begin{multicols}{2}

\section*{Introduction}
Coverage problems lie at the core of combinatorial optimization and have been extensively studied in computer science. A quintessential example of such problems is the maximum coverage problem wherein we are given subsets $T_1, \ldots, T_m$ of a universe $[n]$ along with a positive integer $k$, and the objective is to find a size-$k$ set $S \subseteq [m]$ that maximizes the covering function $C(S) := \abs{\bigcup_{i \in S} T_i}$.

It is well-known that a natural greedy algorithm achieves an approximation ratio of $1-e^{-1}$ for
this problem (see, e.g., \cite{Hochbaum96}). Furthermore, the work of Feige \cite{Feige98} shows that this approximation ratio is tight, under the assumption that P $\not=$ NP. Over the years, a large body of work has been directed towards extending these fundamental results and, more generally, coverage problem have been studied across multiple fields, such as operations research \cite{CFN77}, machine learning \cite{FK14}, and algorithmic game theory \cite{SV15}.

Here, we consider a generalization of this problem where an element $a$ can contribute by an amount that depends on the number of times it is covered. Given a (counting) function $\varphi$, an integer $k \in \mathbb{N}$, a universe of elements $[n]$, positive weights $w_a$ for each $ a \in [n]$, and  subsets $T_1,\ldots,T_m \subseteq [n]$, the $\varphi$-\textsc{MaxCoverage} problem entails maximizing $C^{\varphi}(S) := \sum_{a \in [n]}w_a\varphi(\abs{S}_a)$ over subsets $S \subseteq [m]$ of cardinality at most $k$; here $\abs{S}_a = \abs{\set{i \in S : a \in T_i}}$.

This work focuses on functions $\varphi$ that are nondecreasing and concave (i.e., $\varphi(i+2) - \varphi(i+1) \leq \varphi(i+1) - \varphi(i)$ for $i \in \mathbb{N}$).\footnote{We require $\varphi$ to be defined for nonnegative integers and will extend it over $\mathbb{R}_+$ by considering its piecewise linear extension.}  We will also assume that the function $\varphi$ is normalized in the sense that $\varphi(0) = 0$ and $\varphi(1) = 1$. 

Our approximation guarantees are in terms of the \emph{Poisson concavity ratio} of $\varphi$ which we define as follows 
\begin{align*}
\alpha_{\varphi} := \inf_{x \in \mathbb{N}} \ \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(\mathbb{E}[\Poi(x)])} =  \inf_{x \in \mathbb{N}} \ \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(x)}
\end{align*} 
We will write $\alpha_{\varphi}(x) := \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(x)}$ and, hence, $\alpha_{\varphi}  = \inf_{x \in \mathbb{N}} \alpha_{\varphi}(x) $. 

Our main result is that the $\varphi$-\textsc{MaxCoverage} problem admits an efficient $\alpha_\varphi$-approximation algorithm, when $\varphi$ is nondecreasing and concave, and this approximation guarantee is tight. Formally,  

\begin{theo*}
For any nondecreasing concave function $\varphi$, there exists a polynomial-time $\alpha_\varphi$-approximation algorithm for the $\varphi$-\textsc{MaxCoverage} problem. Furthermore, for $\varphi(n) = o(n)$, it is {\rm NP}-hard to approximate the $\varphi$-\textsc{MaxCoverage} problem within a factor better than $\alpha_\varphi + \varepsilon$, for any constant $\varepsilon >0$.
\end{theo*}

This theorem extends to the problem of maximizing $C^\varphi(\cdot)$ subject to matroid constraints.  

\begin{rk}
For an instance with $m$ sets, one can obtain an  approximation ratio equal to $\min_{x \in [m]} \alpha_{\varphi}(x)$, which can be much better in some cases.
\end{rk}

\paragraph{Special cases:}
Setting $\varphi(j) = \min\{ j,1\}$, one recovers the standard \textsc{MaxCoverage} problem, and in this case $\alpha_{\varphi} = 1 - e^{-1}$. If $\varphi(j) = \min\{ j,\ell\}$, for a given $\ell \in \mathbb{N}$, then we recover the  $\ell$-\textsc{MultiCoverage} problem studied in \cite{BFGG20} and obtain the tight approximation ratio $\alpha_{\varphi} = 1-\frac{\ell^{\ell}e^{-\ell}}{\ell!}$ (see Property \ref{prop:lCover}).

In addition, the problem of utility design for distributed resource allocation---introduced in \cite{PM19}---is a special case of maximizing $C^\varphi$ subject to a matroid constraint. In particular, the \textsc{Vehicle-Target Assignment} problem \cite{Murphey00} corresponds to such a maximization problem with $\varphi^p(j) = \frac{1-(1-p)^j}{p}$, with given parameter $p \in (0,1)$. Here, the Poisson concavity ratio and, hence, our approximation ratio $\alpha_{\varphi^p} = \frac{1 - e^{-p}}{p}$; see Property \ref{prop:VTA}). This bound is slightly better that the numerical approximation ratio obtained in \cite{PM19} for a particular set of instances. Another case studied in \cite{PM19} is $\varphi(j) = j^d$, for a parameter $d \in (0,1)$, which we refer to as the $d$-Power function. For such functions, we have $\alpha_{\varphi} = e^{-1}\sum_{k=1}^{+\infty}\frac{k^d}{k!}$ (Property \ref{prop:dPower}). This approximation ratio seems to match the empirical bounds shown in \cite{PM19} for certain problem instances. The above-mentioned results are summarized in Figure \ref{figComp}.
%numerical computation of approximation ratio found in
\begin{figure}[!h]
  \begin{center}
    \begin{tabular}{|l|l|l|l|l|}
      \hline
      $\varphi$-\textsc{MaxCoverage}  & $\varphi(j)$ & $\alpha_{\varphi}$ & Proof & Reference \\
      \hline
      \textsc{MaxCoverage} & $\min \{ j,1\}$ & $1 - e^{-1}$ & \ref{prop:lCover} & e.g. \cite{Hochbaum96} \\
      $\ell$-\textsc{MultiCoverage} & $\min\{ j,\ell\}$ & $1-\frac{\ell^{\ell}e^{-\ell}}{\ell!}$ & \ref{prop:lCover} & \cite{BFGG20}\\
      $p$-\textsc{Vehicle-Target Assignment} & $\frac{1-(1-p)^j}{p}$ & $\frac{1 - e^{-p}}{p}$ & \ref{prop:VTA} & \cite{Murphey00} \\
      $d$-Power & $j^d$ & $e^{-1}\sum_{k=1}^{+\infty}\frac{k^d}{k!}$ & \ref{prop:dPower} & \cite{PM19} \\
      \hline
    \end{tabular}
  \end{center}
  \caption{Our tight approximation ratios for particular choices of $\varphi$ in the $\varphi$-\textsc{MaxCoverage} problem.}
  \label{figComp}
\end{figure}

\paragraph{Comments on the {Poisson concavity ratio} $\alpha_{\varphi}$}
By Jensen's inequality along with the nonnegativity and concavity of $\varphi$, we have that $\alpha_{\varphi} \in [0,1]$. 

If $\varphi$ is nondecreasing and concave, then $C^{\varphi}$ is submodular. One can show, via a direct calculation, that for submodular $C^{\varphi}$ the curvature (as defined in \cite{CC84}) $c = 1 - (\varphi(m) - \varphi(m-1))$; see Property \ref{prop:SubCurv}. Therefore, the algorithm of Sviridenko et al.~\cite{SVW17} leads to an approximation ratio of $1 - c e^{-1}$ for the $\varphi$-\textsc{MaxCoverage} problem. We note that the {Poisson concavity ratio} $\alpha_{\varphi}$ is in fact always greater or equal to this curvature-dependent ratio (Property \ref{prop:BetterRatio}). Specifically, for $p$-\textsc{Vehicle-Target Assignment}, it is strictly better for all $p \not=0,1$ and for $\ell$-\textsc{MultiCoverage}, it is strictly better for all $\ell \geq 2$ as remarked in \cite{BFGG20}. Therefore, for the setting at hand, the current work improves the approximation guarantee obtained in \cite{SVW17}. 


\section{Approximation Algorithm for the $\varphi$-\textsc{MaxCoverage} problem}
We follow the same strategy as the one developed in \cite{BFGG20}. The algorithm we analyze is composed of two steps (relax and round): First, we solve the natural linear programming relaxation (see (\ref{defi:relaxedProg}) obtaining a fractional, optimal solution $x^* \in [0,1]^m$  which satisfies $\sum_{i=1}^m x^*_i = k$. The second step is to use pipage rounding to find an integral vector $x^{\text{int}} \in \set{0,1}^m$ with the property that $\sum_{i=1}^m x^{\text{int}}_i = k$. This defines naturally a size $k$ set $S := \set{i \in [m] : x^{\text{int}}_i = 1}$, which we identify with $x^{\text{int}}$, and will be the set output by the algorithm. These two steps are detailed below.

\paragraph{Step 1. Solve the LP relaxation:} Specifically, we consider the following linear programming relaxation of the $\varphi$-\textsc{MaxCoverage} problem under cardinality $k$ constraint.

\begin{defi}[Relaxed Program]
  \begin{equation}
    \begin{aligned}
      &\maxi&& \sum_{a \in [n]} w_ac_a \\
      &\st&& c_a \leq \varphi\Big(\sum_{i \in [m] : a \in T_i} x_i\Big), \forall a \in [n]\\
      &&& 0 \leq x_i \leq 1, \forall i \in [m]\\
      &&& \sum_{i=1}^m x_i = k
    \end{aligned}
  \end{equation}
  \label{defi:relaxedProg}
\end{defi}

One can see that for an optimal integral solution, we have that $c_a = \varphi\Big(\sum_{i \in [m] : a \in T_i} x_i\Big) = \varphi(\abs{S}_a)$ since $w_a > 0$, with $S$ defined from $x \in \set{0,1}^m$ as before. Thus the overall value we get is $\sum_{a \in [n]}w_a\varphi(\abs{S}_a) = C^{\varphi}(S)$, and we maximize here over all sets $S$ of size $k$. So it is indeed the same problem when $x$ is an integral solution.

This program is not yet a linear program as such since $\varphi$ is not a linear constraint. However, since $\varphi$ is piecewise linear on nonintegral values, we have that $\forall k \in [m], \exists p_k \text{ linear }, \forall x \in [k-1,k], \varphi(x) = p_k(x)$. Furthermore since $\varphi$ is concave, we have that $\forall k \in [m], \forall x \geq 0, \varphi(x) \leq p_k(x)$. Thus we can replace one constraint $c_a \leq \varphi(y)$ by the $m$ linear constraints $c_a \leq p_k(y), \forall k \in [m]$, so the program is indeed linear. Overall there are $n+m$ variables and $(n+1)m + 1$ linear constraints, and hence an optimal solution can be found in polynomial time.

\paragraph{Step 2. Round the fractional, optimal solution:} We round the computed fractional solution $x^*$ by considering the \emph{multilinear extension} of the objective, and applying pipage rounding \cite{AS04, Vondrak07, CCPV11} on it. Formally, given any function $f : \set{0,1}^m \rightarrow \mathbb{R}$, one can define the multilinear extension $F : [0, 1]^m \rightarrow \mathbb{R}$ by $F(x_1,\ldots,x_m): = \mathbb{E}[f(X_1,\ldots,X_m)]$ where $X_i$ are independent random variables with $X_i \sim \Ber(x_i)$, ie. $X_i \in \set{0,1}$ with $\mathbb{P}(X_i = 1) = x_i$.

For a submodular function $f$ , one can use pipage rounding to transform, in polynomial time, any fractional solution $x \in [0,1]^m$ satisfying $\sum_{i=1}^m x_i = k$ into an integral vector $x^{\text{int}} \in \set{0,1}^m$ such that $\sum_{i=1}^m x^{\text{int}}_i = k$ and $F(x^{\text{int}}) \geq F(x)$. We apply this strategy to $C^{\varphi}$ which was shown to be submodular in property \ref{prop:SubCurv}, and the solution $x^*$ of the LP relaxation \ref{defi:relaxedProg}. We thus get the following lower bound on the value returned by our algorithm, with the notation $X = (X_1,\ldots,X_m)$ and $\Ber(x) = (\Ber(x_1),\ldots,\Ber(x_m))$:

\[C^{\varphi}(x^{\text{int}}) = \mathbb{E}_{X \sim \Ber(x^{\text{int}})}[C^{\varphi}(X)] \geq \mathbb{E}_{X \sim \Ber(x^*)}[C^{\varphi}(X)]\]

Then it suffices to relate $\mathbb{E}_{X \sim \Ber(x^*)}[C^{\varphi}(X)]$ to the value taken by the optimal LP relaxation, which is known to be bigger than the integral LP, since we maximize over a larger space.

\begin{theo*}
  Let $x,c$ a feasible solution of the program \ref{defi:relaxedProg} and $X \sim \Ber(x)$. Let us call $\alpha_{\varphi} = \min_{x \in [m]} \alpha_{\varphi}(x)$. Then:
  \[\mathbb{E}_{X \sim \Ber(x)}[C^{\varphi}(X)] \geq \alpha_{\varphi} \sum_{a \in [n]} w_ac_a\]
  In particular, this implies that the described polynomial time algorithm has an approximation ratio of $\alpha_{\varphi}$:
  \[C^{\varphi}(x^{\text{int}}) \geq \alpha_{\varphi} \sum_{a \in [n]} w_ac^*_a \geq \alpha_{\varphi} \max_{S \subseteq [m] : \abs{S} = k} C^{\varphi}(S)\]
  \label{theo:AlgoCard}
\end{theo*}

In order to prove this theorem, we need the following lemma:

\begin{lem}
    For $\varphi$ concave, and $p \in [0,1]^m$, we have:
    \[\mathbb{E}\Big[\varphi\Big(\sum_{i=1}^m\Ber(p_i)\Big)\Big] \geq \mathbb{E}\Big[\varphi\Big(\Poi\Big(\sum_{i=1}^m p_i\Big)\Big)\Big]\]
  \label{lem:ConvexOrder}
\end{lem}

\begin{proof}
  The notion of concave order discussed in \cite{StochasticOrders} allows us to prove this result. We say that $X \leq_{\text{cv}} Y \iff \mathbb{E}[\varphi(X)] \leq \mathbb{E}[\varphi(Y)]$ for any concave $\varphi$. Thanks to Theorem 3.A.12 of \cite{StochasticOrders}, this order is preserved through convolution, so we only need to show that for $p \in [0,1]$ we have:
  \[\Ber(p) \geq_{\text{cv}} \Poi(p)\]
  since $\sum_{i=1}^m \Poi(p_i) \sim \Poi\Big(\sum_{i=1}^m p_i\Big)$. This was done in Lemma 2.3 of \cite{BFGG20}.
\end{proof}

\begin{proof}[Proof of theorem \ref{theo:AlgoCard}]
  By linearity of expectation and the fact that the weights $w_a$ are positive, it is sufficient to show that for all $a \in [n]$:
  \[ \mathbb{E}[C_a^{\varphi}(X)] \geq \alpha_{\varphi} c_a \]

  where $C_a^{\varphi}(S) := \varphi(\abs{S}_a)$ and $\abs{S}_a = \abs{\set{i \in S : a \in T_i}}$. In particular, we have that $\abs{X}_a = \sum_{i \in A} X_i$ with $A = \set{i \in [m] : a \in T_i}$. Call $x_A = \sum_{i \in A} x_i \in [0,m]$. We have:
  
  \begin{equation}
    \begin{aligned}
      \mathbb{E}[C_a^{\varphi}(X)] &=&& \mathbb{E}\Big[\varphi\Big(\sum_{i \in A} X_i\Big)\Big] = \mathbb{E}\Big[\varphi\Big(\sum_{i \in A}\Ber(x_i) \Big)\Big]\\
      &\geq&& \mathbb{E}\Big[\varphi\Big(\Poi\Big(\sum_{i \in A} x_i\Big)\Big)\Big] \text{ thanks to lemma \ref{lem:ConvexOrder}}\\
      &=&& \mathbb{E}[\varphi(\Poi(x_A))]
    \end{aligned}
  \end{equation}

  Furthermore, since $x \mapsto \mathbb{E}[\varphi(\Poi(x))]$ is concave thanks to lemma \ref{lem:PoiCon}, if we write $x_A = \lambda \lfloor x_A \rfloor + (1-\lambda) \lceil x_A \rceil$ for some $\lambda \in [0,1]$, we have that:
  
  \begin{equation}
    \begin{aligned}
      \mathbb{E}[\varphi(\Poi(x_A))] &\geq&& \lambda \mathbb{E}[\varphi(\Poi(\lfloor x_A \rfloor))] + (1-\lambda) \mathbb{E}[\varphi(\Poi(\lceil x_A \rceil))] \\
      &=&& \lambda \alpha_{\varphi}(\lfloor x_A \rfloor)\varphi(\lfloor x_A \rfloor) + (1-\lambda) \alpha_{\varphi}(\lceil x_A \rceil)\varphi(\lceil x_A \rceil)\text{ by definition of } \alpha_{\varphi}(x)\\
      &\geq&& \alpha_{\varphi} [\lambda \varphi(\lfloor x_A \rfloor) + (1-\lambda) \varphi(\lceil x_A \rceil)] \text{ since } \lfloor x_A \rfloor, \lceil x_A \rceil \in \mathbb{N} \text{ and } \lfloor x_A \rfloor \leq \lceil x_A \rceil \leq m\\
      &=&& \alpha_{\varphi}\varphi(x_A) \text{ since $\varphi$ linear between integer points}\\
      &\geq&& \alpha_{\varphi} c_a \text{  since $c_a \leq \varphi\Big(\sum_{i \in [m] : a \in T_i} x_i\Big) = \varphi(x_A)$}
    \end{aligned}
  \end{equation}
\end{proof}

\section{Generalization to matroid constraints}
\subsection{Generalized result}
We first note that in the proof of theorem \ref{theo:AlgoCard}, we did not use the cardinality constraint $\sum_{i=1}^m x_i = k$. Thus, since the pipage rounding strategy generalizes to matroid constraints $\mathcal{M}$ (see for instance \cite{Vondrak07} (Lemma 3.4)), the strategy and the analysis of its efficiency generalize immediately when applied to the following linear program:

\begin{defi}[Relaxed Matroid Program]
  \begin{equation}
    \begin{aligned}
      &\maxi&& \sum_{a \in [n]} w_ac_a \\
      &\st&& c_a \leq \varphi\Big(\sum_{i \in [m] : a \in T_i} x_i\Big), \forall a \in [n]\\
      &&& 0 \leq x_i \leq 1, \forall i \in [m]\\
      &&& x \in P(\mathcal{M}) \text{ the matroid polytope of } \mathcal{M}
    \end{aligned}
  \end{equation}
  \label{defi:relaxedMatProg}
\end{defi}

\begin{theo*}
  Let $x,c$ a feasible solution of the program \ref{defi:relaxedMatProg} and $X \sim \Ber(x)$. Let us call $\alpha_{\varphi} = \min_{x \in [m]} \alpha_{\varphi}(x)$. Then:
  \[\mathbb{E}_{X \sim \Ber(x)}[C^{\varphi}(X)] \geq \alpha_{\varphi} \sum_{a \in [n]} w_ac_a\]
  In particular, this implies that the described polynomial time algorithm has an approximation ratio of $\alpha_{\varphi}$:
  \[C^{\varphi}(x^{\text{int}}) \geq \alpha_{\varphi} \sum_{a \in [n]} w_ac^*_a \geq \alpha_{\varphi} \max_{S \in \mathcal{M}} C^{\varphi}(S)\]
  \label{theo:AlgoMat}
\end{theo*}

\subsection{Multiple agents problem}
Inspired by \cite{PM19}, we study a kind of generalization of this problem coming from game theory. We consider now $k$ agents, and each one of those chooses one cover set among his different possibilities $\mathcal{A}_i := \set{\mathcal{A}_i^1,\ldots,\mathcal{A}_i^{m_i}} \subseteq \mathcal{P}([n])$. As a remark, we should note that it is not exactly a generalization of the $\varphi$-\textsc{MaxCoverage} problem: when all agents have the same cover sets $\mathcal{A}_i =\set{T_1, \ldots, T_m}$, each cover set can be taken several times by different agents, whereas in our case we do not allow this.

The value we want to maximize is called the global welfare, among possible allocations $A = (A_1,\ldots,A_k) \in \mathcal{A} := \mathcal{A}_1 \times \ldots \times \mathcal{A}_k$, which is given by the following formula:

\[W^{\varphi}(A) := \sum_{e \in [n]} w_e\varphi(\abs{A}_e)\]

where $\abs{A}_e :=  \abs{\set{i \in [k] : e \in A_i}}$.

We show that this problem is equivalent to our problem under some specific matroid constraints. Given an instance of the multiple agents problem, consider the partition matroid $\mathcal{M}$ on $[\sum_{i \in[k]} m_i] := [m_1] + \ldots + [m_k]$ where $(B_i) := ([m_i])$ is a partition of our ground set and we take each degree $d_i=1$. Independent sets are then subsets $I \subseteq [\sum_{i \in[k]} m_i]$ such that $\forall i  \in [k], \abs{I \cap B_i} \leq d_i = 1$.
Thus we have a bijection $f$ between allocations $A \in \mathcal{A}$ and maximal independent sets of $\mathcal{M}$ such that $W^{\varphi}(A) = C^{\varphi}(f(A))$. The maximization of the welfare of multiple agents is then solved with our algorithm, which implies in particular that it is $\alpha_{\varphi}$-approximable in polynomial time thanks to theorem \ref{theo:AlgoMat}.

\subsection{Application to the \textsc{Vehicle-Target Assignment} problem}
In \cite{PM19}, they compute the \emph{Price of Anarchy}, ie. the ratio of the worst Nash Equilibrium on the best global welfare, of the game theory point of view of the multiple agents problem. In particular, since finding a Nash Equilibrium in a particular subset of instances can be found in polynomial time with the round-robin algorithm (see Part VI of \cite{PM19} for more details), the \emph{Price of Anarchy} is their approximation ratio of the multiple agents welfare problem. We have compared their numerical values with our ratio on the \textsc{Vehicle-Target Assignment} problem. For our concerns, we should only focus on the form of $\varphi^p$ which depends only on a parameter $p \in ]0,1[$ : $\varphi^p(j) = \frac{1-(1-p)^j}{p}$ for $j \in \mathbb{N}$. By property \ref{prop:VTA}, we have that $\alpha_{\varphi^p} = \frac{1 - e^{-p}}{p}$.

    Also, since $\varphi^p$ is bounded by $\frac{1}{p}$, the hardness result proved afterwards will hold, so this approximation ratio is tight at least in the general matroid constraints case. When we compare the curve of $p \mapsto \alpha_{\varphi^p} = \frac{1 - e^{-p}}{p}$ to the numerical study of the Price of Anarchy by \cite{PM19}, it seems that we get a slightly better approximation ratio as we can see on figure \ref{figVTA}:

    \begin{figure}[!h]
    \begin{multicols}{2}
      
      \includegraphics[scale=0.20]{VTA_PoA_PM19.png}

      \columnbreak

      \begin{center}
       \begin{tikzpicture}
          \begin{axis}[
            xmin = 0, xmax = 1,
            ymin = 0.5, ymax = 1,
            xtick distance = 0.2,
            ytick distance = 0.1,
            grid = both,
            width = 0.52\textwidth,
            height = 0.33\textwidth,
            legend cell align = {left},
            legend pos = north east,
            xlabel=$p$,
            ]
            \addplot[
              domain = 0:1,
              samples = 21,
              smooth,
              black,
              mark = x,
            ] table[x=p,y=alpha,col sep=comma] {VTA.csv};
            \addplot[
              domain = 0.001:1,
              samples = 21,
              smooth,
              red,
              mark = diamond,
            ] table[x=p,y=app,col sep=comma] {VTA.csv};
            \legend{$\alpha_{\varphi^p} = \frac{1 - e^{-p}}{p}$,App $=1 - ce^{-1}$}
          \end{axis}
       \end{tikzpicture}
    \end{center}
    \end{multicols}
    \caption{Comparison with the results of \cite{PM19} for the \textsc{Vehicle-Target Assignment} problem. On the left, the black curve PoA$^{\text{opt}}$ from \cite{PM19} is the Price of Anarchy coming from the optimal strategy for $m=10$ players. It yields an approximation algorithm of that ratio for a particular subset of instances where a Nash Equilibrium can be found efficiently. This should be compared to our black curve on the right. It represents our approximation ratio $\alpha_{\varphi^p}$, valid for all instances, which is slightly greater than PoA$^{\text{opt}}$. The red curve App depicts in both figures the general approximation ratio (see \cite{SVW17}) obtained for submodular function with curvature $c$, with $c=1-\varphi^p(m) + \varphi^p(m-1)$ here.}
    \label{figVTA}
\end{figure}

    For our concerns, in the figure on the left taken in \cite{PM19}, we only look at the black curve PoA$^{\text{opt}}$, which corresponds to the approximation ratio they have obtained, and the red curve App, which corresponds to the usual approximation ratio $1 - ce^{-1}$ with the curvature $c = 1-\varphi^p(m) + \varphi^p(m-1)$ with $m=10$ players in their simulation. On the right, the red curve is the same as on the left, whereas the black curve is $\alpha_{\varphi^p}$. We can see that it is slightly greater than the curve obtained by \cite{PM19}. In particular, for $p=0.2$, we have that $\alpha_{\varphi^{0.2}} \simeq 0.9063$ where their PoA$^{\text{opt}} \simeq 0.89 < \alpha_{\varphi^{0.2}}$.


    \subsection{Application to $d$-Power}
    As a comparison, we have also plotted the case studied in \cite{PM19} where $\varphi^d(j) = j^d$ for some $d \in ]0,1[$. We have then $\alpha_{\varphi^d} = e^{-1}\sum_{k=1}^{+\infty}\frac{k^d}{k!}$ thanks to property \ref{prop:dPower}. Once again, if we compare our result with those of \cite{PM19}, we get the curves depicted in figure \ref{figdPower}.


    \begin{figure}[!h]
    \begin{multicols}{2}
      
      \includegraphics[scale=0.20]{dPower_PoA_PM19.png}

      \columnbreak

      \begin{center}
       \begin{tikzpicture}
          \begin{axis}[
            xmin = 0, xmax = 1,
            ymin = 0.5, ymax = 1,
            xtick distance = 0.2,
            ytick distance = 0.1,
            grid = both,
            width = 0.52\textwidth,
            height = 0.33\textwidth,
            legend cell align = {left},
            legend pos = north west,
            xlabel=$d$,
            ]
            \addplot[
              domain = 0:1,
              samples = 21,
              smooth,
              black,
              mark = x,
            ] table[x=d,y=alpha,col sep=comma] {dPower.csv};
            \addplot[domain = 0:1,
              samples = 21,
              smooth,
              red,
              mark = diamond,
            ] table[x=d,y=app,col sep=comma] {dPower.csv};
            \legend{$\alpha_{\varphi^d} = e^{-1}\sum_{k=1}^{+\infty}\frac{k^d}{k!}$,App $=1 - ce^{-1}$}
          \end{axis}
       \end{tikzpicture}
    \end{center}
    \end{multicols}
    \caption{Comparison with the results of \cite{PM19} for the $d$-Power problem. On the left, the black curve PoA$^{\text{opt}}$ from \cite{PM19} is the Price of Anarchy coming from the optimal strategy for $m=20$ players. It yields an approximation algorithm of that ratio for a particular subset of instances where a Nash Equilibrium can be found efficiently. This should be compared to our black curve on the right t. It represents our approximation ratio $\alpha_{\varphi^d}$, valid for all instances, which is slightly greater than PoA$^{\text{opt}}$. The red curve App depicts in both figures the general approximation ratio (see \cite{SVW17}) obtained for submodular function with curvature $c$, with $c=1-\varphi^d(m) + \varphi^d(m-1)$ here.}
    \label{figdPower}
    \end{figure}

    However, here it seems that the black curves are exactly matching, ie. PoA$^{\text{opt}} = \alpha_{\varphi^d}$ here. The curve App is obtained as previously, but this time their simulation used $m=20$ players.
    \newpage
    
    \section{Hardness of approximation of the $\varphi$-\textsc{MaxCoverage} problem} 
In this section we establish an inapproximability bound for the $\varphi$-\textsc{MaxCoverage} problem with weights $1$ under cardinality constraints. Throughout this section we use $\Gamma$ to denote the universe of elements and, hence, an instance of the $\varphi$-\textsc{MaxCoverage} problem consists of $\Gamma$, along with a collection of subsets $\mathcal{F} = \set{F_i \subseteq \Gamma}_{i=1}^m$  and an integer $k$. Recall that the objective of this problem is to find a size-$k$ subset $S \subseteq [m]$ that maximizes $C^{\varphi}(S) = \sum_{a \in \Gamma}\varphi(\abs{S}_a)$.

We establish the following theorem in this section:

\begin{theo*}
 It is NP-hard to approximate the $\varphi$-\textsc{MaxCoverage} problem for $\varphi(n) = o(n)$ within a factor greater that $\alpha_{\varphi} + \varepsilon$ for any $\varepsilon > 0$.
  \label{theo:Hardness}
\end{theo*}

Our reduction is based on a equivalent problem of the $\delta$-Gap-Label-Cover used in in \cite{DDMS20}, called $h$-\textsc{AryLabelCover}:

\begin{defi}[$h$-\textsc{AryLabelCover}]
  An instance $\mathcal{G} = (V,E,[L],[R],\set{\pi_{e,v}}_{e \in E, v \in e})$ of $h$-\textsc{AryLabelCover} is characterized by an $h$-uniform regular hypergraph $(V, E)$ and bijection constraints $\pi_{e,v} : [L] \rightarrow [R]$. Here, each $h$-uniform hyperedge represents a $h$-ary constraint. Additionally, for any labeling $\sigma : V \rightarrow [L]$, we have the following notions of strongly and weakly satisfied constraints:
  \begin{itemize}
  \item An edge $e = (v_1,\ldots,v_h) \in E$ is \emph{strongly satisfied} by $\sigma$ if:
    \[ \forall x,y \in [h], \pi_{e,v_x}(\sigma(v_x)) = \pi_{e,v_y}(\sigma(v_y)) \]
  \item An edge $e = (v_1,\ldots,v_h) \in E$ is \emph{weakly satisfied} by $\sigma$ if:
    \[ \exists x\not=y \in [h], \pi_{e,v_x}(\sigma(v_x)) = \pi_{e,v_y}(\sigma(v_y)) \]
  \end{itemize}
\end{defi}

\begin{prop}[$\delta,h$-\textsc{AryGapLabelCover}]
  For any fixed integer $h \geq 2$ and fixed $\delta > 0$, there exists $R$ that can be taken as large as we want, such that it is NP-hard for instances $\mathcal{G} (V,E,[L],[R],\set{\pi_{e,v}}_{e \in E, v \in e})$ of $h$-\textsc{AryLabelCover} with right alphabet $[R]$ to distinguish between:
  
  \begin{itemize}
  \item[\textbf{YES:}] There exists a labeling $\sigma$ that \emph{strongly satisfies} all the edges.
  \item[\textbf{NO:}] No labeling \emph{weakly satisfies} more than $\delta$ fraction of the edges.
  \end{itemize}
  \label{prop:AryGapLabelCover}
\end{prop}

This will be proved in appendix \ref{NPhardnessGap}.

The key ingredient to prove theorem \ref{theo:Hardness} is a constant size combinatorial object called partitioning system, similar to the one introduced in \cite{BFGG20}:

\subsection{Partitioning System}
For any set $[n]$, $\mathcal{Q} \subseteq 2^{[n]}$, we overload the definition $C^{\varphi}(\mathcal{Q}) := \sum_{a \in [n]} \varphi(\abs{\mathcal{Q}}_a)$ with $\abs{\mathcal{Q}}_a:=\abs{\set{P \in \mathcal{Q} : a \in P}}$ and $C_a^{\varphi}(\mathcal{Q}) := \varphi(\abs{\mathcal{Q}}_a)$. Let us take $x_{\varphi} \in \text{argmin}_{x \in \mathbb{N}} \alpha_{\varphi}(x)$ if this set is nonempty, thus $\alpha_{\varphi} = \alpha_{\varphi}(x_{\varphi})$. Otherwise, we can do the hardness proof with any $x \in \mathbb{N}$ and the minimum $\alpha_{\varphi} = \lim_{x \rightarrow \infty} \alpha_{\varphi}(x)$ will be reached as a limit result of the hardness of all $\alpha_{\varphi}(x)$.

We say that $\mathcal{Q}$ is an \emph{$x$-cover} of $x \in \mathbb{N}$ if every element of $[n]$ is covered $x$ times, so $C^{\varphi}(\mathcal{Q}) = n\varphi(x)$.

\begin{defi}
  Given $[n]$, an \emph{$([n],h,R,\varphi,\eta)$-partitioning system} consists of $R$ distinct collections of subsets of $[n]$, $\mathcal{P}_1,\ldots,\mathcal{P}_R \subseteq 2^{[n]}$, that satisfy:
  \begin{enumerate}
  \item For every $i \in [R], \mathcal{P}_i$ is a collection of $h$ subsets $P_{i,1}, \ldots, P_{i,h} \subseteq [n]$ each of size $x_{\varphi}n/h$ (thus we need that $h \geq x_{\varphi}$) which is an $x_{\varphi}$-cover.
  \item For any $T \subseteq [R]$ and $\mathcal{Q} = \set{P_i : i \in T}$ such that $P_i \in \mathcal{P}_i$, we have $\abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} \leq \eta n$ where:
    \[ \psi^{\varphi}_{k,h} := \mathbb{E}\Big[\varphi\Big(\Bin\Big(k,\frac{x_{\varphi}}{h}\Big)\Big)\Big]\]
  \end{enumerate}
  \label{defi:PartSystem}
\end{defi}

\begin{rk}
  In particular, for any $\mathcal{Q} = \set{\mathcal{Q}_1, \ldots, \mathcal{Q}_k}$ with $\mathcal{Q}_i$ of size $\frac{x_{\varphi}n}{h}$, we have that $C^{\varphi}(\mathcal{Q}) \leq n\varphi(k\frac{x_{\varphi}}{h})$. Indeed $C^{\varphi}(\mathcal{Q}) = \sum_{a \in [n]} \varphi(\abs{\mathcal{Q}}_a)$ with $\sum_{a \in [n]} \abs{\mathcal{Q}}_a = \sum_{i \in [k]} \abs{\mathcal{Q}_i} = k \times \frac{x_{\varphi}n}{h}$. By concavity of $\varphi$ and Jensen inequality, this function is maximized when all $\abs{\mathcal{Q}}_a$ are equals, where we get $n\varphi(k\frac{x_{\varphi}}{h})$.
\end{rk}
\begin{prop}
  For every choice of $R,h \in \mathbb{N}$ with $h \geq x_{\varphi}$, $\eta > 0$,  $n \geq \eta^{-2}R\varphi(R)^2\log(20(h+1))$, there exists an $([n],h,R,\varphi,\eta)$-partitioning system, which can be found in time exp($Rn$log$n).$poly$(h)$ (this will be constant time in the reduction).
  \label{prop:Partitioning}
\end{prop}

  
\subsection{The Reduction}
  \begin{proof}[Proof of theorem \ref{theo:Hardness}]
    Let $\varepsilon > 0$. We show that is is NP-hard to reach an approximation greater than $\alpha_{\varphi} + \varepsilon$ for the $\varphi$-\textsc{MaxCoverage} problem, via a reduction from $\delta,h$-\textsc{AryGapLabelCover}. We define the following constants:
  \begin{itemize}
    \item Let $\eta = \frac{\varphi(x_{\varphi})}{4x_{\varphi}} \varepsilon$.
    \item Let $h$ be the integer rank when $\abs{\psi^{\varphi}_{h,h} - \alpha_{\varphi}\varphi(x_{\varphi})} \leq \eta$, which exists thanks to lemma \ref{lem:UnboundBinPoi}.
    \item Let $\theta$ the rank when $\frac{\varphi(x)}{x} \leq \eta$, which exists since $\varphi(x) = o(x)$.
    \item Let $\xi = \frac{x_{\varphi}}{\theta}$.
    \item $\delta = \frac{\eta}{2} \frac{\xi^3}{h^2}$

     Given an instance  $\mathcal{G} = (V,E,[L],[R], \Sigma, \set{\pi_{e,v}}_{e \in E, v \in e})$ of $\delta,h$-\textsc{AryGapLabelCover}, where we ask that $R$ is a constant larger than $h$, we construct an instance $(\Gamma, \mathcal{F}, k)$ of the $\varphi$-\textsc{MaxCoverage} problem with:

    \item $k =\abs{V}$
    \item Let $n$ a large enough integer to have the existence of $([n],h,R,\varphi,\eta)$-partitioning systems thanks to property \ref{prop:Partitioning}.
    \item $\Gamma = [n] \times E$
    \item We consider a  $([n],h,R,\varphi,\eta)$-partitioning system, and we call $\mathcal{P} =\set{\mathcal{P}_1,\ldots,\mathcal{P}_R}$ the corresponding set of collections. We first define sets $T_{\beta}^{e,v_j} = P_{\pi_{e,v_j}(\beta),j} \times \set{e}$ for $e = (v_1,\ldots,v_h)$ and we define $F^v_{\beta} := \bigcup_{e \in E:v \in e} T^{e,v}_{\beta}$ and $\mathcal{F} := \set{F^v_{\beta}, v \in V, \beta \in [L]}$.
  \end{itemize}

Let us prove completeness and soundness properties. Precisely, if we are in a YES instance, we have that there exists $\mathcal{T}$ such that $C^{\varphi}(\mathcal{T}) \geq \varphi(x_{\varphi})\abs{\Gamma}$. If we are in a NO instance, then we have that for all $\mathcal{T}$ of size $k = \abs{V}$, $C^{\varphi}(\mathcal{T}) \leq (\alpha_{\varphi} + \varepsilon)\varphi(x_{\varphi})\abs{\Gamma}$. 

Thus, if we could approximate within a strictly better ratio than $\alpha_{\varphi} + \varepsilon$, we would be able to distinguish between those two instances, which is known to be NP-hard by the $\delta,h$-\textsc{AryGapLabelCover}. Indeed, if we had a YES instance, we would get that $C^{\varphi}(\mathcal{T}_{\text{approx}}) > (\alpha_{\varphi} + \varepsilon)\varphi(x_{\varphi})\abs{\Gamma}$, so it would tell us in particular that we are not in a NO instance. Thus we would be in a YES instance iff $C^{\varphi}(\mathcal{T}_{\text{approx}}) > (\alpha_{\varphi} + \varepsilon)\varphi(x_{\varphi})\abs{\Gamma}$. So finding such a $\mathcal{T}_{\text{approx}}$ is NP-hard.

\subsubsection{Completeness}
Suppose the given $h$-\textsc{AryLabelCover} instance $\mathcal{G}$ is a YES instance. Then, there exists a labeling $\sigma : V \mapsto [L]$ which strongly satisfies all edges. Consider the collection of $\abs{V}$ subsets $\mathcal{T} := \set{F_{\sigma(v)}^v : v \in V}$. Since $e = (v_1,\ldots,v_h)$ is strongly satisfied by $\sigma$, this means that there exists $r \in [R]$ such that $\pi_{e,v_i}(\sigma(v_i)) = r$ for all $i \in [h]$. So $P_{r,i} \times \set{e} = T_{\sigma(v_i)}^{e,v_i} \subseteq F_{\sigma(v_i)}^{v_i}$. Since by construction, $(P_{r,i})_{i \in [h]}$ is an $x_{\varphi}$-cover of $[n]$, we have that $C^{\varphi}(\mathcal{T}) \geq \varphi(x_{\varphi})n\abs{E} = \varphi(x_{\varphi})\abs{\Gamma}$.

\subsubsection{Soundness}
Suppose the given $h$-\textsc{AryLabelCover} instance $\mathcal{G}$ is a NO instance. Let us prove the contrapositive of the soundness: we suppose that there exists $\mathcal{T}$ of size $k = \abs{V}$ such that  $C^{\varphi}(\mathcal{T}) > (\alpha_{\varphi} + \varepsilon)\varphi(x_{\varphi})\abs{\Gamma}$. Let us show that there exists a labelling $\sigma$ that weakly satisfies a strictly larger fraction of the edges than $\delta$.

For every vertex $v \in V$, we define $L(v) := \set{\beta \in [L] : F_{\beta}^v \in \mathcal{T}}$ to be the candidate set of labels that can be associated with the vertex $v$. We extend this definition to hyperedges $e = (v_1,\ldots,v_h)$ where we define $L(e) := \bigcup_{x \in [h]} L(v_x)$ to be the \emph{multiset} of all labels associated with the edge.

We say that $e = (v_1,\ldots,v_h) \in E$ is \emph{consistent} iff $\exists x \not= y \in [h], \pi_{e,v_x}(L(v_x)) \cap \pi_{e,v_y}(L(v_y)) \not= \emptyset$. Let us call for an edge $e$ the contribution of that edge $C^{\varphi,e} := \sum_{a \in [n]_e} C^{\varphi}_a$. We have then that $C^{\varphi,e}(\mathcal{T}) = C^{\varphi}(\mathcal{T}_e)$ where $\mathcal{T}_e = \mathcal{T} \cap \set{F^v_{\beta} :v \in e, \beta \in [L]}$ and in particular $\abs{\mathcal{T}_e} = \abs{L(e)}$.

We then decompose $E$ in three parts:
\begin{itemize}
\item $B$ is the set of edges $e \in E$ with $\abs{L(e)} \geq \frac{h}{\xi}$.
\item $N$ is the set of consistent edges $e \in E$ with $\abs{L(e)} < \frac{h}{\xi}$.
\item $I = E - (B \cup N)$ is the set of inconsistent edges $e \in E$ with $\abs{L(e)} < \frac{h}{\xi}$.
\end{itemize}

We want to show that the contribution of $N$ is not to small, which we will use to construct a labelling weakly satisfying enough edges.

\begin{lem}
  The contribution of $B$ is $\sum_{e \in B} C^{\varphi,e}(\mathcal{T}) \leq \frac{\varepsilon}{4}\varphi(x_{\varphi})\abs{\Gamma}$.
  \label{lem:contribB}
\end{lem}
\begin{proof}
 First we note that:

\[ \mathbb{E}_{e \sim E}[\abs{L(e)}] \leq \frac{h}{\abs{V}} \sum_{v \in V} \abs{L(v)} = h\]

since the hypergraph is regular, hence picking a hyperedge uniformly at random corresponds to selecting vertices with probability $\frac{h}{\abs{V}}$ each. For the last equality, this comes from the fact that $\sum_{v \in V} \abs{L(v)} = \abs{\mathcal{T}} = \abs{V}$.

Therefore, via Markovâs inequality, the size of the label set $\abs{L(e)}$ is greater than $\frac{h}{\xi}$ for at most $\xi$ fraction of the hyperedges:

\[\mathbb{P}\Big(\abs{L(e)} \geq \frac{h}{\xi}\Big) \leq \xi\]

Then we can show that the contribution of edges with large label sets is small. Indeed:

  \[ \sum_{e \in B} C^{\varphi,e}(\mathcal{T}) \leq \sum_{e \in B} n\varphi\Big(\abs{\mathcal{T}_e}\frac{x_{\varphi}}{h}\Big) \leq \abs{B} \times n\varphi\Big(\frac{\abs{E}h}{\abs{B}}\frac{x_{\varphi}}{h}\Big) = \frac{\varphi\big(\frac{\abs{E}x_{\varphi}}{\abs{B}}\big)}{\frac{\abs{E}x_{\varphi}}{\abs{B}}} x_{\varphi} \abs{\Gamma} \]

  By the remark on definition \ref{defi:PartSystem}, we get the first inequality. For the second one, we use the concavity of $\varphi$ via Jensen's inequality and the fact that $\sum_{e \in B} \abs{\mathcal{T}_e} \leq \sum_{e \in E} \abs{\mathcal{T}_e} \leq \abs{E}h$.
  But $\frac{\abs{B}}{\abs{E}} = \mathbb{P}\big(\abs{L(e)} \geq \frac{h}{\xi}\big) \leq \xi$ so $\frac{\abs{E}x_{\varphi}}{\abs{B}} \geq \frac{x_{\varphi}}{\xi} = \theta$. By definition of $\theta$, we get that $\sum_{e \in B} C^{\varphi,e}(\mathcal{T}) \leq \eta x_{\varphi} \abs{\Gamma} = \frac{\varepsilon}{4} \varphi(x_{\varphi})\abs{\Gamma}$.
\end{proof}

In order to bound the contribution of $I$, we will some property on inconsistent edges:

\begin{prop}
  $e = (v_1,\ldots,v_h) \in E$ be an inconsistent hyperedge with respect to $\mathcal{T}$. Then we have that $\abs{C^{\varphi,e}(\mathcal{T}) - \psi^{\varphi}_{\abs{L(e)},h}n } \leq \eta n$.
  \label{prop:inconsistent}
\end{prop}

\begin{proof}
  Since $e$ is inconsistent, $\forall x \not= y \in [h], \pi_{e,v_x}(L(v_x)) \cap \pi_{e,v_y}(L(v_y)) = \emptyset$. Therefore, for every $i \in [R]$, there is at most one $v \in e$ such that $i \in \pi_{e,v}(L(v))$: $\mathcal{T}$ intersects with $\mathcal{P}_i \times \set{e}$ in at most one subset. This gives us a subset $T \subseteq [R]$ of size $\abs{L(e)}$ such that the restriction of $\mathcal{T}$ to $[n] \times \set{e}$ is $\mathcal{Q} = \set{P_{i,j} \times \set{e} : j \in T}$ (up to empty sets). So by the second condition of the partitioning system, we get the expected result.
\end{proof}

Now, we can bound the contribution of $I$:

\begin{lem}
  The contribution of $I$ is $\sum_{e \in I} C^{\varphi,e}(\mathcal{T}) \leq (\alpha_{\varphi} + \frac{\varepsilon}{2})\varphi(x_{\varphi})\abs{\Gamma}$.
  \label{lem:contribI}
\end{lem}

\begin{proof}
Thanks to property \ref{prop:inconsistent}, we have:

  \begin{equation}
    \begin{aligned}
      \sum_{e \in I} C^{\varphi,e}(\mathcal{T}) &\leq&& \sum_{e \in I} (\psi^{\varphi}_{\abs{L(e)},h} +\eta)n \leq \sum_{e \in E} (\psi^{\varphi}_{\abs{L(e)},h} +\eta)n
    \end{aligned} 
  \end{equation}

  since $I \subseteq E$ and $\psi^{\varphi}_{\abs{L(e)},h} \geq 0$. But $\sum_{e \in E} \abs{L(e)} = \abs{E}h$ and $x \mapsto \psi^{\varphi}_{x,h}$ is concave thanks to lemma \ref{lem:BinCon}, so we can use Jensen's inequality to get $\sum_{e \in E} \psi^{\varphi}_{\abs{L(e)},h} \leq \abs{E} \psi^{\varphi}_{\frac{\sum_{e \in E} \abs{L(e)}}{\abs{E}},h} = \abs{E}\psi^{\varphi}_{h,h}$ and thus:
  
  \begin{equation}
    \begin{aligned}
      \sum_{e \in I} C^{\varphi,e}(\mathcal{T}) &\leq&& (\psi^{\varphi}_{h,h} +\eta)n\abs{E} \leq (\alpha_{\varphi}\varphi(x_{\varphi}) + 2\eta)\abs{\Gamma}
    \end{aligned} 
  \end{equation}

  by definition of $h$. This implies that the total contribution of inconsistent edges $I$ is at most $\sum_{e \in I} C^{\varphi,e}(\mathcal{T}) \leq (\alpha_{\varphi}\varphi(x_{\varphi}) + 2\eta)\abs{\Gamma} \leq (\alpha_{\varphi}+ \frac{\varepsilon}{2})\varphi(x_{\varphi})\abs{\Gamma}$ by definition of $\eta$ (and $x_{\varphi} \geq 1$).
\end{proof}

\bigskip

Since we have supposed that $\sum_{e \in E} C^{\varphi,e}(\mathcal{T}) = C^{\varphi}(\mathcal{T}) > (\alpha_{\varphi} + \varepsilon)\varphi(x_{\varphi})\abs{\Gamma}$, and we the help of lemmas \ref{lem:contribB} and \ref{lem:contribI}, we have that the contribution of $N$ is:

\[\sum_{e \in N}  C^{\varphi,e}(\mathcal{T}) > \frac{\varepsilon}{4}\varphi(x_{\varphi})\abs{\Gamma}\]

However, we have that for $e \in N$ that $C^{\varphi,e}(\mathcal{T}) \leq  n\varphi\Big(\abs{\mathcal{T}_e}\frac{x_{\varphi}}{h}\Big) = n\varphi\Big(\abs{L(e)}\frac{x_{\varphi}}{h}\Big) \leq n \varphi\Big(\frac{x_{\varphi}}{\xi}\Big) \leq \frac{nx_{\varphi}}{\xi}$ thanks to the remark on definition \ref{defi:PartSystem} and the bound $\abs{L(e)} < \frac{h}{\xi}$. This implies that:

\[\frac{\abs{N}}{\abs{E}} \geq \frac{\xi}{x_{\varphi}}\frac{\varepsilon \varphi(x_{\varphi})}{4} = \xi \eta\]

\bigskip

Finally, we construct a randomized labeling $\sigma : V \mapsto [L]$ as follows: for $v \in V$, if $L(v) \not= \emptyset$, set $\sigma(v)$ uniformly form $L(v)$, otherwise set it arbitrarily. We claim that in expectation, this labeling must weakly satisfy $\delta$ fraction of the hyperedges.

To see this, fix any $e = (v_1,\ldots,v_h) \in N$. Thus $\exists x \not= y \in [h], \pi_{e,v_x}(L(v_x)) \cap \pi_{e,v_y}(L(v_y)) \not= \emptyset$. Furthermore $\abs{L(v_x)},\abs{L(v_y)} \leq \frac{h}{\xi}$. Thus, we have that $\pi_{e,v_x}(L(v_x)) = \pi_{e,v_y}(L(v_y))$ with probability at least $\frac{1}{\abs{L(v_x)}\abs{L(v_y)}} \geq \Big(\frac{\xi}{h}\Big)^2$.

Therefore:

\begin{equation}
  \begin{aligned}
    &&& \mathbb{E}_{\sigma}\mathbb{E}_{e \sim E}[\sigma \text{ weakly satisfies } e]\\
    &\geq&& \xi \eta \mathbb{E}_{\sigma}\mathbb{E}_{e \sim E}[\sigma \text{ weakly satisfies } e | e \in N]\\
    &>&& \frac{\eta}{2} \frac{\xi^3}{h^2} = \delta
  \end{aligned}
\end{equation}

In particular there exists some labeling $\sigma$ such that $\mathbb{E}_{e \sim E}[\sigma \text{ weakly satisfies } e] > \delta$, and thus the soundness is also proved.
\end{proof}

  \subsection{Hardness of approximation of the Multiple-Agents problem}
  SKETCH: The proof is the same as before, but instead of $\mathcal{F} := \set{F^v_{\beta}, v \in V, \beta \in [L]}$, we take $k = \abs{V}$ the number of agents and $\mathcal{A}_i := \set{F^{v_i}_{\beta}, \beta \in [L]}$ where $V =\set{v_1, \ldots, v_k}$. So instead of subsets of $\mathcal{F}$ of size $k$, we take only one set $F^v_{\beta} \in \mathcal{F}$ for each $v \in V$.

  For the completeness part, the subset described is already of the right form. For the soundness part, this constraint on the shape of the subset of $\mathcal{F}$ only helps us, since it gives more constraints on the given subset from which we want to construct a labelling. Thus both parts of the proof work, and we have the NP-hardness also in this particular case of matroids (which is not a particular case of the uniform matroid and vice-versa).

  As a consequence, if we can reach the PoA in polytime for the games considered (not sure: we have congestion games, but it is not enough, cf VI of \cite{PM19}), this would imply that PoA $\leq \alpha_{\varphi}$, since the PoA would give a polytime algo. The remaining objective would be to show that they are in fact equals as it is suggested numerically.

  
\section*{Conclusion}
The standard coverage function $C(S)$ counts the number of elements $a \in [n]$ that are covered by at least one set $T_i$ with $i \in S$. Note that the contribution of an $a \in [n]$ to $C(S)$ is exactly the same whether $a$ appears in just one set $T_i$ or in all of them. It is very natural to consider settings wherein having more than one copy of $a$ is more valuable than just one copy of it. The $\varphi$-\textsc{MaxCoverage} problem we have introduced study exactly this: having $c$ copies of element $a$ gives a value $\varphi(c)$. We have shown that when $\varphi$ is concave nondecreasing, we can take advantage of this structure and obtain a better approximation guarantee given by the \emph{Poisson concavity ratio} $\alpha_{\varphi} := \inf_{x \in \mathbb{N}} \frac{\mathbb{E}[\varphi(\Poi(x))]}{\varphi(\mathbb{E}[\Poi(x)])}$, which is tight for sublinear functions.

An interesting open question is whether there exists combinatorial algorithms that achieve this approximation ratio. As mentioned in \cite{BFGG20}, for the maximum $\ell$-multi-coverage with $\ell \geq 2$, which is the special case where $\varphi(x) = \min(x,\ell)$, the simple greedy algorithm only gives a $1 - e^{-1}$ approximation ratio, which is strictly less than the ratio $\alpha_{\varphi} = 1-\frac{\ell^{\ell}e^{-\ell}}{\ell!}$ in that case.
  
\bibliographystyle{plain}
\bibliography{these.bib}

\newpage
\appendix

\section{General properties on $C^{\varphi}$}
\begin{prop}
  $C^{\varphi}$ is submodular, its curvature is at most $c = 1 - (\varphi(m) - \varphi(m-1))$ and it cannot be improved for a general instance with $m$ cover sets.
  \label{prop:SubCurv}
\end{prop}

\begin{proof}

  We use the following lemma which is trivial to prove:

  \begin{lem}[Properties of $\abs{S}_a = \abs{\set{i \in S : a \in T_i}}$.]
    We have:
  \begin{enumerate}
  \item $\abs{S}_a \leq \abs{S}$
  \item $\abs{S \cup S'}_a\leq \abs{S}_a + \abs{S'}_a$. In particular, if $S \subseteq T$ then $\abs{S}_a \leq \abs{T}_a$ and $\abs{S\cup\set{x}}_a \leq \abs{S}_a + 1$.
  \item If $S \subseteq T$, $x \not\in T$ then $\abs{S}_a = \abs{T}_a \Rightarrow \abs{S\cup\set{x}}_a = \abs{T\cup\set{x}}_a$
  \end{enumerate}
  \label{lem:ke}
\end{lem}
  
  Let us show first the submodularity of $C^{\varphi}$. Let $S \subseteq T \subseteq [m]$ and $x \not\in T$:
  \begin{equation}
    \begin{aligned}
      &&& C^{\varphi}(S \cup \set{x}) - C^{\varphi}(S) - (C^{\varphi}(T \cup \set{x}) - C^{\varphi}(T))= \\
      &=&& \sum_{a \in [n]} w_a[\varphi(\abs{S\cup\set{x}}_a) - \varphi(\abs{S}_a) - (\varphi(\abs{T\cup\set{x}}_a) - \varphi(\abs{T}_a))]\\
    \end{aligned}
  \end{equation}

  Let us call $g(a) := \varphi(\abs{S\cup\set{x}}_a) - \varphi(\abs{S}_a) - (\varphi(\abs{T\cup\set{x}}_a) - \varphi(\abs{T}_a))$:
  \begin{enumerate}
  \item If $\abs{T}_a = \abs{S}_a$ then thanks to lemma \ref{lem:ke}, we have that $\abs{T\cup\set{x}}_a = \abs{S\cup\set{x}}_a$, so $g(a) = 0$
  \item Else, we have that $\abs{T}_a > \abs{S}_a$:
    \begin{enumerate}
    \item If $\abs{S\cup\set{x}}_a = \abs{S}_a$, then we add elements of $T-S$ using lemma \ref{lem:ke} to get that $\abs{T\cup\set{x}}_a = \abs{T}_a$, so $g(a)=0$ in that case.
    \item Else $\abs{S\cup\set{x}}_a \not= \abs{S}_a$. So with $\abs{S}_a = k$, we get that $\abs{S\cup\set{x}}_a = k+1$ and $\abs{T}_a > \abs{S}_a$ so $\abs{T}_a \geq k+1$.

      \begin{enumerate}
      \item If $\abs{T\cup\set{x}}_a = \abs{T}_a$, then $g(a) =  \varphi(k+1) - \varphi(k) \geq 0$ since $\varphi$ is nondecreasing.
      \item Else $\abs{T\cup\set{x}}_a \not= \abs{T}_a$ so with $\abs{T}_a = \ell$ with $\ell \geq k+1$, we get that $\abs{T}_a = \ell+1$. So we have that:
        \begin{equation}
          \begin{aligned}
            g(a) &=&& \varphi(k+1) - \varphi(k) - (\varphi(\ell+1) - \varphi(\ell))\\
            &=&& \frac{\varphi(k+1) - \varphi(k)}{(k+1) - k} - \frac{\varphi(\ell+1) - \varphi(\ell)}{(\ell+1) - \ell} \geq 0
          \end{aligned}
        \end{equation}
        by concavity of $\varphi$: its slopes are nonincreasing.
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
  So in all cases, we have $g(a) \geq 0$ so $ C^{\varphi}(S \cup \set{x}) - C^{\varphi}(S) - (C^{\varphi}(T \cup \set{x}) - C^{\varphi}(T)) \geq 0$: $C^{\varphi}$ is submodular.

  Let us now compute its curvature:
  \[c = 1 - \min_{i \in [m]} \frac{C^{\varphi}([m]) - C^{\varphi}([m]-\set{i})}{C^{\varphi}(\set{i}) - C^{\varphi}(\emptyset)}\]

  Let $i \in [m]$ fixed:
  \begin{equation}
    \begin{aligned}
      &&& \frac{C^{\varphi}([m]) - C^{\varphi}([m]-\set{i})}{C^{\varphi}(\set{i}) - C^{\varphi}(\emptyset)}\\
      &=&& \frac{\sum_{a \in [n]} w_a[\varphi(\abs{[m]}_a) - \varphi(\abs{[m]-\set{i}}_a)]}{\sum_{a \in [n]}w_a[\varphi(\abs{\set{i}}_a) - \varphi(\abs{\emptyset}_a)]}\\
      &=&& \frac{\sum_{a \in T_i} w_a[\varphi(\abs{[m]}_a) - \varphi(\abs{[m]-\set{i}}_a)]}{\sum_{a \in T_i} w_a}\\
      &=&& \frac{\sum_{a \in T_i} w_a[\varphi(\abs{[m]}_a) - \varphi(\abs{[m]}_a-1)]}{\sum_{a \in T_i} w_a} \text{ since } a \in T_i\\
    \end{aligned}
  \end{equation}

  But $\abs{[m]}_a \leq m$ and $\varphi$ concave, so $\varphi(\abs{[m]}_a)) - \varphi(\abs{[m]}_a-1) \geq \varphi(m) - \varphi(m-1)$ for all $a \in [n]$. As a consequence we have that:

  \[\frac{C^{\varphi}([m]) - C^{\varphi}([m]-\set{i})}{C^{\varphi}(\set{i}) - C^{\varphi}(\emptyset)} \geq \varphi(m) - \varphi(m-1)\]
  
  and this lower bound is true for its minimum over $i$. Thus we get that $c \leq 1 - (\varphi(m) - \varphi(m-1))$.
  Also one can find instances for all $m$ such that this bound is tight: take $T_1 =\set{a}$ and $\forall j \in [m], a \in T_j$ for instance.
\end{proof}

\begin{prop}
  The approximation ratio $\alpha_{\varphi}$ is always better than the general ratio given in $\cite{SVW17}$: $\min_{x \in [m]} \alpha_{\varphi}(x) \geq 1 - ce^{-1}$ with $1-c = \varphi(m) - \varphi(m-1)$. Furthermore, we have always that $\alpha_{\varphi}(0) = 1$, so the minimum range in the definition of $\alpha_{\varphi}$ can be taken on positive integers only.
  \label{prop:BetterRatio}
\end{prop}

\begin{proof}
  We first suppose that $x \geq 1$. Also, we can ask that for all $j \geq m$, we have $\varphi(j+1) -\varphi(j) = \varphi(m) -\varphi(m-1)$, since these quantities are never achieved in a instance with $m$ cover sets. So we can suppose that for all $j$ we have $\varphi(j+1) -\varphi(j) \geq \varphi(m) -\varphi(m-1)$. Also for $k \geq 1$, one can write:

  \[\varphi(k)= \sum_{j=0}^{k-1} \varphi(j+1) -\varphi(j) = 1 + \sum_{j=1}^{k-1} \varphi(j+1) -\varphi(j) \]
  
  Thus:
  
  \begin{equation}
    \begin{aligned}
      &&&\mathbb{E}[\varphi(\Poi(x))] = e^{-x}\sum_{k=1}^{+\infty}\varphi(k) \frac{x^k}{k!}\\
      &=&& e^{-x}\sum_{k=1}^{+\infty}\Big(1 + \sum_{j=1}^{k-1} \varphi(j+1) -\varphi(j)\Big) \frac{x^k}{k!} \\
      &=&& e^{-x}\Big[(e^x - 1) + \sum_{k=1}^{+\infty}\Big(\sum_{j=1}^{k-1} \varphi(j+1) -\varphi(j)\Big) \frac{x^k}{k!} \Big]\\
      &\geq&& (1 - e^{-x}) + e^{-x}\sum_{k=1}^{+\infty}(k-1)[\varphi(m) -\varphi(m-1)] \frac{x^k}{k!}\\
      &=&& (1 - e^{-x}) + [1-c]e^{-x}\Big(x\sum_{k=1}^{+\infty}\frac{x^{k-1}}{(k-1)!} - \sum_{k=1}^{+\infty}\frac{x^k}{k!}\Big)\\
      &=&& (1 - e^{-x}) + [1-c]e^{-x}\Big(xe^x - (e^x-1)\Big)\\
      &=&& 1 - e^{-x} + [1-c](x-1 + e^{-x}) =: f(x)
    \end{aligned}
  \end{equation}

  But since $\varphi(x) \leq x$  and with $g(x) := \frac{f(x)}{x}$, we get that $g'(x) =\frac{c}{x^2}(x-1+e^{-x}) \geq 0$ for $x \geq 1$, thus $g(x) \geq g'(1)$ and then

  \[\alpha_{\varphi}(x) \geq \frac{f(x)}{\varphi(x)} \geq g(x) \geq g(1) = f(1) \]

  with $f(1) =  1 - e^{-1} + (\varphi(m) - \varphi(m-1)) e^{-1}$ the approximation ratio of the general algorithm.

  Let us now suppose that $x \in [0,1]$. We have that $\varphi(x) = x$ on that interval since we have taken its piecewise linear extension and $\varphi(0) = 0$ and $\varphi(1) = 1$. Thus we have that
  \[\alpha_{\varphi}(x) = \frac{\mathbb{E}[\varphi(\Poi(x))]}{x} = e^{-x}\sum_{k=1}^{+\infty}\frac{\varphi(k)}{k}\frac{x^{k-1}}{(k-1)!} \]

  and we can then compute its value at $0$: $\alpha_{\varphi}(0) = e^{-0}\sum_{k=1}^{+\infty}\frac{\varphi(k)}{k}\frac{0^{k-1}}{(k-1)!} = \frac{\varphi(1)}{1}\frac{0^0}{0!} = 1$. Since $\alpha_{\varphi}(x) \leq 1$, we have that $\alpha_{\varphi} = \inf_{x \in \mathbb{N}^*} \alpha_{\varphi}(x)$.
\end{proof}

\begin{lem}
    For $\varphi$ nondecreasing concave with $\varphi(0)=0,\varphi(1)=1$, we have that the function $g : x \mapsto \mathbb{E}[\varphi(\Poi(x))]$ on $\mathbb{R}^+$ is $\mathcal{C}^{\infty}$ nondecreasing concave.
    \label{lem:PoiCon}
  \end{lem}

  \begin{proof}
    Since we have that $\varphi(k) \leq k$ for $k \in \mathbb{N}$, in particular $g(x) = e^{-x}\sum_{k=0}^{+\infty} \varphi(k)\frac{x^k}{k!}$ is $\mathcal{C}^{\infty}$. It is thus enough to compute its first and second derivatives:

    \begin{equation}
      \begin{aligned}
        g'(x) &=&& -e^{-x}\sum_{k=0}^{+\infty}\varphi(k)\frac{x^k}{k!}+ e^{-x}\sum_{k=1}^{+\infty}\varphi(k)e^{-x}k\frac{x^{k-1}}{k!}\\
        &=&& -e^{-x}\sum_{k=0}^{+\infty}\varphi(k)\frac{x^k}{k!}+ e^{-x}\sum_{k=0}^{+\infty}\varphi(k+1)e^{-x}\frac{x^{k}}{k!}\\
        &=&& e^{-x}\sum_{k=0}^{+\infty}(\varphi(k+1) -\varphi(k))\frac{x^k}{k!}
      \end{aligned}
    \end{equation}

    But $\varphi(k+1) -\varphi(k) \geq 0$ since $\varphi$ nondecreasing, so $g'(x) \geq 0$ and $g$ is nondecreasing.

    
    The calculus of $g''$ is the same where we replace $\varphi$ by $\psi(k) := \varphi(k+1) -\varphi(k)$ which is a nonincreasing function by concavity of $\varphi$. Thus:

    \[g''(x) = e^{-x}\sum_{k=0}^{+\infty}(\psi(k+1) -\psi(k))\frac{x^k}{k!} \leq 0\]

    since $\psi(k+1) -\psi(k) \leq 0$, and so $g$ is concave.
\end{proof}

\section{Proofs on calculus of $\alpha_{\varphi}$}
\begin{prop}
  For $\ell \in \mathbb{N}^*$ and $\varphi(j) = \min(j,\ell)$, we have that $\alpha_{\varphi}=1-\frac{\ell^{\ell}e^{-\ell}}{\ell!}$.
  \label{prop:lCover}
\end{prop}
\begin{proof}
  Thanks to property \ref{prop:BetterRatio}, we have that $\alpha_{\varphi} = \inf_{x \in \mathbb{N}^*} \alpha_{\varphi}(x)$. Let us compute $\mathbb{E}[\varphi(\Poi(x))]$:

  \begin{equation}
    \begin{aligned}
      \mathbb{E}[\varphi(\Poi(x))] &=&& e^{-x}\sum_{k=0}^{+\infty}\varphi(x)\frac{x^k}{k!}\\
      &=&& e^{-x}\sum_{k=0}^{\ell}k\frac{x^k}{k!} + e^{-x}\sum_{k=\ell+1}^{+\infty}\ell\frac{x^k}{k!}\\
      &=&& e^{-x}x \sum_{k=0}^{\ell-1}\frac{x^k}{k!} + \ell e^{-x}\sum_{k=\ell+1}^{+\infty}\frac{x^k}{k!}\\
      &=&& e^{-x}\Big[(x-\ell)\sum_{k=0}^{\ell-1}\frac{x^k}{k!} - \ell\frac{x^{\ell}}{\ell!}\Big] + \ell e^{-x}\sum_{k=0}^{+\infty}\frac{x^k}{k!}\\
      &=&& \ell - e^{-x}\Big[\frac{x^{\ell}}{(\ell-1)!} - (x-\ell)\sum_{k=0}^{\ell-1}\frac{x^k}{k!}\Big]
    \end{aligned}
  \end{equation}

  Let us show that $\alpha_{\varphi}(x)$ takes its minimum in $\ell$, where we have indeed:
  \[ \alpha_{\varphi}(\ell) = \frac{1}{\ell}\Big( \ell - e^{-\ell}\Big[\frac{\ell^{\ell}}{(\ell-1)!} - (\ell-\ell)\sum_{k=0}^{\ell-1}\frac{\ell^k}{k!}\Big]\Big) = 1 - e^{-\ell}\frac{\ell^{\ell}}{\ell!} \]

  $\alpha_{\varphi}(x)$ is derivable: let us compute it from $\ell$ to $+\infty$ and then from $1$ to $\ell$:

  \begin{equation}
    \begin{aligned}
      \alpha_{\varphi}'(x) &=&&  e^{-x}\Big[\frac{x^{\ell}}{\ell!} -  (x-\ell)\sum_{k=0}^{\ell-1}\frac{x^k}{\ell k!}\Big]\\ 
      &-&& e^{-x}\Big[\frac{x^{\ell-1}}{(\ell-1)!} - \sum_{k=0}^{\ell-1}\frac{x^k}{\ell k!} - (x-\ell)\sum_{k=0}^{\ell-2}\frac{x^{k}}{\ell k!} \Big] \\
      &=&& e^{-x}\Big[\frac{x^{\ell}}{\ell!} - \frac{x^{\ell-1}}{(\ell-1)!} -(x-\ell)\frac{x^{\ell-1}}{\ell!} + \sum_{k=0}^{\ell-1}\frac{x^k}{\ell k!} \Big]\\
      &=&& e^{-x}\sum_{k=0}^{\ell-1}\frac{x^k}{\ell k!} \geq 0
    \end{aligned}
  \end{equation}
  
  So  $\alpha_{\varphi}(x)$ is nondecreasing from $\ell$ to $+\infty$. Let's compute it now between $1$ and $\ell$:
  
  \begin{equation}
    \begin{aligned}
      \alpha_{\varphi}'(x) &=&& -\frac{\ell}{x^2} + e^{-x}\Big[\frac{x^{\ell-1}}{(\ell-1)!} -  \sum_{k=0}^{\ell-1}\frac{x^k}{k!}  + \ell\sum_{k=0}^{\ell-2}\frac{x^k}{(k+1)!} + \frac{\ell}{x}\Big]\\ 
      &-&& e^{-x}\Big[\frac{x^{\ell-2}}{(\ell-2)!} - \sum_{k=0}^{\ell-2}\frac{x^k}{k!} + \ell\sum_{k=0}^{\ell-3}\frac{x^k}{(k+2)k!} - \frac{\ell}{x^2}\Big]\\
      &=&&  \frac{\ell}{x}\Big(e^{-x}\Big(1+\frac{1}{x}\Big) - \frac{1}{x}\Big)\\
      &+&&  e^{-x}\Big[\Big(\frac{\ell}{\ell-1}-1\Big)\frac{x^{\ell-2}}{(\ell-2)!} + \ell\sum_{k=0}^{\ell-3}\Big(\frac{x^k}{(k+1)!} - \frac{x^k}{(k+2)k!} \Big) \Big]\\
      &=&&  \frac{\ell}{x}\Big(e^{-x}\Big(1+\frac{1}{x}\Big) - \frac{1}{x}\Big)\\
      &+&&  e^{-x}\Big[\frac{x^{\ell-2}}{(\ell-1)!} + \ell\sum_{k=0}^{\ell-3}\frac{x^k}{k!}\Big(\frac{1}{k+1} - \frac{1}{k+2} \Big) \Big]\\
      &=&&  \frac{\ell}{x}\Big(e^{-x}\Big(1+\frac{1}{x} + \frac{x^{\ell-1}}{\ell!} + x\sum_{k=0}^{\ell-3}\frac{x^k}{k!}\frac{1}{(k+1)(k+2)}\Big) - \frac{1}{x}\Big)\\
      &=&&  \frac{\ell e^{-x}}{x^2}\Big(\Big(1+x+ \frac{x^\ell}{\ell!} + \sum_{k=0}^{\ell-3}\frac{x^{k+2}}{(k+2)!}\Big) - e^x\Big)\\
      &=&& \frac{\ell e^{-x}}{x^2}\Big(\sum_{k=0}^{\ell} \frac{x^k}{k!} -e^x\Big) \leq 0
    \end{aligned}
  \end{equation}

  since the partial sum of the exponential series is bounded by its total sum. In order to make all this calculus correct, we need to ask that $\ell \geq 2$. In the case where $\ell=1$, we did not need to prove anything. Thus we have that $\alpha_{\varphi}(x)$ is nonincreasing from $1$ to $\ell$, and nondecreasing after, so it takes indeed its minimum in $\ell$ and the property is proved.
\end{proof}

\begin{prop}
  For $p \in ]0,1[$ and $\varphi(j)=\frac{1-(1-p)^j}{p}$, we have that $\alpha_{\varphi} = \frac{1 - e^{-p}}{p}$.
\label{prop:VTA}
\end{prop}

\begin{proof}
    By definition:
    
    \begin{equation}
      \begin{aligned}
        \alpha_{\varphi^p}(x) &=&& \frac{\mathbb{E}[\varphi^p(\Poi(x))]}{\varphi^p(x)} = \frac{\sum_{k=0}^{+\infty}\varphi^p(k)e^{-x}\frac{x^k}{k!}}{\varphi^p(x)}\\
        &=&& \frac{1-e^{-x}\sum_{k=0}^{+\infty}(1-p)^k\frac{x^k}{k!}}{p\varphi^p(x)}\\
        &=&& \frac{1 - e^{-x}e^{(1-p)x}}{p\varphi^p(x)} = \frac{1 - e^{-px}}{p\varphi^p(x)}
      \end{aligned}
    \end{equation}

    If $x \geq 1$, $\alpha_{\varphi^p}(x) = \frac{1 - e^{-px}}{1-(1-p)^x}$ and:

    \[\alpha_{\varphi^p}'(x) = \frac{pe^{-px}(1-(1-p)^x) - \ln(1-p)(1-p)^x(1-e^{-px})}{(1-(1-p)^x)^2}\]
    
    But $1-(1-p)^x > 0, 1-e^{-px} > 0, \ln(1-p) < 0$ since $x > 0$ and $p \in ]0,1[$, so $\alpha_{\varphi^p}'(x) > 0$ for $x \geq 1$ and so $\alpha_{\varphi^p}(x)$ increases from $1$ to infinity. Thus it takes indeed its minimum in $1$:
    
    \[\alpha_{\varphi^p} = \alpha_{\varphi^p}(1) = \frac{1 - e^{-p}}{p}\]
\end{proof}
\begin{prop}
  For $d \in ]0,1[$ and $\varphi(j)=j^d$, we have that $\alpha_{\varphi} = e^{-1}\sum_{k=1}^{+\infty}\frac{k^d}{k!}$
    \label{prop:dPower}
\end{prop}
\begin{proof}
  We have for $x \geq 1$:
    \[\alpha_{\varphi}(x) = \frac{\mathbb{E}[\Poi(x)^d]}{\varphi(x)} = \frac{e^{-x}\sum_{k=0}^{+\infty}k^d\frac{x^k}{k!}}{\varphi(x)} = e^{-x}\sum_{k=0}^{+\infty}k^d\frac{x^{k-d}}{k!} \]

   Then:
        
   \begin{equation}
     \begin{aligned}
       \alpha_{\varphi}'(x) &=&& -\alpha_{\varphi}(x)+ e^{-x}\sum_{k=1}^{+\infty}(k-d)k^d\frac{x^{k-d-1}}{k!} \\
       &=&& -\alpha_{\varphi}(x)+ e^{-x}\sum_{k=0}^{+\infty}(k+1-d)(k+1)^d\frac{x^{k-d}}{(k+1)!}\\
       &=&& -\alpha_{\varphi}(x)+ e^{-x}\Big((1-d)x^{-d} +  \sum_{k=1}^{+\infty}(k+1-d)(k+1)^{d-1}\frac{x^{k-d}}{k!}\Big)\\
       &=&& e^{-x}x^{-d}\Big(1-d + \sum_{k=1}^{+\infty}(\frac{k+1-d}{k+1}(k+1)^d - k^d)\frac{x^k}{k!}\Big)\\
     \end{aligned}
   \end{equation}

   But the function $f(k) = \frac{k+1-d}{k+1}(k+1)^d - k^d$ is positive on $\mathbb{R}_+^*$, so we get that $\alpha_{\varphi}'(x) > 0$ for $x \geq 1$, thus $\alpha_{\varphi}(x)$ is increasing from $1$ to $+\infty$, so $\alpha_{\varphi} = \alpha_{\varphi}(1) = e^{-1}\sum_{k=1}^{+\infty}\frac{k^d}{k!}$.
\end{proof}

\section{NP-hardness of $\delta,h$-\textsc{AryGapLabelCover}}
\label{NPhardnessGap}
\begin{proof}[Proof of property \ref{prop:AryGapLabelCover}]
  We reduce frome the Label Cover problem described in \cite{DDMS20} which is known no be an NP-hard problem. The main idea of this reduction is the usual equivalence between a bipartite graph and a hypergraph.

  \begin{defi}
    A Label Cover instance $\mathcal{L} = (A,B,E,[L],[R],\set{\pi_e}_{e \in E})$ consists of a bi-regular bipartite graph $(A,B,E)$ with right degree $t$, alphabet sets $[L],[R]$ and for every edge $e \in E$, a constraint $\pi_e :[L] \rightarrow [R]$.
    A \emph{labelling} of $\mathcal{L}$ is a function $\sigma : A \rightarrow [L]$. We say that $\sigma$ \emph{strongly satisfies} a right vertex $v \in B$ if for every two neighbours $u,u'$ of $v$, we have $\pi_{(u,v)}(\sigma(u)) = \pi_{(u',v)}(\sigma(u'))$. Moreover, we say that  $\sigma$ \emph{weakly satisfies} a right vertex $v \in B$ if there exists two neighbours $u,u'$ of $v$ such that $\pi_{(u,v)}(\sigma(u)) = \pi_{(u',v)}(\sigma(u'))$.
  \end{defi}

 \begin{theo}[$\delta$-Gap-Label-Cover$(t,R)$ from \cite{DDMS20}]
  For any fixed integer $t \geq 2$ and fixed $\delta > 0$, there exists $R$ that can be taken as large as we want, such that it is NP-hard for Label Cover instances $\mathcal{L} = (A,B,E,[L],[R],\set{\pi_e}_{e \in E})$ with right degree $t$ to distinguish between:
  
  \begin{itemize}
  \item[\textbf{YES:}] There exists a labeling $\sigma$ that \emph{strongly satisfies} all the right vertices.
  \item[\textbf{NO:}] No labeling \emph{weakly satisfies} more than $\delta$ fraction of the right vertices.
  \end{itemize}
\end{theo}

 The reduction is the following. From $\delta$-Gap-Label-Cover$(t,R)$, we take $h=t$ and the same parameters $\delta,R$. Given an instance $\mathcal{L} = (A,B,E,[L],[R],\set{\pi_e}_{e \in E})$, we take $\mathcal{G} = (A,E',[L],[R],\set{\pi'_{e',v}}_{e' \in E',v \in e'})$ with $E' = \set{N(b), b \in B}$ with $N(b)$ the set of neighbours of $b$ in $\mathcal{L}$, and $\pi'_{e',v} = \pi'_{N(b),v} := \pi_{v,b}$ since $v \in N(b)$.
 Since $(A,B,E)$ is bipartite and biregular, we get that our hypergraph has all hyperedges of size $h = \abs{N(b)} = t$, and that it is regular from the regular left degree of $(A,B,E)$. By construction, the notion of weakly and strongly satisfied is the same in both cases, as well as the labellings, and thus we have the NP-hardness of $\delta,h$-\textsc{AryGapLabelCover}.
 
\end{proof}

\section{Proof of existence of partitioning systems}
\begin{proof}[Proof of property \ref{prop:Partitioning}]
    The existential proof is based on the probabilistic method. We take $\mathcal{P}_i$ an $h$-equi-sized uniform random $x_{\varphi}$-cover of $[n]$. Hence in the collection $\mathcal{P}_i=(P_{i,1},\ldots,P_{i,h})$, each of the $h$ subsets is of cardinality $x_{\varphi}n/h$. Write $\mathcal{P} = (\mathcal{P}_1,\ldots,\mathcal{P}_R)$. We have that for any $a \in [n], \mathbb{P}(a \in P_{i,j}) = x_{\varphi}/h$. We note that although for $i$ fixed those events are not independent, but for different $i$s, these events are independent.

    By construction, the first condition is fulfilled. In order to prove the second condition, we prove some bound on a fixed $T$ and $\mathcal{Q}$ and with union bound we will go over all of them.

    Let fix $T \subseteq [R]$ and $\mathcal{Q} := \set{P_i : i \in T}$ with $P_i :=P_{i, j(i)}$ for some function $j$.

    We have that for some $a \in [n]$:
    \[ \mathbb{E}[C_a^{\varphi}(\mathcal{Q})] =  \mathbb{E}[\varphi(\abs{\mathcal{Q}}_a)] = \mathbb{E}[\varphi(\abs{\set{i \in T: a \in P_i}})]\]

    But the random variables $\set{X_i := \mathbbm{1}_{a \in P_i}}_{i \in T}$ are independent and follow the same Bernoulli law of parameter $x_{\varphi}/h$, so the variable $X =\abs{\set{i \in T: a \in P_i}} = \sum_{i \in T} X_i \sim \Bin(\abs{T},x_{\varphi}/h)$, and thus we have that
    \[\mathbb{E}[C_e^{\varphi}(\mathcal{Q})] = \mathbb{E}[\varphi(\Bin(\abs{T},x_{\varphi}/h))] =\psi^{\varphi}_{\abs{T},h}\]

    Now we have that $0 \leq C_a^{\varphi}(\mathcal{Q}) \leq \varphi(R)$ since $\abs{\mathcal{Q}}_a\leq \abs{\mathcal{Q}} \leq R$ and $\varphi$ nondecreasing. Thanks to Azuma-Hoeffding bound, we get that:

    \[ \mathbb{P}\Big( \abs{C_a^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} > \eta n\Big) \leq 2 \text{exp}\Big(-\Big(\frac{\eta}{\varphi(R)}\Big)^2n\Big)\]

    Now, since there are at most $(h+1)^R$ choices of $T$ and $\mathcal{Q}$, we apply a union bound to this:
    \[ \mathbb{P}\Big(\exists C,\mathcal{Q} : \abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} > \eta n\Big) \leq 2(h+1)^R \text{exp}\Big(-\Big(\frac{\eta}{\varphi(R)}\Big)^2n\Big)\]

    Thus w.p. at least $9/10$, we have that $\abs{C^{\varphi}(\mathcal{Q}) -\psi^{\varphi}_{\abs{T},h} n} \leq \eta n$, since we have taken $n \geq \eta^{-2}R\varphi(R)^2\log(20(h+1))$. So there must exists some choice of $\mathcal{P}$ that satisfies the first and second constraints of partitioning systems.

    Finally, since a random choice of $\mathcal{P} = (\mathcal{P}_1,\ldots,\mathcal{P}_R)$ satisfies the desired properties, we can enumerate over all choices of $\mathcal{P}$ in time exp($Rn$log$n).$poly$(h)$ to find such a partitioning system.
\end{proof}

\section{Lemmas used in the hardness theorem \ref{theo:Hardness} and their proofs}
  \begin{lem}
  For any nonnegative nondecreasing function $f: \mathbb{N} \rightarrow \mathbb{R^+}$ with $f(k) \leq k$, we have that:
  \[ \abs{\mathbb{E}[f(\Bin(n,x/n))] - \mathbb{E}[f(\Poi(x))]} \leq \frac{x f(n)}{2n} + \frac{x^{n+1}}{n!}\]

  In particular for $\varphi$ studied in this article when $\varphi(n) = o(n)$:
  \[ \lim_{h \rightarrow  \infty} \psi^{\varphi}_{h,h} = \alpha_{\varphi}\varphi(x_{\varphi})\]

  \label{lem:UnboundBinPoi}
\end{lem}

\begin{proof}
  Thanks to \cite{TF19,Barbour84}, we have that the total variation distance between $\Bin(n,x/n)$ and $\Poi(x)$ is bounded in the following way:
  \[ \Delta(\Bin(n,x/n),\Poi(x)) \leq \frac{1 - e^{-x}}{2x} n* \Big(\frac{x}{n}\Big)^2 \leq \frac{x}{2n}\]
  Thus with $B \sim \Bin(n,x/n)$ and $P \sim \Poi(x)$:
  \begin{equation}
    \begin{aligned}
      \abs{\mathbb{E}[f(B)] - \mathbb{E}[f(P)]} &=&&  \abs{\sum_{k=0}^{+\infty}f(k)\mathbb{P}(B=k) - \sum_{k=0}^{+\infty}f(k)\mathbb{P}(P=k)}\\
      &=&&  \abs{\sum_{k=0}^{+\infty}f(k)(\mathbb{P}(B=k) - \mathbb{P}(P=k))}\\
      &\leq&& \sum_{k=0}^{+\infty}f(k)\abs{\mathbb{P}(B=k) - \mathbb{P}(P=k)}\\
      &\leq&& f(n)\Delta(\Bin(n,x/n),\Poi(x))\\
      &+&& \sum_{k=n+1}^{+\infty}f(k)\mathbb{P}(P=k)\\
      &\leq&&\frac{x f(n)}{2n}  + e^{-x}\sum_{k=n+1}^{+\infty}k\frac{x^k}{k!}\\
      &=&& \frac{x f(n)}{2n}  + xe^{-x}\sum_{k=n}^{+\infty}\frac{x^k}{k!}\\
      &\leq&& \frac{x f(n)}{2n}  + \frac{x^{n+1}}{n!} \underset{n \rightarrow \infty}{\rightarrow} 0 \text{ when } f(n) = o(n)
    \end{aligned}
  \end{equation}

  by a standard upper bound on the remainder of the exponential series.

  We get the particular case directly since $\varphi$ follows the hypothesis asked for $f$ and that $\psi^{\varphi}_{h,h} = \mathbb{E}[\varphi(\Bin(h,x_{\varphi}/h))]$ and $\alpha_{\varphi}\varphi(x_{\varphi}) = \mathbb{E}[\varphi(\Poi(x_{\varphi}))]$
\end{proof}
  
\begin{lem}
  For $\varphi$ nondecreasing concave with $\varphi(0)=0,\varphi(1)=1$, we have that the function $g_q : n \mapsto \mathbb{E}[\varphi(\Bin(n,q))]$ defined on $\mathbb{N}$ is nondecreasing concave. As a consequence, one can uses Jensen's inequality on the piecewise linear extension of $g_q$ which is also continuous.
  \label{lem:BinCon}
\end{lem}

\begin{proof}
  $\Bin(n,q) \leq_{\text{st}} \Bin(n+1,q)$ and we have that $\varphi$ is nondecreasing, so $\mathbb{E}[\varphi(\Bin(n,q))] \leq \mathbb{E}[\varphi(\Bin(n+1,q))]$, ie $g_q(n+1) - g_q(n) \geq 0$: $g_q$ is nondecreasing.

  We show then the concavity, ie. $g_q(n+2) - g_q(n+1) \leq g_q(n+1) - g_q(n)$. Call $\psi(x) = \varphi(x+1)-\varphi(x)$ which is nonincreasing since $\varphi$ concave. Let us take $X_{k,q} \sim \Bin(k,q)$. Then:

  \begin{equation}
    \begin{aligned}
      g_q(n+1) &=&& \mathbb{E}[\varphi(X_{n+1,q})]\\
      &=&& \sum_{i=0}^n \mathbb{E}[\varphi(X_{n,q}+X_{1,q})|X_{n,q}=i]\mathbb{P}(X_{n,q}=i)\\
      &=&& \sum_{i=0}^n \mathbb{E}[\varphi(i+X_{1,q}) - \varphi(i)]\mathbb{P}(X_{n,q}=i)\\
      &+&& \sum_{i=0}^n \varphi(i)\mathbb{P}(X_{n,q}=i)\\
      &=&& \sum_{i=0}^n \mathbb{E}[\varphi(i+X_{1,q}) - \varphi(i)]\mathbb{P}(X_{n,q}=i) + g_q(n)
    \end{aligned}
  \end{equation}

  Thus:

  \begin{equation}
    \begin{aligned}
      g_q(n+1) -g_q(n) &=&& \sum_{i=0}^n \mathbb{E}[\varphi(i+X_{1,q}) - \varphi(i)]\mathbb{P}(X_{n,q}=i)\\
      &=&& \sum_{i=0}^n q(\varphi(i+1) - \varphi(i))\mathbb{P}(X_{n,q}=i)\\
      &=&& q \mathbb{E}[\psi(\Bin(n,q))]
    \end{aligned}
  \end{equation}

  Then thanks to the fact that  $\Bin(n,q) \leq_{\text{st}} \Bin(n+1,q)$ and $\psi$ is nonincreasing, we have that $\mathbb{E}[\psi(\Bin(n,q))] \geq \mathbb{E}[\psi(\Bin(n+1,q))]$, ie. $g_q(n+2) - g_q(n+1) \leq g_q(n+1) - g_q(n)$.
\end{proof}
%\end{multicols}
\end{document}
